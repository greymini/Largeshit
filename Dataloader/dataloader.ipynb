{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x1064986e0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Text Loader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"speech.txt\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content=\"If you get hold of two magnets and you push them, you can feel this pushing between them. Now, turn around the other way and then they slam together. Now what is it the feeling between those two magnets?\\n What do you mean what's the feeling between the two?\\nThere's something there isn't there. I  mean that the sensation is that there's something there when you push these two magnets together.\\nListen to my question. What is the meaning when you say that there's, there's a feeling? Of course you feel it. Now what do you want to know?\\nWhat I want to know is what's going on between these two bits, these two bits of metal?\\n They just repel each other.\\nWell then what does that, but, what does that mean or why are they doing that or how are they doing it?\\nYou're asking…\\nI must say I think it's a perfectly reasonable question.\\nOf course it's a reasonable — it's an excellent question, okay. But the problem that you're asking — you see when you ask why something happens. How does a person answer why something happens?\\nFor example Aunt Minnie is in a hospital. Why? Because she slipped. She went out and she slipped on the ice and broke her hip. That satisfies people. It satisfies. But it wouldn't satisfy someone who came from another planet who knew nothing about things. \\nAt first you understand why when you break your hip do you go to the hospital. How do you get to the hospital with the, when the hip was broken? Well because her husband seeing that she had the hip was broken called the hospital up and sent somebody to get her.\\nAll that is understood by people. Now when you explain a why you have to be in some framework that you allow something to be true. Otherwise you're perpetually asking why.\\nWhy did the husband call up the hospital?\\nBecause the husband is interested in his wife's welfare. Not always. Some husbands aren't interested in their wife’s welfare when they're drunk and they're angry and so you begin to get a very interesting understanding of the world and all its complications.\\n If you try to follow anything up you go deeper and deeper in various directions. If, for example, you go ‘Why did she slip on the ice?’, well, ice is slippery. Everybody knows that. No problem.\\nBut you ask ‘Why is ice slippery?’ That's kind of curious. Ice is extremely slippery. It's very interesting.  You say how does it work?  You could, you see,  you could either say I'm satisfied that you've answered me — ice is slippery. That explains it. Or,  you could go on and say why is it slippery? And then you're involved with something because there aren't many things as slippery as ice. It's very hard to get greasy stuff,  but that's sort of wet and slimy. But a solid that's so slippery, because it is in the case of ice, that when you stand on it they say, momentarily, the pressure melts the ice a little bit so you got a sort of instantaneous water surface on which you're slipping.\\nWhy on ice and not on other things? Because ice expands. Water expands when it freezes, so the pressure tries to undo the expansion and melts it, is capable of melting it. But other substances contract when they're freezing and when you push them they're just satisfied to be solid.\\nWhy does water expand when it freezes and other substances don't expand when they freeze?\\nAll right. \\nHave I  answered your question? But I'm telling you how difficult a why question is. You have to know what it is that you're permitted to understand and allow to be understood and known and what it is you're not.\\n You'll notice in this example that the more I ask why, it gets interesting afterwards, my idea. The deeper the thing is, the more interesting. And, we could even go further and say why did she fall down when she slipped? That has to do with gravity and involves all the planets and everything else. Never mind it goes on and on.\\nNow, when you ask for examples why two magnets repel, there are many different levels. It depends on whether you're a student of physics or an ordinary person who doesn't know anything or not. If you're somebody who doesn't know anything at all about it, all I can say is that there's a magnetic force that makes them repel and that you're feeling that force. You say, that's very strange because I don't feel a kind of force like that in other circumstances. When you turn them the other way they attract.\\nThere's a very analogous force, electrical force, which is the same kind of a question and you say that's also very weird. But you're not at all disturbed by the fact that when you put your hand on the chair it pushes you back. But we found out by looking at it that that's the same force as a matter of fact, the electrical force, not magnetic exactly in that case.  But it's the same electrical repulsions that are involved in keeping your finger away from the chair because everything's made out of its electrical forces in minor — in microscopic details. There's other forces involved, but this is connected to electrical forces. \\nIt turns out that the magnetic and the electric force with which I wish to explain these things, this, this repulsion in the first place, is what ultimately is the deeper thing that we have to start with to explain many other things that looked like they were — everybody would just accept them.\\nYou know you can't put your hand through the chair. That's taken for granted.  But that you can't put your hand through the chair when looked at more closely, why, that involves these same repulsive forces that appear in magnets. \\nThe situation you then have to explain is why in magnets it goes over a bigger distance than ordinarily. And there it has to do with the fact that in iron all the electrons are spinning in the same direction. They all get lined up and they magnify the effect of the force until it's large enough at a distance that you can feel it. But it's a force which is present, all the time and very common and is in a basic force of almost…  I mean I can go a little further back if I were more technical but in an early level I've just got to have to tell you that's going to be one of the things you'll just have to take as an element in the world. The existence of magnetic repulsion, or electrical attraction, magnetic attraction. I can't explain that attraction in terms of anything else that's familiar to you. \\nFor example if I said the magnets attract like as if they were connected by rubber bands, I would be cheating you because they're not connected by rubber bands. I shouldn't be in trouble. But you’d soon ask me about the nature of the bands and secondly if you were curious enough you'd ask me why rubber bands tend to pull back together again and I would end up explaining that in terms of electrical forces. Which are the very things that I'm trying to use the rubber bands to explain. So I have cheated very badly you see.\\nSo I'm not going to be able to give you an answer to why magnets attract each other, except to tell you that they do. And to tell you that that's one of the elements in the world. There are different kinds of forces. There are electrical forces, magnetic forces, gravitational forces, and others and those are some of the parts.\\nIf you were a student you'd go further. I could go further. I could tell you that the magnetic forces are related to the electrical forces very intimately. That our relationship between the gravity forces and electrical forces remains unknown and so on. But I really can't do a good job, any job, of explaining magnetic force, in terms of something else that you're more familiar with because I don't understand it in terms of anything else that you're more familiar with.\")]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_document = loader.load()\n",
    "text_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##reading a pdf file \n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"SentBert.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'SentBert.pdf', 'page': 0}, page_content='Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\\nNils Reimers and Iryna Gurevych\\nUbiquitous Knowledge Processing Lab (UKP-TUDA)\\nDepartment of Computer Science, Technische Universit¨at Darmstadt\\nwww.ukp.tu-darmstadt.de\\nAbstract\\nBERT (Devlin et al., 2018) and RoBERTa (Liu\\net al., 2019) has set a new state-of-the-art\\nperformance on sentence-pair regression tasks\\nlike semantic textual similarity (STS). How-\\never, it requires that both sentences are fed\\ninto the network, which causes a massive com-\\nputational overhead: Finding the most sim-\\nilar pair in a collection of 10,000 sentences\\nrequires about 50 million inference computa-\\ntions (~65 hours) with BERT. The construction\\nof BERT makes it unsuitable for semantic sim-\\nilarity search as well as for unsupervised tasks\\nlike clustering.\\nIn this publication, we present Sentence-BERT\\n(SBERT), a modiﬁcation of the pretrained\\nBERT network that use siamese and triplet net-\\nwork structures to derive semantically mean-\\ningful sentence embeddings that can be com-\\npared using cosine-similarity. This reduces the\\neffort for ﬁnding the most similar pair from 65\\nhours with BERT / RoBERTa to about 5 sec-\\nonds with SBERT, while maintaining the ac-\\ncuracy from BERT.\\nWe evaluate SBERT and SRoBERTa on com-\\nmon STS tasks and transfer learning tasks,\\nwhere it outperforms other state-of-the-art\\nsentence embeddings methods.1\\n1 Introduction\\nIn this publication, we present Sentence-BERT\\n(SBERT), a modiﬁcation of the BERT network us-\\ning siamese and triplet networks that is able to\\nderive semantically meaningful sentence embed-\\ndings2. This enables BERT to be used for certain\\nnew tasks, which up-to-now were not applicable\\nfor BERT. These tasks include large-scale seman-\\n1Code available: https://github.com/UKPLab/\\nsentence-transformers\\n2With semantically meaningfulwe mean that semantically\\nsimilar sentences are close in vector space.\\ntic similarity comparison, clustering, and informa-\\ntion retrieval via semantic search.\\nBERT set new state-of-the-art performance on\\nvarious sentence classiﬁcation and sentence-pair\\nregression tasks. BERT uses a cross-encoder: Two\\nsentences are passed to the transformer network\\nand the target value is predicted. However, this\\nsetup is unsuitable for various pair regression tasks\\ndue to too many possible combinations. Finding\\nin a collection of n = 10 000sentences the pair\\nwith the highest similarity requires with BERT\\nn·(n−1)/2 = 49 995 000inference computations.\\nOn a modern V100 GPU, this requires about 65\\nhours. Similar, ﬁnding which of the over 40 mil-\\nlion existent questions of Quora is the most similar\\nfor a new question could be modeled as a pair-wise\\ncomparison with BERT, however, answering a sin-\\ngle query would require over 50 hours.\\nA common method to address clustering and se-\\nmantic search is to map each sentence to a vec-\\ntor space such that semantically similar sentences\\nare close. Researchers have started to input indi-\\nvidual sentences into BERT and to derive ﬁxed-\\nsize sentence embeddings. The most commonly\\nused approach is to average the BERT output layer\\n(known as BERT embeddings) or by using the out-\\nput of the ﬁrst token (the [CLS] token). As we\\nwill show, this common practice yields rather bad\\nsentence embeddings, often worse than averaging\\nGloVe embeddings (Pennington et al., 2014).\\nTo alleviate this issue, we developed SBERT.\\nThe siamese network architecture enables that\\nﬁxed-sized vectors for input sentences can be de-\\nrived. Using a similarity measure like cosine-\\nsimilarity or Manhatten / Euclidean distance, se-\\nmantically similar sentences can be found. These\\nsimilarity measures can be performed extremely\\nefﬁcient on modern hardware, allowing SBERT\\nto be used for semantic similarity search as well\\nas for clustering. The complexity for ﬁnding the\\narXiv:1908.10084v1  [cs.CL]  27 Aug 2019'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 1}, page_content='most similar sentence pair in a collection of 10,000\\nsentences is reduced from 65 hours with BERT to\\nthe computation of 10,000 sentence embeddings\\n(~5 seconds with SBERT) and computing cosine-\\nsimilarity (~0.01 seconds). By using optimized\\nindex structures, ﬁnding the most similar Quora\\nquestion can be reduced from 50 hours to a few\\nmilliseconds (Johnson et al., 2017).\\nWe ﬁne-tune SBERT on NLI data, which cre-\\nates sentence embeddings that signiﬁcantly out-\\nperform other state-of-the-art sentence embedding\\nmethods like InferSent (Conneau et al., 2017) and\\nUniversal Sentence Encoder (Cer et al., 2018). On\\nseven Semantic Textual Similarity (STS) tasks,\\nSBERT achieves an improvement of 11.7 points\\ncompared to InferSent and 5.5 points compared to\\nUniversal Sentence Encoder. On SentEval (Con-\\nneau and Kiela, 2018), an evaluation toolkit for\\nsentence embeddings, we achieve an improvement\\nof 2.1 and 2.6 points, respectively.\\nSBERT can be adapted to a speciﬁc task. It\\nsets new state-of-the-art performance on a chal-\\nlenging argument similarity dataset (Misra et al.,\\n2016) and on a triplet dataset to distinguish sen-\\ntences from different sections of a Wikipedia arti-\\ncle (Dor et al., 2018).\\nThe paper is structured in the following way:\\nSection 3 presents SBERT, section 4 evaluates\\nSBERT on common STS tasks and on the chal-\\nlenging Argument Facet Similarity (AFS) corpus\\n(Misra et al., 2016). Section 5 evaluates SBERT\\non SentEval. In section 6, we perform an ablation\\nstudy to test some design aspect of SBERT. In sec-\\ntion 7, we compare the computational efﬁciency of\\nSBERT sentence embeddings in contrast to other\\nstate-of-the-art sentence embedding methods.\\n2 Related Work\\nWe ﬁrst introduce BERT, then, we discuss state-\\nof-the-art sentence embedding methods.\\nBERT (Devlin et al., 2018) is a pre-trained\\ntransformer network (Vaswani et al., 2017), which\\nset for various NLP tasks new state-of-the-art re-\\nsults, including question answering, sentence clas-\\nsiﬁcation, and sentence-pair regression. The input\\nfor BERT for sentence-pair regression consists of\\nthe two sentences, separated by a special [SEP]\\ntoken. Multi-head attention over 12 (base-model)\\nor 24 layers (large-model) is applied and the out-\\nput is passed to a simple regression function to de-\\nrive the ﬁnal label. Using this setup, BERT set a\\nnew state-of-the-art performance on the Semantic\\nTextual Semilarity (STS) benchmark (Cer et al.,\\n2017). RoBERTa (Liu et al., 2019) showed, that\\nthe performance of BERT can further improved by\\nsmall adaptations to the pre-training process. We\\nalso tested XLNet (Yang et al., 2019), but it led in\\ngeneral to worse results than BERT.\\nA large disadvantage of the BERT network\\nstructure is that no independent sentence embed-\\ndings are computed, which makes it difﬁcult to de-\\nrive sentence embeddings from BERT. To bypass\\nthis limitations, researchers passed single sen-\\ntences through BERT and then derive a ﬁxed sized\\nvector by either averaging the outputs (similar to\\naverage word embeddings) or by using the output\\nof the special CLS token (for example: May et al.\\n(2019); Zhang et al. (2019); Qiao et al. (2019)).\\nThese two options are also provided by the popu-\\nlar bert-as-a-service-repository3. Up to our knowl-\\nedge, there is so far no evaluation if these methods\\nlead to useful sentence embeddings.\\nSentence embeddings are a well studied area\\nwith dozens of proposed methods. Skip-Thought\\n(Kiros et al., 2015) trains an encoder-decoder ar-\\nchitecture to predict the surrounding sentences.\\nInferSent (Conneau et al., 2017) uses labeled\\ndata of the Stanford Natural Language Inference\\ndataset (Bowman et al., 2015) and the Multi-\\nGenre NLI dataset (Williams et al., 2018) to train\\na siamese BiLSTM network with max-pooling\\nover the output. Conneau et al. showed, that\\nInferSent consistently outperforms unsupervised\\nmethods like SkipThought. Universal Sentence\\nEncoder (Cer et al., 2018) trains a transformer\\nnetwork and augments unsupervised learning with\\ntraining on SNLI. Hill et al. (2016) showed, that\\nthe task on which sentence embeddings are trained\\nsigniﬁcantly impacts their quality. Previous work\\n(Conneau et al., 2017; Cer et al., 2018) found that\\nthe SNLI datasets are suitable for training sen-\\ntence embeddings. Yang et al. (2018) presented\\na method to train on conversations from Reddit\\nusing siamese DAN and siamese transformer net-\\nworks, which yielded good results on the STS\\nbenchmark dataset.\\nHumeau et al. (2019) addresses the run-time\\noverhead of the cross-encoder from BERT and\\npresent a method (poly-encoders) to compute\\na score between m context vectors and pre-\\n3https://github.com/hanxiao/\\nbert-as-service/'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 2}, page_content='Sentence A Sentence B \\nBERT BERT \\nu v \\npooling pooling \\n(u, v, |u-v|) \\nSoftmax classifier \\nFigure 1: SBERT architecture with classiﬁcation ob-\\njective function, e.g., for ﬁne-tuning on SNLI dataset.\\nThe two BERT networks have tied weights (siamese\\nnetwork structure).\\ncomputed candidate embeddings using attention.\\nThis idea works for ﬁnding the highest scoring\\nsentence in a larger collection. However, poly-\\nencoders have the drawback that the score function\\nis not symmetric and the computational overhead\\nis too large for use-cases like clustering, which\\nwould require O(n2) score computations.\\nPrevious neural sentence embedding methods\\nstarted the training from a random initialization.\\nIn this publication, we use the pre-trained BERT\\nand RoBERTa network and only ﬁne-tune it to\\nyield useful sentence embeddings. This reduces\\nsigniﬁcantly the needed training time: SBERT can\\nbe tuned in less than 20 minutes, while yielding\\nbetter results than comparable sentence embed-\\nding methods.\\n3 Model\\nSBERT adds a pooling operation to the output\\nof BERT / RoBERTa to derive a ﬁxed sized sen-\\ntence embedding. We experiment with three pool-\\ning strategies: Using the output of the CLS-token,\\ncomputing the mean of all output vectors ( MEAN-\\nstrategy), and computing a max-over-time of the\\noutput vectors (MAX-strategy). The default conﬁg-\\nuration is MEAN.\\nIn order to ﬁne-tune BERT / RoBERTa, we cre-\\nate siamese and triplet networks (Schroff et al.,\\n2015) to update the weights such that the produced\\nsentence embeddings are semantically meaningful\\nand can be compared with cosine-similarity.\\nThe network structure depends on the available\\nSentence A Sentence B \\nBERT BERT \\nu v \\npooling pooling \\ncosine-sim(u, v) \\n-1 … 1 \\nFigure 2: SBERT architecture at inference, for exam-\\nple, to compute similarity scores. This architecture is\\nalso used with the regression objective function.\\ntraining data. We experiment with the following\\nstructures and objective functions.\\nClassiﬁcation Objective Function. We con-\\ncatenate the sentence embeddings u and v with\\nthe element-wise difference |u−v|and multiply it\\nwith the trainable weight Wt ∈R3n×k:\\no= softmax(Wt(u,v, |u−v|))\\nwhere n is the dimension of the sentence em-\\nbeddings and kthe number of labels. We optimize\\ncross-entropy loss. This structure is depicted in\\nFigure 1.\\nRegression Objective Function. The cosine-\\nsimilarity between the two sentence embeddings\\nu and v is computed (Figure 2). We use mean-\\nsquared-error loss as the objective function.\\nTriplet Objective Function. Given an anchor\\nsentence a, a positive sentence p, and a negative\\nsentence n, triplet loss tunes the network such that\\nthe distance between a and p is smaller than the\\ndistance between a and n. Mathematically, we\\nminimize the following loss function:\\nmax(||sa −sp||−||sa −sn||+ ϵ,0)\\nwith sx the sentence embedding for a/n/p, ||·||\\na distance metric and margin ϵ. Margin ϵensures\\nthat sp is at least ϵcloser to sa than sn. As metric\\nwe use Euclidean distance and we set ϵ= 1in our\\nexperiments.\\n3.1 Training Details\\nWe train SBERT on the combination of the SNLI\\n(Bowman et al., 2015) and the Multi-Genre NLI'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 3}, page_content='Model STS12 STS13 STS14 STS15 STS16 STSb SICK-R Avg.\\nAvg. GloVe embeddings 55.14 70.66 59.73 68.25 63.66 58.02 53.76 61.32\\nAvg. BERT embeddings 38.78 57.98 57.98 63.15 61.06 46.35 58.40 54.81\\nBERT CLS-vector 20.16 30.01 20.09 36.88 38.08 16.50 42.63 29.19\\nInferSent - Glove 52.86 66.75 62.15 72.77 66.87 68.03 65.65 65.01\\nUniversal Sentence Encoder 64.49 67.80 64.61 76.83 73.18 74.92 76.69 71.22\\nSBERT-NLI-base 70.97 76.53 73.19 79.09 74.30 77.03 72.91 74.89\\nSBERT-NLI-large 72.27 78.46 74.90 80.99 76.25 79.23 73.75 76.55\\nSRoBERTa-NLI-base 71.54 72.49 70.80 78.74 73.69 77.77 74.46 74.21\\nSRoBERTa-NLI-large 74.53 77.00 73.18 81.85 76.82 79.10 74.29 76.68\\nTable 1: Spearman rank correlation ρbetween the cosine similarity of sentence representations and the gold labels\\nfor various Textual Similarity (STS) tasks. Performance is reported by convention as ρ×100. STS12-STS16:\\nSemEval 2012-2016, STSb: STSbenchmark, SICK-R: SICK relatedness dataset.\\n(Williams et al., 2018) dataset. The SNLI is a col-\\nlection of 570,000 sentence pairs annotated with\\nthe labels contradiction, eintailment, and neu-\\ntral. MultiNLI contains 430,000 sentence pairs\\nand covers a range of genres of spoken and written\\ntext. We ﬁne-tune SBERT with a 3-way softmax-\\nclassiﬁer objective function for one epoch. We\\nused a batch-size of 16, Adam optimizer with\\nlearning rate 2e−5, and a linear learning rate\\nwarm-up over 10% of the training data. Our de-\\nfault pooling strategy is MEAN.\\n4 Evaluation - Semantic Textual\\nSimilarity\\nWe evaluate the performance of SBERT for com-\\nmon Semantic Textual Similarity (STS) tasks.\\nState-of-the-art methods often learn a (complex)\\nregression function that maps sentence embed-\\ndings to a similarity score. However, these regres-\\nsion functions work pair-wise and due to the com-\\nbinatorial explosion those are often not scalable if\\nthe collection of sentences reaches a certain size.\\nInstead, we always use cosine-similarity to com-\\npare the similarity between two sentence embed-\\ndings. We ran our experiments also with nega-\\ntive Manhatten and negative Euclidean distances\\nas similarity measures, but the results for all ap-\\nproaches remained roughly the same.\\n4.1 Unsupervised STS\\nWe evaluate the performance of SBERT for STS\\nwithout using any STS speciﬁc training data. We\\nuse the STS tasks 2012 - 2016 (Agirre et al., 2012,\\n2013, 2014, 2015, 2016), the STS benchmark (Cer\\net al., 2017), and the SICK-Relatedness dataset\\n(Marelli et al., 2014). These datasets provide la-\\nbels between 0 and 5 on the semantic relatedness\\nof sentence pairs. We showed in (Reimers et al.,\\n2016) that Pearson correlation is badly suited for\\nSTS. Instead, we compute the Spearman’s rank\\ncorrelation between the cosine-similarity of the\\nsentence embeddings and the gold labels. The\\nsetup for the other sentence embedding methods\\nis equivalent, the similarity is computed by cosine-\\nsimilarity. The results are depicted in Table 1.\\nThe results shows that directly using the output\\nof BERT leads to rather poor performances. Av-\\neraging the BERT embeddings achieves an aver-\\nage correlation of only 54.81, and using the CLS-\\ntoken output only achieves an average correlation\\nof 29.19. Both are worse than computing average\\nGloVe embeddings.\\nUsing the described siamese network structure\\nand ﬁne-tuning mechanism substantially improves\\nthe correlation, outperforming both InferSent and\\nUniversal Sentence Encoder substantially. The\\nonly dataset where SBERT performs worse than\\nUniversal Sentence Encoder is SICK-R. Universal\\nSentence Encoder was trained on various datasets,\\nincluding news, question-answer pages and dis-\\ncussion forums, which appears to be more suitable\\nto the data of SICK-R. In contrast, SBERT was\\npre-trained only on Wikipedia (via BERT) and on\\nNLI data.\\nWhile RoBERTa was able to improve the per-\\nformance for several supervised tasks, we only\\nobserve minor difference between SBERT and\\nSRoBERTa for generating sentence embeddings.\\n4.2 Supervised STS\\nThe STS benchmark (STSb) (Cer et al., 2017) pro-\\nvides is a popular dataset to evaluate supervised\\nSTS systems. The data includes 8,628 sentence\\npairs from the three categoriescaptions, news, and\\nforums. It is divided into train (5,749), dev (1,500)\\nand test (1,379). BERT set a new state-of-the-art\\nperformance on this dataset by passing both sen-\\ntences to the network and using a simple regres-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 4}, page_content='sion method for the output.\\nModel Spearman\\nNot trained for STS\\nAvg. GloVe embeddings 58.02\\nAvg. BERT embeddings 46.35\\nInferSent - GloVe 68.03\\nUniversal Sentence Encoder 74.92\\nSBERT-NLI-base 77.03\\nSBERT-NLI-large 79.23\\nTrained on STS benchmark dataset\\nBERT-STSb-base 84.30 ±0.76\\nSBERT-STSb-base 84.67 ±0.19\\nSRoBERTa-STSb-base 84.92 ±0.34\\nBERT-STSb-large 85.64 ±0.81\\nSBERT-STSb-large 84.45 ±0.43\\nSRoBERTa-STSb-large 85.02 ±0.76\\nTrained on NLI data + STS benchmark data\\nBERT-NLI-STSb-base 88.33 ±0.19\\nSBERT-NLI-STSb-base 85.35 ±0.17\\nSRoBERTa-NLI-STSb-base 84.79 ±0.38\\nBERT-NLI-STSb-large 88.77 ±0.46\\nSBERT-NLI-STSb-large 86.10 ±0.13\\nSRoBERTa-NLI-STSb-large 86.15 ±0.35\\nTable 2: Evaluation on the STS benchmark test set.\\nBERT systems were trained with 10 random seeds and\\n4 epochs. SBERT was ﬁne-tuned on the STSb dataset,\\nSBERT-NLI was pretrained on the NLI datasets, then\\nﬁne-tuned on the STSb dataset.\\nWe use the training set to ﬁne-tune SBERT us-\\ning the regression objective function. At predic-\\ntion time, we compute the cosine-similarity be-\\ntween the sentence embeddings. All systems are\\ntrained with 10 random seeds to counter variances\\n(Reimers and Gurevych, 2018).\\nThe results are depicted in Table 2. We ex-\\nperimented with two setups: Only training on\\nSTSb, and ﬁrst training on NLI, then training on\\nSTSb. We observe that the later strategy leads to a\\nslight improvement of 1-2 points. This two-step\\napproach had an especially large impact for the\\nBERT cross-encoder, which improved the perfor-\\nmance by 3-4 points. We do not observe a signiﬁ-\\ncant difference between BERT and RoBERTa.\\n4.3 Argument Facet Similarity\\nWe evaluate SBERT on the Argument Facet Sim-\\nilarity (AFS) corpus by Misra et al. (2016). The\\nAFS corpus annotated 6,000 sentential argument\\npairs from social media dialogs on three contro-\\nversial topics: gun control, gay marriage, and\\ndeath penalty. The data was annotated on a scale\\nfrom 0 (“different topic”) to 5 (“completely equiv-\\nalent”). The similarity notion in the AFS corpus\\nis fairly different to the similarity notion in the\\nSTS datasets from SemEval. STS data is usually\\ndescriptive, while AFS data are argumentative ex-\\ncerpts from dialogs. To be considered similar, ar-\\nguments must not only make similar claims, but\\nalso provide a similar reasoning. Further, the lex-\\nical gap between the sentences in AFS is much\\nlarger. Hence, simple unsupervised methods as\\nwell as state-of-the-art STS systems perform badly\\non this dataset (Reimers et al., 2019).\\nWe evaluate SBERT on this dataset in two sce-\\nnarios: 1) As proposed by Misra et al., we evaluate\\nSBERT using 10-fold cross-validation. A draw-\\nback of this evaluation setup is that it is not clear\\nhow well approaches generalize to different top-\\nics. Hence, 2) we evaluate SBERT in a cross-topic\\nsetup. Two topics serve for training and the ap-\\nproach is evaluated on the left-out topic. We repeat\\nthis for all three topics and average the results.\\nSBERT is ﬁne-tuned using the Regression Ob-\\njective Function. The similarity score is computed\\nusing cosine-similarity based on the sentence em-\\nbeddings. We also provide the Pearson correla-\\ntion r to make the results comparable to Misra et\\nal. However, we showed (Reimers et al., 2016)\\nthat Pearson correlation has some serious draw-\\nbacks and should be avoided for comparing STS\\nsystems. The results are depicted in Table 3.\\nUnsupervised methods like tf-idf, average\\nGloVe embeddings or InferSent perform rather\\nbadly on this dataset with low scores. Training\\nSBERT in the 10-fold cross-validation setup gives\\na performance that is nearly on-par with BERT.\\nHowever, in the cross-topic evaluation, we ob-\\nserve a performance drop of SBERT by about 7\\npoints Spearman correlation. To be considered\\nsimilar, arguments should address the same claims\\nand provide the same reasoning. BERT is able to\\nuse attention to compare directly both sentences\\n(e.g. word-by-word comparison), while SBERT\\nmust map individual sentences from an unseen\\ntopic to a vector space such that arguments with\\nsimilar claims and reasons are close. This is a\\nmuch more challenging task, which appears to re-\\nquire more than just two topics for training to work\\non-par with BERT.\\n4.4 Wikipedia Sections Distinction\\nDor et al. (2018) use Wikipedia to create a the-\\nmatically ﬁne-grained train, dev and test set for\\nsentence embeddings methods. Wikipedia arti-\\ncles are separated into distinct sections focusing\\non certain aspects. Dor et al. assume that sen-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 5}, page_content='Model r ρ\\nUnsupervised methods\\ntf-idf 46.77 42.95\\nAvg. GloVe embeddings 32.40 34.00\\nInferSent - GloVe 27.08 26.63\\n10-fold Cross-Validation\\nSVR (Misra et al., 2016) 63.33 -\\nBERT-AFS-base 77.20 74.84\\nSBERT-AFS-base 76.57 74.13\\nBERT-AFS-large 78.68 76.38\\nSBERT-AFS-large 77.85 75.93\\nCross-Topic Evaluation\\nBERT-AFS-base 58.49 57.23\\nSBERT-AFS-base 52.34 50.65\\nBERT-AFS-large 62.02 60.34\\nSBERT-AFS-large 53.82 53.10\\nTable 3: Average Pearson correlation r and average\\nSpearman’s rank correlation ρon the Argument Facet\\nSimilarity (AFS) corpus (Misra et al., 2016). Misra et\\nal. proposes 10-fold cross-validation. We additionally\\nevaluate in a cross-topic scenario: Methods are trained\\non two topics, and are evaluated on the third topic.\\ntences in the same section are thematically closer\\nthan sentences in different sections. They use this\\nto create a large dataset of weakly labeled sen-\\ntence triplets: The anchor and the positive exam-\\nple come from the same section, while the neg-\\native example comes from a different section of\\nthe same article. For example, from the Alice\\nArnold article: Anchor: Arnold joined the BBC\\nRadio Drama Company in 1988., positive: Arnold\\ngained media attention in May 2012., negative:\\nBalding and Arnold are keen amateur golfers.\\nWe use the dataset from Dor et al. We use the\\nTriplet Objective, train SBERT for one epoch on\\nthe about 1.8 Million training triplets and evaluate\\nit on the 222,957 test triplets. Test triplets are from\\na distinct set of Wikipedia articles. As evaluation\\nmetric, we use accuracy: Is the positive example\\ncloser to the anchor than the negative example?\\nResults are presented in Table 4. Dor et al. ﬁne-\\ntuned a BiLSTM architecture with triplet loss to\\nderive sentence embeddings for this dataset. As\\nthe table shows, SBERT clearly outperforms the\\nBiLSTM approach by Dor et al.\\n5 Evaluation - SentEval\\nSentEval (Conneau and Kiela, 2018) is a popular\\ntoolkit to evaluate the quality of sentence embed-\\ndings. Sentence embeddings are used as features\\nfor a logistic regression classiﬁer. The logistic re-\\ngression classiﬁer is trained on various tasks in a\\n10-fold cross-validation setup and the prediction\\naccuracy is computed for the test-fold.\\nModel Accuracy\\nmean-vectors 0.65\\nskip-thoughts-CS 0.62\\nDor et al. 0.74\\nSBERT-WikiSec-base 0.8042\\nSBERT-WikiSec-large 0.8078\\nSRoBERTa-WikiSec-base 0.7945\\nSRoBERTa-WikiSec-large 0.7973\\nTable 4: Evaluation on the Wikipedia section triplets\\ndataset (Dor et al., 2018). SBERT trained with triplet\\nloss for one epoch.\\nThe purpose of SBERT sentence embeddings\\nare not to be used for transfer learning for other\\ntasks. Here, we think ﬁne-tuning BERT as de-\\nscribed by Devlin et al. (2018) for new tasks is\\nthe more suitable method, as it updates all layers\\nof the BERT network. However, SentEval can still\\ngive an impression on the quality of our sentence\\nembeddings for various tasks.\\nWe compare the SBERT sentence embeddings\\nto other sentence embeddings methods on the fol-\\nlowing seven SentEval transfer tasks:\\n• MR: Sentiment prediction for movie reviews\\nsnippets on a ﬁve start scale (Pang and Lee,\\n2005).\\n• CR: Sentiment prediction of customer prod-\\nuct reviews (Hu and Liu, 2004).\\n• SUBJ: Subjectivity prediction of sentences\\nfrom movie reviews and plot summaries\\n(Pang and Lee, 2004).\\n• MPQA: Phrase level opinion polarity classi-\\nﬁcation from newswire (Wiebe et al., 2005).\\n• SST: Stanford Sentiment Treebank with bi-\\nnary labels (Socher et al., 2013).\\n• TREC: Fine grained question-type classiﬁ-\\ncation from TREC (Li and Roth, 2002).\\n• MRPC: Microsoft Research Paraphrase Cor-\\npus from parallel news sources (Dolan et al.,\\n2004).\\nThe results can be found in Table 5. SBERT\\nis able to achieve the best performance in 5 out\\nof 7 tasks. The average performance increases\\nby about 2 percentage points compared to In-\\nferSent as well as the Universal Sentence Encoder.\\nEven though transfer learning is not the purpose of\\nSBERT, it outperforms other state-of-the-art sen-\\ntence embeddings methods on this task.'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 6}, page_content='Model MR CR SUBJ MPQA SST TREC MRPC Avg.\\nAvg. GloVe embeddings 77.25 78.30 91.17 87.85 80.18 83.0 72.87 81.52\\nAvg. fast-text embeddings 77.96 79.23 91.68 87.81 82.15 83.6 74.49 82.42\\nAvg. BERT embeddings 78.66 86.25 94.37 88.66 84.40 92.8 69.45 84.94\\nBERT CLS-vector 78.68 84.85 94.21 88.23 84.13 91.4 71.13 84.66\\nInferSent - GloVe 81.57 86.54 92.50 90.38 84.18 88.2 75.77 85.59\\nUniversal Sentence Encoder 80.09 85.19 93.98 86.70 86.38 93.2 70.14 85.10\\nSBERT-NLI-base 83.64 89.43 94.39 89.86 88.96 89.6 76.00 87.41\\nSBERT-NLI-large 84.88 90.07 94.52 90.33 90.66 87.4 75.94 87.69\\nTable 5: Evaluation of SBERT sentence embeddings using the SentEval toolkit. SentEval evaluates sentence\\nembeddings on different sentence classiﬁcation tasks by training a logistic regression classiﬁer using the sentence\\nembeddings as features. Scores are based on a 10-fold cross-validation.\\nIt appears that the sentence embeddings from\\nSBERT capture well sentiment information: We\\nobserve large improvements for all sentiment tasks\\n(MR, CR, and SST) from SentEval in comparison\\nto InferSent and Universal Sentence Encoder.\\nThe only dataset where SBERT is signiﬁcantly\\nworse than Universal Sentence Encoder is the\\nTREC dataset. Universal Sentence Encoder was\\npre-trained on question-answering data, which ap-\\npears to be beneﬁcial for the question-type classi-\\nﬁcation task of the TREC dataset.\\nAverage BERT embeddings or using the CLS-\\ntoken output from a BERT network achieved bad\\nresults for various STS tasks (Table 1), worse than\\naverage GloVe embeddings. However, for Sent-\\nEval, average BERT embeddings and the BERT\\nCLS-token output achieves decent results (Ta-\\nble 5), outperforming average GloVe embeddings.\\nThe reason for this are the different setups. For\\nthe STS tasks, we used cosine-similarity to es-\\ntimate the similarities between sentence embed-\\ndings. Cosine-similarity treats all dimensions\\nequally. In contrast, SentEval ﬁts a logistic regres-\\nsion classiﬁer to the sentence embeddings. This\\nallows that certain dimensions can have higher or\\nlower impact on the classiﬁcation result.\\nWe conclude that average BERT embeddings /\\nCLS-token output from BERT return sentence em-\\nbeddings that are infeasible to be used with cosine-\\nsimilarity or with Manhatten / Euclidean distance.\\nFor transfer learning, they yield slightly worse\\nresults than InferSent or Universal Sentence En-\\ncoder. However, using the described ﬁne-tuning\\nsetup with a siamese network structure on NLI\\ndatasets yields sentence embeddings that achieve\\na new state-of-the-art for the SentEval toolkit.\\n6 Ablation Study\\nWe have demonstrated strong empirical results for\\nthe quality of SBERT sentence embeddings. In\\nthis section, we perform an ablation study of dif-\\nferent aspects of SBERT in order to get a better\\nunderstanding of their relative importance.\\nWe evaluated different pooling strategies\\n(MEAN, MAX, and CLS). For the classiﬁcation\\nobjective function, we evaluate different concate-\\nnation methods. For each possible conﬁguration,\\nwe train SBERT with 10 different random seeds\\nand average the performances.\\nThe objective function (classiﬁcation vs. regres-\\nsion) depends on the annotated dataset. For the\\nclassiﬁcation objective function, we train SBERT-\\nbase on the SNLI and the Multi-NLI dataset. For\\nthe regression objective function, we train on the\\ntraining set of the STS benchmark dataset. Perfor-\\nmances are measured on the development split of\\nthe STS benchmark dataset. Results are shown in\\nTable 6.\\nNLI STSb\\nPooling Strategy\\nMEAN 80.78 87.44\\nMAX 79.07 69.92\\nCLS 79.80 86.62\\nConcatenation\\n(u,v) 66.04 -\\n(|u−v|) 69.78 -\\n(u∗v) 70.54 -\\n(|u−v|,u ∗v) 78.37 -\\n(u,v,u ∗v) 77.44 -\\n(u,v, |u−v|) 80.78 -\\n(u,v, |u−v|,u ∗v) 80.44 -\\nTable 6: SBERT trained on NLI data with the clas-\\nsiﬁcation objective function, on the STS benchmark\\n(STSb) with the regression objective function. Con-\\nﬁgurations are evaluated on the development set of the\\nSTSb using cosine-similarity and Spearman’s rank cor-\\nrelation. For the concatenation methods, we only report\\nscores with MEAN pooling strategy.\\nWhen trained with the classiﬁcation objective\\nfunction on NLI data, the pooling strategy has a\\nrather minor impact. The impact of the concate-\\nnation mode is much larger. InferSent (Conneau'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 7}, page_content='et al., 2017) and Universal Sentence Encoder (Cer\\net al., 2018) both use (u,v, |u−v|,u ∗v) as input\\nfor a softmax classiﬁer. However, in our architec-\\nture, adding the element-wise u∗vdecreased the\\nperformance.\\nThe most important component is the element-\\nwise difference |u−v|. Note, that the concate-\\nnation mode is only relevant for training the soft-\\nmax classiﬁer. At inference, when predicting sim-\\nilarities for the STS benchmark dataset, only the\\nsentence embeddings uand v are used in combi-\\nnation with cosine-similarity. The element-wise\\ndifference measures the distance between the di-\\nmensions of the two sentence embeddings, ensur-\\ning that similar pairs are closer and dissimilar pairs\\nare further apart.\\nWhen trained with the regression objective\\nfunction, we observe that the pooling strategy has\\na large impact. There, the MAX strategy perform\\nsigniﬁcantly worse than MEAN or CLS-token strat-\\negy. This is in contrast to (Conneau et al., 2017),\\nwho found it beneﬁcial for the BiLSTM-layer of\\nInferSent to use MAX instead of MEAN pooling.\\n7 Computational Efﬁciency\\nSentence embeddings need potentially be com-\\nputed for Millions of sentences, hence, a high\\ncomputation speed is desired. In this section, we\\ncompare SBERT to average GloVe embeddings,\\nInferSent (Conneau et al., 2017), and Universal\\nSentence Encoder (Cer et al., 2018).\\nFor our comparison we use the sentences from\\nthe STS benchmark (Cer et al., 2017). We com-\\npute average GloVe embeddings using a sim-\\nple for-loop with python dictionary lookups and\\nNumPy. InferSent 4 is based on PyTorch. For\\nUniversal Sentence Encoder, we use the Tensor-\\nFlow Hub version 5, which is based on Tensor-\\nFlow. SBERT is based on PyTorch. For improved\\ncomputation of sentence embeddings, we imple-\\nmented a smart batching strategy: Sentences with\\nsimilar lengths are grouped together and are only\\npadded to the longest element in a mini-batch.\\nThis drastically reduces computational overhead\\nfrom padding tokens.\\nPerformances were measured on a server with\\nIntel i7-5820K CPU @ 3.30GHz, Nvidia Tesla\\n4https://github.com/facebookresearch/\\nInferSent\\n5https://tfhub.dev/google/\\nuniversal-sentence-encoder-large/3\\nV100 GPU, CUDA 9.2 and cuDNN. The results\\nare depicted in Table 7.\\nModel CPU GPU\\nAvg. GloVe embeddings 6469 -\\nInferSent 137 1876\\nUniversal Sentence Encoder 67 1318\\nSBERT-base 44 1378\\nSBERT-base - smart batching 83 2042\\nTable 7: Computation speed (sentences per second) of\\nsentence embedding methods. Higher is better.\\nOn CPU, InferSent is about 65% faster than\\nSBERT. This is due to the much simpler net-\\nwork architecture. InferSent uses a single Bi-\\nLSTM layer, while BERT uses 12 stacked trans-\\nformer layers. However, an advantage of trans-\\nformer networks is the computational efﬁciency\\non GPUs. There, SBERT with smart batching\\nis about 9% faster than InferSent and about 55%\\nfaster than Universal Sentence Encoder. Smart\\nbatching achieves a speed-up of 89% on CPU and\\n48% on GPU. Average GloVe embeddings is obvi-\\nously by a large margin the fastest method to com-\\npute sentence embeddings.\\n8 Conclusion\\nWe showed that BERT out-of-the-box maps sen-\\ntences to a vector space that is rather unsuit-\\nable to be used with common similarity measures\\nlike cosine-similarity. The performance for seven\\nSTS tasks was below the performance of average\\nGloVe embeddings.\\nTo overcome this shortcoming, we presented\\nSentence-BERT (SBERT). SBERT ﬁne-tunes\\nBERT in a siamese / triplet network architec-\\nture. We evaluated the quality on various com-\\nmon benchmarks, where it could achieve a sig-\\nniﬁcant improvement over state-of-the-art sen-\\ntence embeddings methods. Replacing BERT with\\nRoBERTa did not yield a signiﬁcant improvement\\nin our experiments.\\nSBERT is computationally efﬁcient. On a GPU,\\nit is about 9% faster than InferSent and about 55%\\nfaster than Universal Sentence Encoder. SBERT\\ncan be used for tasks which are computationally\\nnot feasible to be modeled with BERT. For exam-\\nple, clustering of 10,000 sentences with hierarchi-\\ncal clustering requires with BERT about 65 hours,\\nas around 50 Million sentence combinations must\\nbe computed. With SBERT, we were able to re-\\nduce the effort to about 5 seconds.'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 8}, page_content='Acknowledgments\\nThis work has been supported by the German\\nResearch Foundation through the German-Israeli\\nProject Cooperation (DIP, grant DA 1600/1-1 and\\ngrant GU 798/17-1). It has been co-funded by the\\nGerman Federal Ministry of Education and Re-\\nsearch (BMBF) under the promotional references\\n03VP02540 (ArgumenText).\\nReferences\\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel\\nCer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei\\nGuo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada\\nMihalcea, German Rigau, Larraitz Uria, and Janyce\\nWiebe. 2015. SemEval-2015 Task 2: Semantic Tex-\\ntual Similarity, English, Spanish and Pilot on Inter-\\npretability. In Proceedings of the 9th International\\nWorkshop on Semantic Evaluation (SemEval 2015),\\npages 252–263, Denver, Colorado. Association for\\nComputational Linguistics.\\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel\\nCer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei\\nGuo, Rada Mihalcea, German Rigau, and Janyce\\nWiebe. 2014. SemEval-2014 Task 10: Multilingual\\nSemantic Textual Similarity. In Proceedings of the\\n8th International Workshop on Semantic Evaluation\\n(SemEval 2014), pages 81–91, Dublin, Ireland. As-\\nsociation for Computational Linguistics.\\nEneko Agirre, Carmen Banea, Daniel M. Cer, Mona T.\\nDiab, Aitor Gonzalez-Agirre, Rada Mihalcea, Ger-\\nman Rigau, and Janyce Wiebe. 2016. SemEval-\\n2016 Task 1: Semantic Textual Similarity, Mono-\\nlingual and Cross-Lingual Evaluation. In Proceed-\\nings of the 10th International Workshop on Seman-\\ntic Evaluation, SemEval@NAACL-HLT 2016, San\\nDiego, CA, USA, June 16-17, 2016, pages 497–511.\\nEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-\\nAgirre, and Weiwei Guo. 2013. *SEM 2013 shared\\ntask: Semantic Textual Similarity. In Second Joint\\nConference on Lexical and Computational Seman-\\ntics (*SEM), Volume 1: Proceedings of the Main\\nConference and the Shared Task: Semantic Textual\\nSimilarity, pages 32–43, Atlanta, Georgia, USA. As-\\nsociation for Computational Linguistics.\\nEneko Agirre, Mona Diab, Daniel Cer, and Aitor\\nGonzalez-Agirre. 2012. SemEval-2012 Task 6: A\\nPilot on Semantic Textual Similarity. In Proceed-\\nings of the First Joint Conference on Lexical and\\nComputational Semantics - Volume 1: Proceedings\\nof the Main Conference and the Shared Task, and\\nVolume 2: Proceedings of the Sixth International\\nWorkshop on Semantic Evaluation, SemEval ’12,\\npages 385–393, Stroudsburg, PA, USA. Association\\nfor Computational Linguistics.\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn Proceedings of the 2015 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n632–642, Lisbon, Portugal. Association for Compu-\\ntational Linguistics.\\nDaniel Cer, Mona Diab, Eneko Agirre, Iigo Lopez-\\nGazpio, and Lucia Specia. 2017. SemEval-2017\\nTask 1: Semantic Textual Similarity Multilingual\\nand Crosslingual Focused Evaluation. In Proceed-\\nings of the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1–14, Vancou-\\nver, Canada.\\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\\nNicole Limtiaco, Rhomni St. John, Noah Constant,\\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\\nYun-Hsuan Sung, Brian Strope, and Ray Kurzweil.\\n2018. Universal Sentence Encoder. arXiv preprint\\narXiv:1803.11175.\\nAlexis Conneau and Douwe Kiela. 2018. SentEval: An\\nEvaluation Toolkit for Universal Sentence Represen-\\ntations. arXiv preprint arXiv:1803.05449.\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo ¨ıc\\nBarrault, and Antoine Bordes. 2017. Supervised\\nLearning of Universal Sentence Representations\\nfrom Natural Language Inference Data. In Proceed-\\nings of the 2017 Conference on Empirical Methods\\nin Natural Language Processing, pages 670–680,\\nCopenhagen, Denmark. Association for Computa-\\ntional Linguistics.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2018. BERT: Pre-training of\\nDeep Bidirectional Transformers for Language Un-\\nderstanding. arXiv preprint arXiv:1810.04805.\\nBill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-\\nsupervised Construction of Large Paraphrase Cor-\\npora: Exploiting Massively Parallel News Sources.\\nIn Proceedings of the 20th International Confer-\\nence on Computational Linguistics, COLING ’04,\\nStroudsburg, PA, USA. Association for Computa-\\ntional Linguistics.\\nLiat Ein Dor, Yosi Mass, Alon Halfon, Elad Venezian,\\nIlya Shnayderman, Ranit Aharonov, and Noam\\nSlonim. 2018. Learning Thematic Similarity Metric\\nfrom Article Sections Using Triplet Networks. In\\nProceedings of the 56th Annual Meeting of the As-\\nsociation for Computational Linguistics (Volume 2:\\nShort Papers), pages 49–54, Melbourne, Australia.\\nAssociation for Computational Linguistics.\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen.\\n2016. Learning Distributed Representations of Sen-\\ntences from Unlabelled Data. In Proceedings of\\nthe 2016 Conference of the North American Chap-\\nter of the Association for Computational Linguis-\\ntics: Human Language Technologies, pages 1367–\\n1377, San Diego, California. Association for Com-\\nputational Linguistics.'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 9}, page_content='Minqing Hu and Bing Liu. 2004. Mining and Sum-\\nmarizing Customer Reviews. In Proceedings of the\\nTenth ACM SIGKDD International Conference on\\nKnowledge Discovery and Data Mining, KDD ’04,\\npages 168–177, New York, NY , USA. ACM.\\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\\nand Jason Weston. 2019. Real-time Inference\\nin Multi-sentence Tasks with Deep Pretrained\\nTransformers. arXiv preprint arXiv:1905.01969,\\nabs/1905.01969.\\nJeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2017.\\nBillion-scale similarity search with GPUs. arXiv\\npreprint arXiv:1702.08734.\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-Thought Vectors. In\\nC. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama,\\nand R. Garnett, editors, Advances in Neural Infor-\\nmation Processing Systems 28, pages 3294–3302.\\nCurran Associates, Inc.\\nXin Li and Dan Roth. 2002. Learning Question Classi-\\nﬁers. In Proceedings of the 19th International Con-\\nference on Computational Linguistics - Volume 1,\\nCOLING ’02, pages 1–7, Stroudsburg, PA, USA.\\nAssociation for Computational Linguistics.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoBERTa: A Robustly Optimized BERT Pretrain-\\ning Approach. arXiv preprint arXiv:1907.11692.\\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\\nBentivogli, Raffaella Bernardi, and Roberto Zam-\\nparelli. 2014. A SICK cure for the evaluation of\\ncompositional distributional semantic models. In\\nProceedings of the Ninth International Conference\\non Language Resources and Evaluation (LREC’14),\\npages 216–223, Reykjavik, Iceland. European Lan-\\nguage Resources Association (ELRA).\\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\\nBowman, and Rachel Rudinger. 2019. On Mea-\\nsuring Social Biases in Sentence Encoders. arXiv\\npreprint arXiv:1903.10561.\\nAmita Misra, Brian Ecker, and Marilyn A. Walker.\\n2016. Measuring the Similarity of Sentential Ar-\\nguments in Dialogue. In Proceedings of the SIG-\\nDIAL 2016 Conference, The 17th Annual Meeting\\nof the Special Interest Group on Discourse and Di-\\nalogue, 13-15 September 2016, Los Angeles, CA,\\nUSA, pages 276–287.\\nBo Pang and Lillian Lee. 2004. A Sentimental Educa-\\ntion: Sentiment Analysis Using Subjectivity Sum-\\nmarization Based on Minimum Cuts. In Proceed-\\nings of the 42nd Meeting of the Association for\\nComputational Linguistics (ACL’04), Main Volume,\\npages 271–278, Barcelona, Spain.\\nBo Pang and Lillian Lee. 2005. Seeing Stars: Exploit-\\ning Class Relationships for Sentiment Categoriza-\\ntion with Respect to Rating Scales. In Proceedings\\nof the 43rd Annual Meeting of the Association for\\nComputational Linguistics (ACL’05), pages 115–\\n124, Ann Arbor, Michigan. Association for Compu-\\ntational Linguistics.\\nJeffrey Pennington, Richard Socher, and Christo-\\npher D. Manning. 2014. GloVe: Global Vectors for\\nWord Representation. In Empirical Methods in Nat-\\nural Language Processing (EMNLP), pages 1532–\\n1543.\\nYifan Qiao, Chenyan Xiong, Zheng-Hao Liu, and\\nZhiyuan Liu. 2019. Understanding the Be-\\nhaviors of BERT in Ranking. arXiv preprint\\narXiv:1904.07531.\\nNils Reimers, Philip Beyer, and Iryna Gurevych. 2016.\\nTask-Oriented Intrinsic Evaluation of Semantic Tex-\\ntual Similarity. In Proceedings of the 26th Inter-\\nnational Conference on Computational Linguistics\\n(COLING), pages 87–96.\\nNils Reimers and Iryna Gurevych. 2018. Why Com-\\nparing Single Performance Scores Does Not Al-\\nlow to Draw Conclusions About Machine Learn-\\ning Approaches. arXiv preprint arXiv:1803.09578,\\nabs/1803.09578.\\nNils Reimers, Benjamin Schiller, Tilman Beck, Jo-\\nhannes Daxenberger, Christian Stab, and Iryna\\nGurevych. 2019. Classiﬁcation and Clustering of\\nArguments with Contextualized Word Embeddings.\\nIn Proceedings of the 57th Annual Meeting of the As-\\nsociation for Computational Linguistics, pages 567–\\n578, Florence, Italy. Association for Computational\\nLinguistics.\\nFlorian Schroff, Dmitry Kalenichenko, and James\\nPhilbin. 2015. FaceNet: A Uniﬁed Embedding for\\nFace Recognition and Clustering. arXiv preprint\\narXiv:1503.03832, abs/1503.03832.\\nRichard Socher, Alex Perelygin, Jean Wu, Jason\\nChuang, Christopher D. Manning, Andrew Ng, and\\nChristopher Potts. 2013. Recursive Deep Models for\\nSemantic Compositionality Over a Sentiment Tree-\\nbank. In Proceedings of the 2013 Conference on\\nEmpirical Methods in Natural Language Process-\\ning, pages 1631–1642, Seattle, Washington, USA.\\nAssociation for Computational Linguistics.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is All\\nyou Need. In I. Guyon, U. V . Luxburg, S. Bengio,\\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\\nnett, editors, Advances in Neural Information Pro-\\ncessing Systems 30, pages 5998–6008.\\nJanyce Wiebe, Theresa Wilson, and Claire Cardie.\\n2005. Annotating Expressions of Opinions and\\nEmotions in Language. Language Resources and\\nEvaluation, 39(2):165–210.'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 10}, page_content='Adina Williams, Nikita Nangia, and Samuel Bowman.\\n2018. A Broad-Coverage Challenge Corpus for\\nSentence Understanding through Inference. In Pro-\\nceedings of the 2018 Conference of the North Amer-\\nican Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Vol-\\nume 1 (Long Papers), pages 1112–1122. Association\\nfor Computational Linguistics.\\nYinfei Yang, Steve Yuan, Daniel Cer, Sheng-Yi Kong,\\nNoah Constant, Petr Pilar, Heming Ge, Yun-hsuan\\nSung, Brian Strope, and Ray Kurzweil. 2018.\\nLearning Semantic Textual Similarity from Conver-\\nsations. In Proceedings of The Third Workshop\\non Representation Learning for NLP, pages 164–\\n174, Melbourne, Australia. Association for Compu-\\ntational Linguistics.\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G.\\nCarbonell, Ruslan Salakhutdinov, and Quoc V . Le.\\n2019. XLNet: Generalized Autoregressive Pretrain-\\ning for Language Understanding. arXiv preprint\\narXiv:1906.08237, abs/1906.08237.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\\nWeinberger, and Yoav Artzi. 2019. BERTScore:\\nEvaluating Text Generation with BERT. arXiv\\npreprint arXiv:1904.09675.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                       bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                           class_= (\"post-title\", \"post-content\", \"post-header\")\n",
    "                       ))\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\nFig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Arxiv\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "docs = ArxivLoader(query=\"1605.08386\", load_max_docs = 2).load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2016-05-26', 'Title': 'Heat-bath random walks with Markov bases', 'Authors': 'Caprice Stanley, Tobias Windisch', 'Summary': 'Graphs on lattice points are studied whose edges come from a finite set of\\nallowed moves of arbitrary length. We show that the diameter of these graphs on\\nfibers of a fixed integer matrix can be bounded from above by a constant. We\\nthen study the mixing behaviour of heat-bath random walks on these graphs. We\\nalso state explicit conditions on the set of moves so that the heat-bath random\\nwalk, a generalization of the Glauber dynamics, is an expander in fixed\\ndimension.'}, page_content='arXiv:1605.08386v1  [math.CO]  26 May 2016\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nAbstract. Graphs on lattice points are studied whose edges come from a ﬁnite set of\\nallowed moves of arbitrary length. We show that the diameter of these graphs on ﬁbers of a\\nﬁxed integer matrix can be bounded from above by a constant. We then study the mixing\\nbehaviour of heat-bath random walks on these graphs. We also state explicit conditions\\non the set of moves so that the heat-bath random walk, a generalization of the Glauber\\ndynamics, is an expander in ﬁxed dimension.\\nContents\\n1.\\nIntroduction\\n1\\n2.\\nGraphs and statistics\\n3\\n3.\\nBounds on the diameter\\n4\\n4.\\nHeat-bath random walks\\n8\\n5.\\nAugmenting Markov bases\\n14\\nReferences\\n19\\n1. Introduction\\nA ﬁber graph is a graph on the ﬁnitely many lattice points F ⊂Zd of a polytope where\\ntwo lattice points are connected by an edge if their diﬀerence lies in a ﬁnite set of allowed\\nmoves M ⊂Zd. The implicit structure of these graphs makes them a useful tool to explore\\nthe set of lattice points randomly: At the current lattice point u ∈F, an element m ∈±M\\nis sampled and the random walk moves along m if u + m ∈F and stays at u otherwise.\\nThe corresponding Markov chain is irreducible if the underlying ﬁber graph is connected and\\nthe set M is called a Markov basis for F in this case. This paper investigates the heat-bath\\nversion of this random walk: At the current lattice point u ∈F, we sample m ∈M and move\\nto a random element in the integer ray (u + Z · m) ∩F. The authors of [6] discovered that\\nthis random walk can be seen as a discrete version of the hit-and-run algorithm [15, 26, 16]\\nthat has been used frequently to sample from all the points of a polytope – not only from its\\nlattice points. The popularity of the continuous version of the hit-and-run algorithm has not\\nspread to its discrete analog, and not much is known about its mixing behaviour. One reason\\nis that it is already challenging to guarantee that all points in the underlying set F can be\\nDate: September 12, 2018.\\n2010 Mathematics Subject Classiﬁcation. Primary: 05C81, Secondary: 37A25, 11P21.\\nKey words and phrases. Heat-bath random walks, sampling, lattice points, Markov bases.\\n1\\n2\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nreached by a random walk that uses moves from M, whereas for the continuous version, a\\nrandom sampling from the unit sphere suﬃces. However, in many situations where a Markov\\nbasis is known, the heat-bath random walk is evidently fast. For instance, it was shown in [3]\\nthat the heat-bath random walk on contingency tables mixes rapidly when the number of\\ncolumns is ﬁxed. To work around the connectedness issue, a discrete hit-and-run algorithm\\nwas introduced in [1] for arbitrary ﬁnite sets F ⊂Zd. At each step in this random walk, a\\nsubordinate and unrestricted random walk starts at the current lattice point u ∈F and uses\\nthe unit vectors to collect a set of proposals S ⊂Zd. The random walk then moves from u\\nto a random point in S ∩F.\\nRandom walks of the heat-bath type, such as the one presented above, have been studied\\nrecently in [8] in a more general context. In this paper, we explore the mixing behaviour of\\nheat-bath random walks on the lattice points of polytopes with Markov bases. Throughout,\\nwe assume that a Markov basis has been found already and refer to the relevant literature\\nfor their computation [24, 25, 11, 17, 10, 21]. We call the underlying graph of the heat-bath\\nrandom walk a compressed ﬁber graph (Deﬁnition 2.5) and determine in Section 3 bounds on\\nits graph-diameter. We prove that for any A ∈Zm×d with kerZ(A)∩Nd = {0}, the diameter of\\ncompressed ﬁber graphs on {u ∈Nd : Au = b} that use a ﬁxed Markov bases M ⊂kerZ(A) is\\nbounded from above by a constant as b varies (Theorem 3.15). In contrast, we show that the\\ndiameter of conventional ﬁber graphs grow linearly under a dilation of the underlying polytope\\n(Remark 3.9). This gives rise to slow mixing results for conventional ﬁber walks as observed\\nin [27]. In Section 4, we study in more detail the combinatorial and analytical structure\\nof the transition matrices of heat-bath random walks on lattice points and prove upper and\\nlower bounds on their second largest eigenvalues. We also discuss how the distribution on the\\nmoves M aﬀects the speed of convergence (Example 4.21). Theorem 5.8 establishes with the\\ncanonical path approach from [23] an upper bound on the second largest eigenvalue when the\\nMarkov basis is augmenting (Deﬁnition 5.1) and the stationary distribution is uniform. From\\nthat, we conclude fast mixing results for random walks on lattice points in ﬁxed dimension.\\nAcknowledgements. CS was partially supported by the US National Science Foundation\\n(DMS 0954865). TW gratefully acknowledges the support received from the German National\\nAcademic Foundation.\\nConventions and Notation. The natural numbers are N := {0, 1, 2, . . .} and for any N ∈N,\\nN>N := {n ∈N : n > N} and N≥N := {N} ∪N>N. For n ∈N>0, let [n] := {1, . . . , n}. Let\\nM ⊂Qd be a ﬁnite set, then Z·M := {λm : m ∈M, λ ∈Z} and NM is the aﬃne semigroup\\nin Zd generated by M. For an integer matrix A ∈Zm×d with columns a1, . . . , ad ∈Zm,\\nwe write NA := N{a1, . . . , ad}. A graph is always undirected and can have multiple loops.\\nThe distance of two nodes u, v which are contained in the same connected component of a\\ngraph G, i.e. the number of edges in a shortest path between u and v in G, is denoted by\\ndistG(u, v). We set distG(u, v) := ∞if u and v are disconnected. A mass function on a ﬁnite\\nset Ωis a map f : Ω→[0, 1] such that P\\nω∈Ωf(ω) = 1. A mass function f on Ωis positive\\nif f(ω) > 0 for all ω ∈Ω. A set F ⊂Zd is normal if it there exists a polytope P ⊂Qd such\\nthat P ∩Zd = F.\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n3\\n2. Graphs and statistics\\nWe ﬁrst introduce the statistical framework in which this paper lives and recall important\\naspects of the interplay between graphs and statistics. A random walk on a graph G = (V, E)\\nis a map H : V × V →[0, 1] such that for all v ∈V , P\\nu∈V H(v, u) = 1 and such that\\nH(v, u) = 0 if {v, u} ̸∈E. When there is no ambiguity, we represent a random walk as an\\n|V | × |V |-matrix, for example when it is clear how the elements of V are ordered. Fix a\\nrandom walk H on G. Then H is irreducible if for all v, u ∈V there exists t ∈N such that\\nHt(v, u) > 0. The random walk H is reversible if there exists a mass function µ : V →[0, 1]\\nsuch that µ(u) · H(u, v) = µ(v) · H(v, u) for all u, v ∈V and symmetric if H is a symmetric\\nmap. A mass function π : V →[0, 1] is a stationary distribution of H if π ◦H = π. For\\nsymmetric random walks, the uniform distribution on V is always a stationary distribution.\\nIf |V | = n, then we denote the eigenvalues of H by 1 = λ1(H) ≥λ2(H) ≥· · · ≥λn(H) ≥−1\\nand we write λ(H) := max{λ2(H), −λn(H)} for the second largest eigenvalue modulus of H.\\nAny irreducible random walk has a unique stationary distribution [14, Corollary 1.17] and\\nλ(H) ∈[0, 1] measures the convergence rate: the smaller λ(H), the faster the convergence.\\nThe aim of this paper is to study random walks on lattice points that use a set of moves.\\nTypically, this is achieved by constructing a graph on the set of lattice points as follows\\n(compare to [7, Section 1.3] and [24, Chapter 5]).\\nDeﬁnition 2.1. Let F ⊂Zd be a ﬁnite set and M ⊂Zd. The graph F(M) is the graph on\\nF where two nodes u, v ∈F are adjacent if u −v ∈M or v −u ∈M.\\nA normal set F ⊂Zd is ﬁnite and satisﬁes F = convQ(F)∩Zd. A canonical class of normal\\nsets that arise in many applications, is given by the ﬁbers of an integer matrix:\\nDeﬁnition 2.2. Let A ∈Zm×d and b ∈NA. The set FA,b := {u ∈Nd : Au = b} is the b-ﬁber\\nof A. The collection of all ﬁbers of A is PA := {FA,b : b ∈NA}. For M ⊂kerZ(A), the graph\\nFA,b (M) is a ﬁber graph.\\nLet F, M ⊂Zd be ﬁnite. If the membership in F can be veriﬁed eﬃciently – for instance\\nwhen F is given implicitly by linear equations and inequalities – then it is possible to explore\\nF randomly using M as follows: At a given node v ∈F, a uniform element m ∈M is\\nselected. If v + m ∈M, then the random walk moves along m to v + m and if v + m ̸∈M,\\nthe we stay at v. Formally, we obtain the following random walk.\\nDeﬁnition 2.3. Let F ⊂Zd and M ⊂Zd be two ﬁnite sets. The simple walk is the random\\nwalk on F(M) where the probability to traverse between to adjacent nodes u and v is |±M|−1\\nand the probability to stay at a node u is |{m ∈±M : u + m ̸∈F}| · | ± M|−1.\\nThe simple walk is symmetric and hence the uniform distribution is a stationary distribu-\\ntion (see also [27, Section 2]). To ensure convergence, the random walk has to be irreducible,\\nthat is, the underlying graph has to be connected. The following deﬁnition is a slight adaption\\nof the generalized Markov basis as deﬁned in [21, Deﬁnition 1].\\nDeﬁnition 2.4. Let P be a collection of ﬁnite subsets of Zd. A ﬁnite set M ⊂Zd is a\\nMarkov basis of P, if for all F ∈P, F(M) is a connected graph.\\n4\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nWe refer to [6, Theorem 3.1] for a proof that for collections PA, a ﬁnite Markov basis\\nalways exists and can be computed with tools from commutative algebra (see also [11] for\\nmore on the computation of Markov bases). We now introduce a construction of graphs on\\nlattice points that also give rise to implementable random walks, but whose edges have far\\nmore reach.\\nDeﬁnition 2.5. Let F ⊂Zd and M ⊂Zd be ﬁnite sets. The compression of the graph\\nF(M) is the graph Fc(M) := F(Z · M).\\nFigure 1. Compressing graphs.\\nCompressing a graph F(M) preserves its connectedness: F(M) is connected if and only\\nif Fc(M) is connected.\\n3. Bounds on the diameter\\nIn general knowledge of the diameter of the graph underlying a Markov chain can provide\\ninformation about the mixing time. For random walks on ﬁber graphs, the chains which we\\nconsider, the underlying graph coincides with the ﬁber graph. In this section, we determine\\nlower and upper bounds on the diameter of ﬁber graphs and their compressed counterparts.\\nFor a ﬁnite set M ⊂Zd and any norm ∥· ∥on Rd, let ∥M∥:= maxm∈M ∥m∥.\\nLemma 3.1. Let F ⊂Zd and M ⊂Zd be ﬁnite sets, then\\ndiam(F(M)) ≥\\n1\\n∥M∥· max{∥u −v∥: u, v ∈F}.\\nProof. If F(M) is not connected, then the statement holds trivially, so assume that M is a\\nMarkov basis for F. Let u′, v′ ∈F such that ∥u′ −v′∥= max{∥u −v∥: u, v ∈F} and let\\nm1, . . . , mr ∈M so that u′ = v′+Pr\\ni=1 mi is a path of minimal length, then ∥u′−v′∥≤r·∥M∥\\nand the claim follows from diam(F(M)) ≥distF(M)(u′, v′) = r.\\n□\\nRemark 3.2. Let F ⊂Zd be a normal set. For all l ∈{−1, 0, 1}d and u, v ∈F we have\\n(u −v)T l ≤∥u −v∥1 and thus widthl(F) := max{(u −v)T l : u, v ∈F} ≤max{∥u −v∥1 :\\nu, v ∈F}. Suppose that u′, v′ ∈F are such that ∥u′ −v′∥1 = max{∥u −v∥1 : u, v ∈F} and\\nlet l′\\ni := sign(u′\\ni −v′\\ni) for i ∈[d], then\\n∥u′ −v′∥1 = (u′ −v′)T · l′ ≤widthl′(F) ≤max{∥u −v∥1 : u, v ∈F} = ∥u′ −v′∥1.\\nThe lattice width of F is width(F) := minl∈Zd widthl(F) and thus Lemma 3.1 gives\\n∥M∥1 · diam(F(M)) ≥width(F).\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n5\\nDeﬁnition 3.3. Let P be a collection of ﬁnite subsets of Zd.\\nA ﬁnite set M ⊂Zd is\\nnorm-like for P if there exists a constant C ∈N such that for all F ∈P and all u, v ∈F,\\ndistF(M)(u, v) ≤C · ∥u −v∥. The set M is ∥· ∥-norm-reducing for P if for all F ∈P and all\\nu, v ∈F there exists m ∈M such that u + m ∈F and ∥u + m −v∥< ∥u −v∥.\\nThe property of being norm-like does not depend on the norm, whereas being norm-\\nreducing does.\\nNorm-reducing sets are always norm-like, and norm-like sets are in turn\\nalways Markov bases, but the reverse of both statements is false in general (Example 3.4 and\\nExample 3.5). For collections PA however, every Markov basis is norm-like (Proposition 3.7).\\nExample 3.4. For any n ∈N, consider the normal set Fn := ([2]×[n]×{0})∪{(2, n, 1)} with\\nthe Markov basis {(0, 1, 0), (0, 0, 1), (−1, 0, −1)}. The distance between (1, 1, 0) and (2, 1, 0)\\nin Fn(M) is 2n and thus M is not norm-like for {Fn : n ∈N} (see also Figure 2).\\nExample 3.5. Let d ∈N and consider A := (1, . . . , 1) ∈Z1×d, then the set M := {e1 −ei :\\n2 ≤i ≤d} is a Markov basis for the collection PA. However, M is not ∥·∥p-norm-reducing for\\nany d ≥3 and any p ∈[1, ∞]. For instance, consider e2 and e3 in FA,1 (M). The only move\\nfrom M that can be applied on e2 is e1−e2, but ∥(e2+e1−e2)−e3)∥p = ∥e2−e3∥p. On the other\\nhand, in the case we cannot ﬁnd a move that decreases the 1-norm of two nodes u, v ∈FA,b\\nby 1, we can ﬁnd instead two moves m1, m2 ∈M such that u + m1, u + m1 + m2 ∈FA,b and\\n∥u + m1 + m2 −v∥= ∥u −v∥−2. Thus, the graph-distance of any two elements u and v in\\nFA,b (M) is at most ∥u −v∥1 and hence M is norm-like for PA.\\nFigure 2. The graph from Example 3.4\\nRemark 3.6. Let P be a collection of ﬁnite subsets of Zd and M ⊂Zd be norm-like for P.\\nIt follows from the deﬁnition that there exists a constant C ∈Q≥0 such that for all F ∈P\\ndiam(F(M)) ≤C · max{∥u −v∥: u, v ∈F}.\\nThe proof of our next results uses the Graver basis GA ⊂Zd for an integer matrix A ∈Zm×d\\nwith kerZ(A) ∩Nd = {0}. We refer to [4, Chapter 3] for a precise deﬁnition.\\nProposition 3.7. Let A ∈Zm×d with kerZ(A) ∩Nd = {0} and M ⊂kerZ(A) be a Markov\\nbasis of PA. Then M is norm-like for PA.\\nProof. Let M be a Markov basis for PA. The Graver basis GA for A is a ﬁnite set which\\nis ∥· ∥1-norm-reducing for PA. Thus, deﬁne C := maxg∈GA diam(FA,Ag+ (M)). Now, pick\\nu, v ∈FA,b arbitrarily and let u = v + Pr\\ni=1 gi be a walk from u to v in FA,b (GA) of minimal\\nlength. Since the Graver basis is norm-reducing for FA,b, there always exists a path of length\\nat most ∥u −v∥1 and hence r ≤∥u −v∥1. Every gi can be replaced by a path in FA,Ag+\\ni (M)\\n6\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nof length at most C and these paths stay in FA,b. This gives a path of length C · r, hence\\ndistFA,b(M)(u, v) ≤C∥u −v∥1.\\n□\\nProposition 3.8. Let P ⊂Zd be a polytope with dim(P ∩Zd) > 0 and let M be a Markov\\nbasis for Fi := (i · P) ∩Zd for all i ∈N. There exists a constant C′ ∈Q>0 such that for all\\ni ∈N, C′ · i ≤diam(Fi(M)). If M is norm-like for {Fi : i ∈N}, then there exists a constant\\nC ∈Q>0 such that diam(Fi(M)) ≤C · i for all i ∈N.\\nProof. For the lower bound on the diameter, it suﬃces to show the existence of C′ such that\\nC′ · i ≤max{∥u −v∥: u, v ∈Fi} for all i ∈N due to Lemma 3.1. Since dim(P ∩Zd) > 0,\\nwe can pick distinct w, w′ ∈P ∩Zd. For all i ∈N, i · w, i · w′ ∈Fi and hence i · ∥w −w′∥≤\\nmax{∥u −v∥: u, v ∈Fi}.\\nTo show the upper bound, assume that M is norm-like. It suﬃces to show that there\\nexists C ∈Q≥0 such that max{∥u −v∥: u, v ∈Fi} ≤i · C by Remark 3.6.\\nNow, let\\nv1, . . . , vr ∈Qd such that P = convQ(v1, . . . , vr) and deﬁne C := max{∥vs −vt∥: s ̸= t}.\\nSince Fi = (i·P)∩Zd ⊂convQ(iv1, . . . , ivr) for all i ∈N, we have max{∥u−v∥: u, v ∈Fi} ≤\\nmax{∥ivs −ivt∥: s ̸= t} ≤C · i.\\n□\\nRemark 3.9. Let A ∈Zm×n with kerZ(A) ∩Nd = {0} and let M be a Markov basis for PA.\\nThen M is norm-like due to Proposition 3.7 and thus for all b ∈NA there exists C, C′ ∈Q≥0\\nsuch that\\ni · C′ ≤diam(FA,ib (M)) ≤i · C\\nfor all i ∈N. This generalizes for instance [20, Proposition 2.10] and [27, Example 4.7], where\\nlinear diameters on a ray in NA have been observed. This also implies that the construction\\nof expanders from [27, Section 4] works for every right-hand side b ∈NA.\\nRemark 3.10. Let A ∈Zm×d with kerZ(A)∩Nd = {0}, b ∈NA, and let M be a Markov basis\\nfor PA. Proposition 3.8 provides a new proof that the simple walk on (FA,ib (M))i∈N cannot\\nmix rapidly. The lower bound on the diameter from Proposition 3.8 implies, in general, the\\nfollowing upper bound on the edge-expansion (see for example [9, Proposition 1.30]):\\nh(FA,i·b (M)) ≤|M|\\n\\x12\\nexp\\n\\x12log |FA,i·b|\\nD · i\\n\\x13\\n−1\\n\\x13\\n.\\nIn particular, the edge-expansion cannot be bounded from below by Ω( 1\\np(i))i∈N for a polyno-\\nmial p ∈Q[t] and since (|FA,i·b|)i∈N ∈O(ir)i∈N, the simple walk cannot mix rapidly. In [27],\\nit was shown that the edge-expansion can be bounded from above by O(1\\ni )i∈N, which cannot\\nbe concluded from the upper expression.\\nWe now turn our attention to the diameter of compressed ﬁber graphs.\\nIn particular,\\nwe want to know for which collections of normal sets is their diameter bounded. In general,\\ncompressing a ﬁber graph does not necessarily have an eﬀect on the diameter (Example 3.11).\\nAlthough a low diameter is a necessary condition for good mixing, it is not suﬃcient. For\\ninstance, let Gn be the disjoint union of two complete graphs Kn connected by a single edge.\\nThen diam(Gn) = 3, but h(Gn) ≤1\\nn implies that the simple walk does not mix rapidly.\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n7\\nExample 3.11. For any n ∈N, let Fn := {(0, 0), (0, 1), (1, 1), (1, 2), . . . , (n, n)} ⊂Z2. The\\nunit vectors M = {e1, e2} are a Markov basis for {Fn : n ∈N}. However, Fc\\nn(M) = Fn(M)\\nand thus diam(Fc\\nn(M)) = diam(Fn(M)) = 2n is unbounded.\\nLemma 3.12. Let A ∈Zm×d and z ∈kerZ(A). There exists r ∈[2d −2], distinct elements\\ng1, . . . , gr ∈GA, and λ1, . . . , λr ∈N>0 such that z = Pr\\ni=1 λigi and gi ⊑z for all i ∈[r]\\nProof. This is [4, Lemma 3.2.3], although it only becomes clear from the original proof of [22,\\nTheorem 2.1] that the appearing elements are all distinct.\\n□\\nProposition 3.13. Let A ∈Zm×d and P :=\\n\\x08\\n{x ∈Zd : Ax = b, l ≤x ≤u} : l, u ∈Zd, b ∈Zm\\t\\n.\\nThen for all F ∈P, diam(Fc(GA)) ≤2d −2.\\nProof. Let s, t ∈{x ∈Zd : Ax = b, l ≤x ≤u}, then s−t ∈kerZ(A) and thus s = t+Pr\\ni=1 λigi\\nwith r ≤2d −2, λ1, . . . , λr ∈N>0, and distinct g1, . . . , gr ∈GA such that gi ⊑s −t according\\nto Lemma 3.12. It’s now a consequence from [4, Lemma 3.2.4] that all intermediate points\\nt + Pk\\ni=1 λigi for k ≤r are in {x ∈Zd : Ax = b, l ≤x ≤u}.\\n□\\nLemma 3.14. Let F ⊂Zd be ﬁnite and let Fi := (i · convQ(F)) ∩Zd for i ∈N. For all\\nu, v ∈F, distFc\\ni (M)(iu, iv) ≤distF(M)(u, v) for all i ∈N.\\nProof. The statement is trivially true if u and v are disconnected in F(M). Thus, assume\\nthe contrary and let u = v + Pk\\nj=1 mj with mj ∈M be a path in F(M) of length k =\\ndistF(M)(u, v) and let i ∈N. Clearly, i · u = i · v + i · Pk\\nj=1 mj = i · v + Pk\\nl=1 i · mj, so\\nit is left to prove that the elements traversed by this paths are in Fi. Let l ∈[k], since\\nv + Pl\\nj=1 mj ∈F, we have i · v + Pl\\nj=1 i · mj ∈i · F ⊆Fi. Hence, this is a path in Fc\\ni (M)\\nof length k = distF(M)(u, v).\\n□\\nWe are ready to prove that the diameter of compressed ﬁber graphs coming from an integer\\nmatrix can be bounded for all right-hand sides simultaneously.\\nTheorem 3.15. Let A ∈Zm×d with kerZ(A) ∩Nd = {0} and let M be a Markov basis for\\nPA. There exists a constant C ∈N such that diam(Fc(M)) ≤C for all F ∈PA.\\nProof. Our proof relies on basic properties of the Graver basis GA of A. For any g ∈GA,\\nlet Fg := FA,Ag+ and let K := max{distFg(M)(g+, g−) : g ∈GA}.\\nWe show that the\\ndiameter of any compressed ﬁber graph of A is bounded from above by (2d −2) · K. Let\\nb ∈NA arbitrary and choose elements u, v ∈FA,b. According to Proposition 3.13, there\\nexists r ∈[2d −2], g1, . . . , gr ∈GA and λ1, . . . , λr ∈Z such that u = v + Pr\\ni=1 λigi, and\\nv + Pl\\ni=1 λigi ∈Nd for all l ∈[r].\\nAccording to Lemma 3.14, for any i ∈[r] there are\\nmi\\n1, . . . , mi\\nki ∈M and α1, . . . , αki ∈Z such that λig+\\ni = λig−\\ni + Pki\\nj=1 αjmi\\nj is a path in the\\ncompression of FA,Aλig+\\ni (M) of length ki ≤K. Lifting these paths for every i ∈[r] yields a\\npath u = v + Pr\\ni=1\\nPki\\nj=1 αjmi\\nj in Fc\\nA,b (M) of length r · K ≤(2d −2) · K.\\n□\\n8\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\n4. Heat-bath random walks\\nIn this section, we establish the heat-bath random walk on compressed ﬁber graphs. We\\nrefer to [8] for a more general introduction on random walks of heat-bath type. Let F ⊂Zd\\nbe ﬁnite set. For any u ∈F and m ∈Zd, the ray in F through u along m is denoted by\\nRF,m(u) := (u + m · Z) ∩F. Additionally, given a mass function π : F →[0, 1], we deﬁne\\nHπ\\nF,m(x, y) :=\\n(\\nπ(y)\\nπ(RF,m(x))\\n, if y ∈RF,m(x)\\n0\\n, otherwise\\nfor x, y ∈F. For M ⊂Zd and a mass function f : M →[0, 1], the heat-bath random walk is\\n(4.1)\\nHπ,f\\nF,M =\\nX\\nm∈M\\nf(m) · Hπ\\nF,m.\\nThe underlying graph of the heat-bath random walk is the compression Fc(M) and in this\\nsection, we assume throughout that for all m ∈M and λ ∈Z \\\\ {−1, 1}, λ · m ̸∈M. Let us\\nﬁrst recall the basic properties of this random walk (compare also to [6, Lemma 2.2]).\\nAlgorithm 1 Heat-bath random walk on compressed ﬁber graphs\\nInput: F ⊂Zd, M ⊂Zd, v ∈F, mass functions f : M →[0, 1] and π : F →[0, 1], r ∈N\\n1: procedure HeatBath:\\n2:\\nv0 := v\\n3:\\nFOR s = 0; s = s + 1, s < r\\n4:\\nSample m ∈M according to f\\n5:\\nSample vs+1 ∈RF,m(vs) according to RF,m(vs) →[0, 1], y 7→\\nπ(y)\\nπ(RF,m(vs))\\n6:\\nRETURN v1, . . . , vr\\nProposition 4.1. Let F ⊂Zd and M ⊂Zd be ﬁnite sets. Let f : M →[0, 1] and π : F →\\n(0, 1) be mass functions. Then Hπ,f\\nF,M is aperiodic, has stationary distribution π, is reversible\\nwith respect to π, and all of its eigenvalues are non-negative. The random walk is irreducible\\nif and only if {m ∈M : f(m) > 0} is a Markov basis for F.\\nProof. Since for any u ∈F and any m ∈M, Hπ\\nF,m(u, u) > 0, there are halting states and\\nthus Hπ,f\\nF,M is aperiodic. By deﬁnition, π(x)Hπ\\nF,m(x, y) = π(y)Hπ\\nF,m(y, x) and thus Hπ,f\\nF,M\\nis reversible with respect to π and π is a stationary distribution.\\nThe statement on the\\neigenvalues is exactly [8, Lemma 1.2]. Let M′ = {m ∈M : f(m) > 0} and f ′ = f|M′, then\\nHπ,f\\nF,M = Hπ,f′\\nF,M′ and thus the heat-bath random walk is irreducible if and only if M′ is a\\nMarkov basis for F.\\n□\\nRemark 4.2. Analyzing the speed of convergence of random walks with second largest\\neigenvalues does not take the computation time of a single transition into account. From\\na computational point of view, the diﬀerence of the simple walk and the heat-bath random\\nwalk is Step 4 of Algorithm 1. However, we argue that Step 4 can be done eﬃciently in\\nmany cases. For instance, a hard normalizing constant of π cancels out. If π is the uniform\\ndistribution, then one needs to sample uniformly from RF,m(v) in Step 4, which can be done\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n9\\neﬃciently. If the input of Algorithm 1 is a normal set F = {u ∈Zd : Au ≤b} that is given\\nin H-representation, then the length of the ray RF,m(v) can be computed with a number of\\nrounding, division, and comparing operations that is linear in the number of rows of A.\\nThere are situations in which the heat-bath random walk provides no speed-up compared\\nwith the simple walk (Example 4.3). Intuitively, adding more moves to the set of allowed\\nmoves should improve the mixing time of the random walk. In general, however, this is not\\ntrue for the heat-bath walk (Example 4.4).\\nExample 4.3. For n ∈N, consider the normal set\\nFn :=\\n\\x1a\\x14\\n0\\n1\\n1\\n· · ·\\n1\\n1\\n0\\n0\\n· · ·\\n0\\n\\x15\\n,\\n\\x14\\n1\\n0\\n1\\n· · ·\\n1\\n0\\n1\\n0\\n· · ·\\n0\\n\\x15\\n, . . . ,\\n\\x14\\n1\\n1\\n· · ·\\n1\\n0\\n0\\n0\\n· · ·\\n0\\n1\\n\\x15\\x1b\\n⊂Q2×n.\\nIn the language of [7, Section 1.1], Fn is precisely the ﬁber of the 2 × n independence model\\nwhere row sums are (n −1, 1) and column sums are (1, 1, . . . , 1).\\nThe minimal Markov\\nbasis of the independence model, often referred to as the basic moves, is precisely the set\\nMn := {v −u : u, v ∈Fn} \\\\ {0}. In particular, the ﬁber graph Fn(Mn) is the complete\\ngraph on n nodes. All rays along basic moves have length 2 and thus the transition matrices\\nof the simple random walk and the heat-bath random walk coincide. There are n · (n −1)\\nmany basic moves and the transition matrix of both random walks is\\n1\\nn(n −1)\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n1\\n. . .\\n1\\n...\\n...\\n1\\n. . .\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fb+ (n(n −1) −n)\\nn(n −1)\\n· In.\\nThe second largest eigenvalue is 1 −\\n1\\nn−1 which implies that for n →∞, neither the simple\\nwalk nor the heat-bath random walk are rapidly mixing.\\nExample 4.4. Let F = [2] × [5] ⊂Z2, M = {e1, e2, 2e1 + e2}, and let π be the uniform\\ndistribution on F.\\nSince {e2, 2e1 + e2} is not a Markov basis for F, any mass function\\nf : M →[0, 1] must have f(e1) > 0 in order to make the corresponding heat-bath random\\nwalk irreducible.\\nComparing the second largest eigenvalue modulus of heat-bath random\\nwalks that sample from {e1, e2} and M uniformly, we obtain\\nλ\\n\\x121\\n2Hπ\\nF,e1 + 1\\n2Hπ\\nF,e2\\n\\x13\\n= 1\\n2 < 2\\n3 = λ\\n\\x121\\n3Hπ\\nF,e1 + 1\\n3Hπ\\nF,e2 + 1\\n3Hπ\\nF,2e1+e2\\n\\x13\\n.\\nSo, adding 2e1 + e2 to the set of allowed moves slows the walk down. This phenomenon does\\nnot appear for the simple walk on F, where the second largest eigenvalue modulus improves\\nfrom ≈0.905 to ≈0.888 when adding the move 2e1 + e2 to the Markov basis.\\n=\\n+\\n+\\nFigure 3. Decomposition of the graph in Example 4.4\\n10\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nRemark 4.5. Let F ⊂Zd be ﬁnite and M = {m1, . . . , md} ⊂Zd be a linearly independent\\nMarkov basis of F. If the moves are selected uniformly, then the heat-bath random walk on\\nF coincides with the Glauber dynamics on F. To see it, choose u ∈F and let\\nF′ := {λ ∈Zd : u + λ1m1 + · · · + λdmd ∈F}.\\nIt is easy to check that F′ is unique up to translation and depends only on F, u, and M.\\nSince the vectors in M are linearly independent, every element of F can be represented by\\na unique choice of coeﬃcients in F′. Thus, the heat-bath random walk on F using M is\\nequivalent to the heat-bath random walk on on F′ using the unit vectors as moves. For any\\nunit vector ei ∈Zd, the ray through an element v ∈F′ is {w ∈F : wj = vj∀j ̸= i} and this\\nis precisely the form desired in the Glauber dynamics [14, Section 3.3.2].\\nFor the remainder of this section, we primarily focus on heat-bath random walks Hπ,f\\nF,M\\nthat converge to the uniform distribution π on a ﬁnite, but not necessarily normal, set\\nF. We particularly aim for bounds on its second largest eigenvalue by making use of the\\ndecomposition from equation 4.1. Our ﬁrst observations consider its summands Hπ\\nF,m that\\ncan be well understood analytically (Proposition 4.6) and combinatorially (Proposition 4.7).\\nProposition 4.6. Let F ⊂Zd be a ﬁnite set, m ∈Zd, and π : F →[0, 1] be the uniform\\ndistribution. Let R1, . . . , Rk be the disjoint rays through F along m. Then\\n1. Hπ\\nF,m is symmetric and idempotent.\\n2. img(Hπ\\nF,m) = spanR\\nnP\\nx∈R1 ex, P\\nx∈R2 ex, . . . , P\\nx∈Rk ex\\no\\n.\\n3. ker(Hπ\\nF,m) = Lk\\ni=1 spanR {ex −ey : x, y ∈Ri, x ̸= y}.\\n4. rank(Hπ\\nF,m) = k and dim ker(Hπ\\nF,m) = |F| −k.\\n5. The spectrum of Hπ\\nF,m is {0, 1}.\\nProof. Symmetry of Hπ\\nF,m follows from the deﬁnition. By assumption, F is the disjoint union\\nof R1, . . . , Rk and hence there exists a permutation matrix S such that SHπ\\nF,mST is a block\\nmatrix whose building blocks are the matrices\\n1\\n|Ri|\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n1\\n. . .\\n1\\n...\\n...\\n1\\n. . .\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fb∈Q|Ri|×|Ri|.\\nThus, Hπ\\nF,m is idempotent and the rank of Hπ\\nF,m is k. A basis of its image and its kernel can\\nbe read oﬀdirectly and idempotent matrices can only have the eigenvalues 0 and 1.\\n□\\nProposition 4.7. Let F ⊂Zd and M ⊂Zd be ﬁnite sets, π : F →[0, 1] be the uniform\\ndistribution, and let V1, . . . , Vc ⊆F be the nodes of the connected components of F(M), then\\n\\\\\\nm∈M\\nimg(Hπ\\nF,m) = spanR\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nX\\nx∈V1\\nex, . . . ,\\nX\\nx∈Vc\\nex\\n\\uf8fc\\n\\uf8fd\\n\\uf8fe.\\nProof. It is clear by Proposition 4.6 that the set on the right-hand side is contained in any\\nimg(Hπ\\nF,m) since any Vi decomposes disjointly into rays along m ∈M. To show the other\\ninclusion, write M = {m1, . . . , mk} and let for any i ∈[k], Ri\\n1, . . . , Ri\\nni be the disjoint rays\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n11\\nthrough F parallel to mi. In particular, {Ri\\n1, . . . , Ri\\nni} is a partition of F for any i ∈[k]. Let\\nv ∈T\\nm∈M img(Hπ\\nF,m). Again by Proposition 4.6, there exists for any i ∈[k], λi\\n1, . . . , λi\\nni ∈Q\\nsuch that\\nv =\\nni\\nX\\nj=1\\nX\\nx∈Ri\\nj\\nλi\\njex.\\nNotice that if two distinct Markov moves mi and mi′ and two indices j ∈[ni] and j′ ∈[ni′]\\nsatisfy Ri\\nj ∩Ri′\\nj′ ̸= ∅, then λi\\nj = λi′\\nj′. We show that for any i ∈[k] and any a ∈[c], λi\\nj = λi\\nj′\\nwhen Ri\\nj and Ri\\nj′ are a subset of Va. This implies the proposition. So take distinct x, x′ ∈Va\\nand assume that x and x′ lie on diﬀerent rays of mi and let that be x ∈Ri\\nj and x′ ∈Ri\\nj′ with\\nj ̸= j′. Since x and x′ are in the same connected component Va of F(M), let yi0, . . . , yir ∈F\\nbe the nodes on a minimal path in Fc(M) with yi0 = x and yir = x′. For any s ∈[r], yis\\nand yis−1 are contained in the same ray Rks\\nts coming from a Markov move mks. In particular,\\nRts−1\\nks−1 ∩Rks\\nts ̸= ∅and due to our observation made above λi\\nj = λk1\\nt1 = λk2\\nt2 = · · · = λkr\\ntr = λi\\nj′\\nwhich ﬁnishes the proof.\\n□\\nDeﬁnition 4.8. Let F ⊂Zd and M ⊂Zd be ﬁnite sets and M′ ⊆M. Let V be the set of\\nconnected components of F(M\\\\M′) and R be the set of all rays through F along all elements\\nof M′. The ray matrix of F(M) along M′ is AF(M, M′) := (|R ∩V |)R∈R,V ∈V ∈NR×V.\\nExample 4.9. Let F = [3]×[3], M = {e1, e2, e1 +e2}, and M′ = {e1, e2}. Then F(M\\\\M′)\\nhas ﬁve connected components and the ray matrix of F(M) along M′ is\\nAF(M, M′) =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n1\\n1\\n0\\n0\\n0\\n1\\n1\\n1\\n0\\n0\\n0\\n1\\n1\\n1\\n0\\n0\\n1\\n1\\n1\\n0\\n1\\n1\\n1\\n0\\n1\\n1\\n1\\n0\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\nRemark 4.10. Let F ⊂Z2, then the rays through F along e1 are the connected components\\nof F({e1, e2} \\\\ {e2}) and the rays through F along e2 are the connected components of\\nF({e1, e2} \\\\ {e1}), thus AF(M, e1) = AF(M, e2)T .\\nProposition 4.11. Let F ⊂Zd and M ⊂Zd be ﬁnite sets, π : F →[0, 1] be the uniform\\ndistribution, and M′ ⊆M. Then\\nker(AF(M, M′)) ∼=\\n\\\\\\nm∈M\\\\M′\\nimg(Hπ\\nF,m) ∩\\n\\\\\\nm∈M′\\nker(Hπ\\nF,m).\\nProof. Let V1, . . . , Vc be the connected components of F(M \\\\ M′) and R1, . . . , Rr be the\\nrays along elements in M′. Let I := T\\nm∈M\\\\M′ img(Hπ\\nF,m) and K := T\\nm∈M′ ker(Hπ\\nF,m). By\\nProposition 4.7, any element of I has the form v = Pc\\ni=1(λi\\nP\\nx∈Vi ex) for λ1, . . . , λc ∈Q.\\nAssume additionally that v ∈ker(Hπ\\nF,m) for m ∈M′ and let Ri1, . . . Rij be the rays which\\nbelong to m, then for any k ∈[j], 0 = P\\nx∈Rik vx = Pc\\nj=1 λj|Rik∩Vj|. Put diﬀerently, a vector\\nλ ∈Rc is in the kernel of (|Ri ∩Vj|)i∈[r],j∈[c] if and only if Pc\\ni=1(λi\\nP\\nx∈Vi ex) ∈I ∩K.\\n□\\n12\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nConditions on the kernel of the ray matrix allow us to give a lower bound on the second\\nlargest eigenvalue of the heat-bath random walk.\\nProposition 4.12. Let F ⊂Zd and M ⊂Zd be ﬁnite sets and π be the uniform distribution.\\nLet M′ ⊆M such that ker(AF(M, M′)) ̸= {0}, then λ(Hπ,f\\nF,M) ≥1 −P\\nm∈M′ f(m) for any\\nmass function f : M →[0, 1].\\nProof. Using the isomorphism from Proposition 4.11, we can choose a non-zero v ∈QP such\\nthat Hπ\\nF,mv = v for all m ∈M \\\\ M′ and Hπ\\nF,mv = 0 for all m ∈M′. In particular\\nHπ,f\\nF,Mv =\\nX\\nm∈M\\nf(m)Hπ\\nF,mv =\\nX\\nm∈M\\\\M′\\nf(m)Hπ\\nF,mv =\\nX\\nm∈M\\\\M\\nf(m)v.\\nSince f is a mass function, 1 −P\\nm∈M′ f(m) is an eigenvalue of Hπ,f\\nF,M.\\n□\\nDeﬁnition 4.13. Let F ⊂Zd and m, m′ ∈Zd not collinear.\\nThe pair (m, m′) has the\\nintersecting ray property in F if the following holds: For any pair of rays R1, R2 parallel\\nto m and any pair of rays R′\\n1, R′\\n2 parallel to m′ where both R1 ∩R′\\n1 and R2 ∩R′\\n2 are not\\nempty, then R1 ∩R′\\n2 ̸= ∅implies R′\\n1 ∩R2 ̸= ∅and |R1| · |R′\\n1|−1 = |R2| · |R′\\n2|−1. For a\\nﬁnite set M ⊂Zd, the graph Fc(M) has the intersecting ray property if all (m, m′) have the\\nintersecting ray property in F.\\nExample 4.14. The compressed ﬁber graph on [n1] × · · · × [nd] ⊂Zd that uses the unit\\nvectors {e1, . . . , ed} as moves has the intersecting ray property. On the other hand, consider\\nF = {u ∈N2 : u1 + u2 ≤1} and take the rays R1 := {(0, 0), (0, 1)} and R2 := {(1, 0)}\\nthat are parallel to e2 and the rays R′\\n1 := {(0, 1)} and R′\\n2 := {(0, 0), (1, 0)} that are parallel\\nto e1. Then R1 ∩R′\\n1 = {(1, 0)} and R2 ∩R′\\n2 = {(0, 1)}, but R1 ∩R′\\n2 = {(0, 0)} ̸= ∅and\\nR′\\n1 ∩R2 = ∅.\\nProposition 4.15. Let m, m′ ∈Zd not collinear and F ⊂Zd be a ﬁnite set. The matrices\\nHπ\\nF,m and Hπ\\nF,m′ commute if and only if (m, m′) have the intersecting ray property in F.\\nProof. Let u1, u2 ∈F. Then\\n(Hπ\\nF,m · Hπ\\nF,m′)u1,u2 =\\n(\\n|RF,m(u1)|−1 · |RF,m′(u2)|−1,\\nif RF,m(u1) ∩RF,m′(u2) ̸= ∅\\n0,\\notherwise\\n.\\nLet R1 := RF,m(u1), R′\\n1 := RF,m′(u1), R2 := RF,m(u2), and R′\\n2 := RF,m′(u2) Thus,\\n(Hπ\\nF,m · Hπ\\nF,m′)u1,u2 = (Hπ\\nF,m′ · Hπ\\nF,m)u1,u2. It is easy to see that the matrices commute if and\\nonly if (m, m′) have the intersecting ray property.\\n□\\nLemma 4.16. Let H1, . . . , Hn ∈Rn×n be pairwise commuting matrices. Then any eigenvalue\\nof Pn\\ni=1 Hi has the form λ1 + · · · + λn where λi is an eigenvalue of Hi.\\nProof. This is a straightforward extension of the case n = 2 in [12, Theorem 2.4.8.1] and\\nrelies on the fact that commuting matrices are simultaneously triangularizable.\\n□\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n13\\nProposition 4.17. Let F ⊂Zd and M ⊂Zd be ﬁnite sets and suppose there exists m ∈M\\nsuch that (m, m′) has the intersecting ray property in F for all m′ ∈M′ := M \\\\ {m}. Let\\nV1, . . . , Vc be the connected components of F(M′), πi : Vi →[0, 1] the uniform distribution,\\nand f ′ = (1 −f(m))−1 · f|M′, then\\nλ(Hπ,f\\nF,M) ≤f(m) + (1 −f(m)) · max{λ(Hπi,f′\\nVi,M′) : i ∈[c]}.\\nProof. Let H := Hπ,f′\\nF,M′ be the heat-bath random walk on F(M) that samples moves from M′\\naccording to f ′, then Hπ,f\\nF,M = f(m)·Hπ\\nF,m +(1−f(m))·H. By assumption, all pairs (m, m′)\\nwith m′ ∈M′ have the intersecting ray property and thus the matrices Hπ\\nF,m and H commute\\naccording to Proposition 4.15. The eigenvalues of all involved matrices are non-negative and\\nthus Lemma 4.16 implies that the second largest eigenvalue of Hπ,f\\nF,M has the form λ + λ′\\nwhere λ ∈{0, f(m)} by Proposition 4.6 and where λ′ is an eigenvalue of (1 −f(m)) · H. The\\nmatrix H is a block matrix whose building blocks are the matrices Hπ,f′\\nVi,M′ = Hπi,f′\\nVi,M′ and thus\\nthe statement follows.\\n□\\nProposition 4.18. Let F ⊂Zd and M ⊂Zk be ﬁnite sets. If F(M) has the intersecting\\nray property, then λ(Hπ,f\\nF,M) ≤1 −min(f).\\nProof. Let M = {m1, . . . , mk}.\\nThe intersecting ray property and Proposition 4.15 give\\nthat the matrices f(m1) · Hπ\\nF,mi, . . . , f(mk) · Hπ\\nF,mk commute pairwise. According to Propo-\\nsition 4.6, the eigenvalues of f(mi) · Hπ\\nF,mi are {0, f(mi)}. Lemma 4.16 gives that the second\\nlargest eigenvalue of Hπ,f\\nF,M, which equals the second largest eigenvalue modulus since all of\\nits eigenvalues are non-negative, fulﬁlls λ(Hπ,f\\nF,M) = P\\ni∈I f(mi) for a subset I ⊆[k]. Since\\nλ(Hπ,f\\nF,M) < 1 and Pk\\ni=1 f(mi) = 1, we have I ̸= [k] and the claim follows.\\n□\\nProposition 4.19. Let n1, . . . , nd ∈N>1, F = [n1]×· · ·×[nd], and M = {e1, . . . , ed}. Then\\nfor any positive mass function f : M →[0, 1], λ(Hπ,f\\nF,M) = 1 −min(f).\\nProof. It is easy to verify that Fc(M) has the intersecting ray property and thus Proposi-\\ntion 4.18 shows λ(Hπ,f\\nF,M) ≤1−min(f). Assume that min(f) = f(ei). The connected compo-\\nnents of Fc({e1, . . . , ed} \\\\ {ei}) are the layers Vj := {u ∈F : ui = j} for any j ∈[ni] and the\\nrays through F parallel are Rk := {(0, k)+s·ei : s ∈[ni]} for k = (k1, . . . , ki−1, ki+1, . . . , kd) ∈\\n[n1] × · · · × [ni−1] × [ni+1] × · · · × [nd]. In particular, any ray intersects any connected com-\\nponent exactly once.\\nThus, the matrix (|Rk ∩Vj|)k,j is the all-ones matrix, which has a\\nnon-trivial kernel. Proposition 4.12 implies λ(Hπ,f\\nF,M) ≥1 −f(ei).\\n□\\nRemark 4.20. In the special case n := n1 = · · · = nd and f : {e1, . . . , ed} →[0, 1] the\\nuniform distribution in Proposition 4.19, the heat-bath random walk on [n]d is known as\\nRook’s walk in the literature. In this case, Proposition 4.19 is exactly [13, Proposition 2.3].\\nIn [18], upper bounds on the mixing time of the Rook’s walk were obtained with path-coupling.\\nThe stationary distribution of the heat-bath random walk is independent of the actual\\nmass function on the Markov moves. The problem of ﬁnding the mass function which leads\\n14\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nto the fastest mixing behaviour can be formulated as the following optimization problem:\\n(4.2)\\narg min\\n(\\nλ(Hπ,f\\nF,M) : f : M →(0, 1),\\nX\\nm∈M\\nf(m) = 1\\n)\\n.\\nIt follows from Proposition 4.19 that the optimal value of (4.2) for F = [n1] × · · · × [nd],\\nM = {e1, . . . , ed}, and the uniform distribution π on F is the uniform distribution on M.\\nAnother example where the uniform distribution is the optimal solution to (4.2), but where\\nthe veriﬁcation is more involved, is presented in Example 4.21.\\nExample 4.21. Let F = [2] × [5] as in Example 4.4 and consider M = {e1, 2e1 + e2}. We\\ninvestigate for which µ ∈(0, 1), the transition matrix µHπ\\nF,e1 + (1 −µ)Hπ\\nF,2e1+e2 has the\\nsmallest second largest eigenvalue modulus. Its characteristic polynomial in Q[µ, x] is\\n−1\\n25x4(x −1)(µ + x −1)6(−5x2 + 5x + 2µ2 −2µ)(−5x2 + 5x + 4µ2 −4µ)\\nand hence its eigenvalues are\\nx1(µ) := 1,\\nx2(µ) := 1 −µ,\\nx3(µ) := 1\\n2\\n\"\\n1 +\\nr\\n1 + 8\\n5(µ2 −µ)\\n#\\n,\\nx4(µ) := 1\\n2\\n\"\\n1 −\\nr\\n1 + 8\\n5(µ2 −µ)\\n#\\n,\\nx5(µ) := 1\\n2\\nh\\n1 +\\np\\n1 + 4(µ2 −µ)\\ni\\n,\\nx6(µ) := 1\\n2\\nh\\n1 −\\np\\n1 + 4(µ2 −µ)\\ni\\n.\\nIt is straightforward to check that x5(µ) > 1\\n2 > x6(µ), x3(µ) > 1\\n2 > x4(µ). Since µ2 −µ < 0\\nfor u ∈(0, 1) and x3(µ) ≥x6(µ). We can show that x4(µ) ≥x2(µ) and thus\\nλ(µHπ\\nF,e1 + (1 −µ)Hπ\\nF,2e1+e2) = 1\\n2\\n\"\\n1 +\\nr\\n1 + 8\\n5(µ2 −µ)\\n#\\n.\\nThe fastest heat-bath random walk on F(M) which converges to uniform is thus obtained for\\nµ = 1\\n2, i.e. when the moves are selected uniformly. The second largest eigenvalue in this case\\nis\\n1\\n10(5 +\\n√\\n15) ≈0.887, which is larger than the second largest eigenvalue of the heat-bath\\nwalk that selects uniformly from {e1, e2} (see Proposition 4.19).\\n5. Augmenting Markov bases\\nIt follows from our investigation in Section 3 that the diameter of all compressed ﬁber\\ngraphs coming from a ﬁxed integer matrix A ∈Zm×d can be bounded from above by a\\nconstant. However, Markov moves can be used twice in a minimal path which can make the\\ndiameter of the compressed ﬁber graph larger than the size of the Markov basis. The next\\ndeﬁnition puts more constraints on the Markov basis and postulates the existence of a path\\nthat uses every move from the Markov basis at most once.\\nDeﬁnition 5.1. Let F ⊂Zd be a ﬁnite set and M = {m1, . . . , mk} ⊂Zd. An augmenting\\npath between distinct u, v ∈F of length r ∈N is a path in Fc(M) of the form\\nu →u + λi1mi1 →u + λi1mi1 + λi2mi2 →· · · →u +\\nr\\nX\\nk=1\\nλikmik = v\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n15\\nwith distinct indices i1, . . . , ir ∈[k]. An augmenting path is minimal for u, v ∈F if there\\nexists no shorter augmenting path between u and v in Fc(M).\\nA Markov basis M for\\nF is augmenting if there is an augmenting path between any distinct nodes in F.\\nThe\\naugmentation length AM(F) of an augmenting Markov basis M is the maximum length of\\nall minimal augmenting paths in Fc(M).\\nNot every Markov basis is augmenting (see Example 3.11), but the diameter of compressed\\nﬁber graphs that use an augmenting Markov basis is at most the number of the moves. For\\nﬁber graphs coming from an integer matrix, an augmenting Markov basis for all of its ﬁbers\\ncan be computed (Remark 5.2).\\nRemark 5.2. Let A ∈Zm×d with kerZ(A) ∩Nd = {0} and let b ∈NA. The Graver basis\\nis an augmenting Markov basis for FA,b for any b ∈NA. We claim that when A is totally\\nunimodular, then AGA(FA,b) ≤d2(rank(A) + 1).\\nIn particular, the augmentation length\\nis independent of the right-hand side b.\\nLet u, v ∈FA,b be arbitrary and for i ∈N, let\\nli := min{ui, vi}, wi := max{ui, vi}, and ci := sign(ui −vi) ∈{−1, 0, 1}. Then v is the unique\\noptimal value of the linear integer optimization problem\\nmin{cT x : Ax = b, l ≤x ≤w, x ∈Zd}.\\nA discrete steepest decent as deﬁned in [5, Deﬁnition 3] using Graver moves needs at most\\n∥c∥1 · d · (rank(A) + 1) ≤d2 · (rank(A) + 1) many augmentations from u to reach the optimal\\nvalue v. We refer to [5, Corollary 8] which ensures that every Graver move is used at most\\nonce. Note that in [5], x is constrained to x ≥0 instead to x ≥l, but their argument works\\nfor any lower bound.\\nExample 5.3. Fix d ∈N and consider A and M from Example 3.5. We show that M is an\\naugmenting Markov basis for FA,b for any b ∈N. Let u, v ∈FA,b be distinct, then there exists\\ni ∈[d] such that ui > vi or ui < vi, thus, we can walk from u to u′ := u + (ui −vi)(e1 −ei)\\nor from v to v′ := v + (vi −ui)(e1 −ei). In any case, after that augmentation, the pairs\\n(u′, v) and (v′, u) coincide in the ith coordinate and thus we ﬁnd an augmenting path by\\ninduction on the dimension d. We have used at most d −1 many edges in these paths and\\nhence AM(FA,b) ≤d −1 for all b ∈N.\\nWe now show that the augmentation length is essentially bounded from below by the\\ndimension of the node set and hence the bound observed in Example 5.3 cannot be improved.\\nWe ﬁrst need the following lemma.\\nLemma 5.4. Let v1, . . . , vk ∈Qd such that any v ∈spanQ {v1, . . . , vk} can be represented by\\na linear combination of r vectors. Then dim(spanQ {v1, . . . , vk}) ≤r.\\nProof. Let B ⊂P(v1, . . . , vk) the set of all subsets of cardinality r. By our assumption,\\n∪B∈BspanQ {B} = spanQ {v1, . . . , vk}. Since dim(spanQ {B}) ≤r for all B ∈B and since B\\nis ﬁnite, the claim follows.\\n□\\nProposition 5.5. Let P ⊂Qd be polytope and let M ⊂Zd be an augmenting Markov basis\\nfor Fi := (i · P) ∩Zd for all i ∈N. Then dim(P) ≤maxi∈N AM(Fi).\\n16\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nProof. Without restricting generality, we can assume that 0 ∈P. Let V := spanQ {P} be\\nthe Q-span of P, then dim(P) = dim(V ). We must have dim(spanQ {M}) = dim(V ) since\\ndim(P) = dim(convQ(Fi)) for i suﬃciently large and since M is a Markov basis for Fi. Deﬁne\\nr := maxi∈N AM(Fi) and choose any non-zero v ∈V and u ∈relint(P) ⊂Qd. Then there\\nexists δ ∈Q>0 such that u + δv ∈P. Thus, 1\\nδu + v ∈1\\nδP. Let c ∈N≥1 such that i := c\\nδ ∈N\\nand w := c\\nδu ∈Zd. Then w + cv = c(1\\nδ u + v) ∈(i · P) ∩Zd = Fi. By assumption, there exists\\nan augmenting path from w to w + cv using only r elements from M. Put diﬀerently, the\\nelement cv from V can be represented by a linear combination of r vectors from M. Since v\\nwas chosen arbitrarily, Lemma 5.4 implies dim(P) = dim(V ) ≤r.\\n□\\nRemark 5.6. It is a consequence from Proposition 5.5 that for any matrix A ∈Zm×d with\\nkerZ(A) ∩Nd = {0} and an augmenting Markov basis M, there exists F ∈PA such that\\nAM(F) ≥dim(kerZ(A)).\\nLet us now shortly recall the framework from [23] which is necessary to prove our main\\ntheorem. Let G = (V, E) be a graph. For any ordered pair of distinct nodes (x, y) ∈V × V ,\\nlet px,y ⊆E be a path from x to y in G and let Γ := {px,y : (x, y) ∈V × V, x ̸= y} be\\nthe collection of these paths, then Γ is a set of canonical paths. Let for any edge e ∈E,\\nΓe := {p ∈Γ : e ∈p} be the set of paths from Γ that use e. Now, let H : V × V →[0, 1] be a\\nsymmetric random walk on G and deﬁne\\nρ(Γ, H) := max{|p| : p ∈Γ}\\n|V |\\n· max\\n{u,v}∈E\\n|Γ{u,v}|\\nH(u, v).\\nObserve that symmetry of H is needed to make ρ(Γ, H) well-deﬁned. This can be used to\\nprove the following upper bound on the second largest eigenvalue.\\nLemma 5.7. Let G be a graph, H be a symmetric random walk on G, and Γ be a set of\\ncanonical paths in G. Then λ2(H) ≤1 −\\n1\\nρ(Γ,H).\\nProof. The stationary distribution of H is the uniform distribution and thus the statement\\nis a direct consequence of [23, Theorem 5], since ρ(Γ, H) is an upper bound on the constant\\ndeﬁned in [23, equation 4].\\n□\\nTheorem 5.8. Let F ⊂Zd be ﬁnite and let M := {m1, . . . , mk} ⊂Zd be an augmenting\\nMarkov basis. Let π be the uniform and f be a positive distribution on F and M respectively.\\nFor i ∈[k], let ri := max{|RF,mi(u)| : u ∈F} and suppose that r1 ≥r2 ≥· · · ≥rk. Then\\nλ(Hπ,f\\nM,F) ≤1 −\\n|F| · min(f)\\nAM(F) · AM(F)! · 3AM(F)−1 · 2|M| · r1r2 · · · rAM(F)\\n.\\nProof. Choose for any distinct u, v ∈F an augmenting path pu,v of minimal length in Fc(M)\\nand let Γ be the collection of all these paths. Let u + µmk = v be an edge in Fc(M), then\\nour goal is to bound |Γ{u,v}| from above. Let S := {S ⊆[r] : |S| ≤AM(F), k ∈S} and take\\nany path px,y ∈Γ{u,v}. Then there exists S := {i1, . . . , is} with s := |S| ≤AM(F) such that\\nx + Ps\\nk=1 λikmik = y. Since px,y uses the edge {u, v}, there is j ∈[s] such that ij = k and\\nλij = µ. Since |λik| ≤rik, there are at most\\ns! · (2ri1 + 1) · · · (2rij−1 + 1) · (2rij+1 + 1) · · · (2ris + 1) ≤s! · 3s−1\\nY\\nt∈S\\\\{k}\\nrt\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n17\\npaths in Γ{u,v} that uses the edge {u, v} and the moves mi1, . . . , mij−1, mij+1 . . . , mis. Since\\nall the paths are minimal, they have length at most AM(F) so indeed every path in Γ has\\nthat form.\\n|Γu,v|\\nHπ,f\\nF,M(u, v)\\n≤3AM(F)−1\\nP\\nS∈S\\n\\x10\\n|S|! Q\\nt∈S\\\\{k} rt\\n\\x11\\nf(mij) ·\\n1\\n|Rmij (u)|\\n≤3AM(F)−1 · AM(F)! · |S| · r1r2 . . . rAM(F)\\nf(mij)\\n,\\nwhere we have used the assumption r1 ≥r2 ≥· · · ≥rk. Bounding |S| rigorously from above\\nby 2|M|, the claim follows from Lemma 5.7.\\n□\\nDeﬁnition 5.9. Let F ⊂Zd and M ⊂Zd be ﬁnite sets. The longest ray through F along\\nvectors of M is RF,M := arg max{|RF,m(u)| : m ∈M, u ∈F}.\\nCorollary 5.10. Let (Fi)i∈N be a sequence of ﬁnite sets in Zd and let πi be the uniform\\ndistribution on Fi. Let M ⊂Zd be an augmenting Markov basis for Fi with AM(Fi) ≤\\ndim(Fi) and suppose that (|RFi,M|)dim(Fi))i∈N ∈O(|Fi|)i∈N.\\nThen for any positive mass\\nfunction f : M →[0, 1], there exists ǫ > 0 such that λ(Hπi,f\\nFi,M) ≤1 −ǫ for all i ∈N.\\nProof. This is a straightforward application of Theorem 5.8.\\n□\\nCorollary 5.11. Let P ⊂Zd be a polytope, Fi := (i · P) ∩Zd for i ∈N, and let πi be the\\nuniform distribution on Fi. Suppose that M ⊂Zd is an augmenting Markov basis {Fi : i ∈N}\\nsuch that AM(Fi) ≤dim(P) for all i ∈N. Then for any positive mass function f : M →[0, 1],\\nthere exists ǫ > 0 such that λ(Hπi,f\\nFi,M) ≤1 −ǫ for all i ∈N.\\nProof. Let r := dim(P). We ﬁrst show that (|RFi,M|)i∈N ∈O(i)i∈N. Write M = {m1, . . . , mk}\\nand denote by li := max{|(u + mi · Z) ∩P| : u ∈P} be the length of the longest ray through\\nthe polytope P along mi. It suﬃces to prove that i · (lk + 1) is an upper bound on the\\nlength of any ray along mk through Fi. For that, let u ∈Fi such that u + λmk ∈Fi for\\nsome λ ∈N, then 1\\ni u + λ\\ni mk ∈P and thus ⌊λ\\ni ⌋≤lk, which gives λ ≤i · (lk + 1). With\\nC := max{l1, . . . , lk} + 1 we have |RFi,M| ≤C · i. Ehrhart’s theorem [2, Theorem 3.23]\\ngives (|Fi|)i∈N ∈Ω(ir)i∈N and since |RFi,M| ≤C · i, we have (|RFi,M|r)i∈N ∈O(|Fi|)i∈N. An\\napplication of Corollary 5.10 proves the claim.\\n□\\nExample 5.12. Fix d, r ∈N and let Cd,r := {u ∈Zd : ∥u∥1 ≤r} be the set of integers of the\\nd-dimensional cross-polytope with radius r. The set Md = {e1, . . . , ed} is a Markov basis for\\nCd,r for any r ∈N. We show that Md is an augmenting Markov basis whose augmentation\\nlength is at most d. For that, let u, v ∈Cd,r distinct elements. We claim that there exists\\ni ∈[d] such that xi ̸= vi and ui + (vi −ui) ∈Cd,r. Let S ⊆[d] be the set of indices where u\\nand v diﬀer and let s = r −||u||1. If |S| = 1, then the result is clear so suppose |S| ≥2. If\\nthe result doesn’t hold then for all i ∈S, |vi| −|ui| > s. It follows that\\n∥v∥1 =\\nX\\ni/∈S\\n|ui| +\\nX\\ni∈S\\n|vi| >\\nX\\ni/∈S\\n|ui| +\\nX\\ni∈S\\ns + |ui| = |Suv| · s + ∥u∥1 = (|S| −1) · s + r.\\nBut we assumed that v ∈Cd,r. It follows that for any pair of points u, v in Cd,r, there is a\\nwalk, using the unit vectors as moves, that uses each move at most once. Corollary 5.10 yield\\nthat for any d ∈N, the second largest eigenvalue modulus of the heat-bath random walk on\\nCd,r with uniform as stationary distribution can be strictly bounded away from 1 for r →∞.\\n18\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nThe bound on the second largest eigenvalue in Theorem 5.8 is quite general and can be\\nimproved vastly, provided one has better control over the paths. For example, this can be\\nachieved for hyperrectangles intersected with a halfspace.\\nProposition 5.13. Let a ∈Nd\\n>0, b ∈N, F = {u ∈Nd : aT · u ≤b}, and M := {e1, . . . , ed}.\\nIf π and f are the uniform distributions on F and M respectively, then\\nλ(Hπ,f\\nF,M) ≤1 −|F|\\nd2\\nd\\nY\\ni=1\\nai\\nb .\\nProof. Observe that M is a Markov basis for F since all nodes are connected with 0 ∈F.\\nLet u, v ∈F be distinct. We ﬁrst show that there exists k ∈[d] such that uk ̸= vk and\\nu + (vk −uk)ek ∈F. If u ≤v, the statement trivially holds. Otherwise, there exists k ∈[d]\\nsuch that uk > vk and the vector obtained by replacing the kth coordinate of u by vk remains\\nin F. Now, consider for the following path between u and v: Choose the smallest index\\nk ∈[d] such that uk ̸= vk and such that u + (vk −uk) · ek ∈F and proceed recursively with\\nu + (vk −uk) and v. This gives a path pu,v between u and v of length at most d. Let Γ be\\nthe collection of all these paths. We want to apply Lemma 5.7. Thus, let x ∈F and consider\\nthe edge x →x + c · es. Let us count the paths pu,v that use that edge. Let u, v ∈F and let\\nk1, . . . , kr ∈[d] be distinct indices such that\\nu →u + (vk1 −uk1)ek1 →u + (vk1 −uk1)ek1 + (vk2 −uk2)ek2 →· · · →v\\nrepresents the path pu,v constructed by the upper rule.\\nAssume that pu,v uses the edge\\n{x, x + ces} and let kl = s and (vkl −ukl) = c. In particular,\\nu + (vk1 −uk1)ek1 + · · · + (vkl−1 −ukl−1)ekl−1 = x\\nx + (vkl −ukl)ekl + · · · + (vkr −ukr)ekr = v.\\nWe see that vkt = xkt for all t < l and that ukt = xkt for all t ≥l. In particular, vkl =\\nukl + c = xkl + c is also ﬁxed. The coordinates ukt and vkt are bounded from above by\\nb\\nakt\\nfor all t ∈[r], and hence there can be at most\\n l−1\\nY\\nt=1\\nb\\nakt\\n!\\n·\\n \\nr\\nY\\nt=l+1\\nb\\nakt\\n!\\n.\\nSince k1, . . . , kt are distinct coordinate indices, we have\\n|Γx,x+c·es|\\nHπ,f\\nF,M(x, x + c · es)\\n≤d ·\\nd\\nY\\ni=1\\nb\\nai\\n.\\nLemma 5.7 ﬁnishes the proof.\\n□\\nIn ﬁxed dimension, Proposition 5.13 leads to rapid mixing, but for d →∞, no statement\\ncan be made. In [19], it was shown that the simple walk with an additional halting probability\\non {u ∈Nd : atu ≤b} ∩{0, 1}d has mixing time in O(d4.5+ǫ). For zero-one polytopes, simple\\nand heat-bath walk coincide and we are conﬁdent that a similar statement holds without the\\nrestriction on zero-one polytopes.\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n19\\nThe heat-bath random walk mixes rapidly when an augmenting Markov basis with a small\\naugmentation length is used.\\nWe think that it is interesting to question how might an\\naugmenting Markov bases be obtained and how their augmentation length can be improved.\\nQuestion 5.14. Let M be an augmenting Markov basis of A. Can we ﬁnd ﬁnitely many\\nmoves m1, . . . , mk such that the augmentation length of M∪{m1, . . . , mk} on FA,b is at most\\ndim(kerZ(A)) for all b ∈NA?\\nReferences\\n1. Stephen Baumert, Archis Ghate, Seksan Kiatsupaibul, Yanfang Shen, Robert L. Smith, and Zelda B.\\nZabinsky, Discrete Hit-and-Run for Sampling Points from Arbitrary Distributions Over Subsets of Integer\\nHyperrectangles, Operations Research 57 (2009), no. 3, 727–739.\\n2. Matthias Beck and Sinai Robins, Computing the Continuous Discretely, Springer, New York, 2007.\\n3. Mary Cryan, Martin Dyer, Leslie Ann Goldberg, Mark Jerrum, and Russell Martin, Rapidly Mixing\\nMarkov Chains for Sampling Contingency Tables with a Constant Number of Rows, SIAM Journal on\\nComputing 36 (2006), no. 1, 247–278.\\n4. Jes´us A. De Loera, Raymond Hemmecke, and Matthias K¨oppe, Algebraic and Geometric Ideas in the\\nTheory of Discrete Optimization, MPS-SIAM Series on Optimization, SIAM, Cambridge, 2013.\\n5. Jes´us A. De Loera, Raymond Hemmecke, and Jon Lee, On Augmentation Algorithms for Linear and\\nInteger-Linear Programming: From Edmonds–Karp to Bland and Beyond, SIAM Journal on Optimization\\n25 (2015), no. 4, 2494–2511.\\n6. Persi Diaconis and Bernd Sturmfels, Algebraic algorithms for sampling from conditional distributions, The\\nAnnals of statistics 26 (1998), no. 1, 363–397.\\n7. Mathias Drton, Bernd Sturmfels, and Seth Sullivant, Lectures on algebraic statistics, Oberwolfach Semi-\\nnars, vol. 39, Springer, Berlin, 2009, A Birkh¨auser book.\\n8. Martin Dyer, Catherine Greenhill, and Mario Ullrich, Structure and eigenvalues of heat-bath Markov\\nchains, Linear Algebra and its Applications 454 (2014), 57–71.\\n9. Giles Gardam, Expander Graphs and Kazhdan’ s Property (T), Bachelor’s thesis, University of Sidney,\\n2012.\\n10. Hisayuki Hara, Akimichi Takemura, and Ruriko Yoshida, On connectivity of ﬁbers with positive marginals\\nin multiple logistic regression, Journal of Multivariate Analysis (2010), 1–26.\\n11. Raymond Hemmecke and Peter N. Malkin, Computing generating sets of lattice ideals and Markov bases\\nof lattices, Journal of Symbolic Computation 44 (2009), no. 10, 1463–1476.\\n12. Roger A. Horn, Matrix Analysis, 2nd ed., Cambridge University Press, New York, 2013.\\n13. Steven S. Kim, Mixing Time of a Rook’s Walk, Undergraduate certiﬁcate paper (2012).\\n14. David A. Levin, Yuval Peres, and Elisabeth L. Wilmer, Markov chains and mixing times, American\\nMathematical Society, Providence, RI, 2009.\\n15. L´aszl´o Lov´asz, Hit-and-run mixes fast, Mathematical Programming 86 (1999), no. 3, 443–461.\\n16. L´aszl´o Lov´asz and Santosh Vempala, Hit-and-Run from a Corner, SIAM Journal on Computing 35 (2006),\\nno. 4, 985–1005.\\n17. Peter N. Malkin, Computing Markov bases, Gr¨obner bases, and extreme rays, Phd thesis, 2007, p. 223.\\n18. Cam McLeman, Peter T. Otto, John Rahmani, and Matthew Sutter, Mixing times for the Rook’s walk\\nvia path coupling, to appear in Involve (2016), 1–12.\\n19. Ben Morris and Alistair Sinclair, Random Walks on Truncated Cubes and Sampling 0-1 Knapsack Solutions,\\nSIAM Journal on Computing 34 (2004), no. 1, 195–226.\\n20. Samu Potka, Higher connectivity of ﬁber graphs of Gr¨obner bases, Journal of Algebraic Statistics 4 (2013),\\nno. 1, 93–107.\\n21. Johannes Rauh and Seth Sullivant, Lifting Markov bases and higher codimension toric ﬁber products,\\nJournal of Symbolic Computation 74 (2016), 276–307.\\n22. Andr´as Seb¨o, Hilbert Bases, Caratheodory’s Theorem and Combinatorial Optimization, (1990), 431–455.\\n20\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\n23. Alistair Sinclair, Improved Bounds for Mixing Rates of Markov Chains and Multicommodity Flow, Com-\\nbinatorics, Probability and Computing 1 (1992), no. 4, 351–370.\\n24. Bernd Sturmfels, Gr¨obner bases and convex polytopes, American Mathematical Society, Providence, R.I.,\\n1996.\\n25. Seth Sullivant, Markov bases of binary graph models, Annals of Combinatorics 7 (2003), 441–466.\\n26. Santosh S. Vempala, Geometric Random Walks: A Survey, MSRI Combinatorial and Computational\\nGeometry 52 (2005), 573–612.\\n27. Tobias Windisch, Rapid mixing and Markov bases, preprint, arXiv:1505.03018 (2015), 1–18.\\nNC State University, Raleigh, NC 27695, USA\\nE-mail address: crstanl2@ncsu.edu\\nOtto-von-Guericke Universit¨at, Magdeburg, Germany\\nE-mail address: windisch@ovgu.de\\n')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Splitting from Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'SentBert.pdf', 'page': 0}, page_content='Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\\nNils Reimers and Iryna Gurevych\\nUbiquitous Knowledge Processing Lab (UKP-TUDA)\\nDepartment of Computer Science, Technische Universit¨at Darmstadt\\nwww.ukp.tu-darmstadt.de\\nAbstract\\nBERT (Devlin et al., 2018) and RoBERTa (Liu\\net al., 2019) has set a new state-of-the-art\\nperformance on sentence-pair regression tasks\\nlike semantic textual similarity (STS). How-\\never, it requires that both sentences are fed\\ninto the network, which causes a massive com-\\nputational overhead: Finding the most sim-\\nilar pair in a collection of 10,000 sentences\\nrequires about 50 million inference computa-\\ntions (~65 hours) with BERT. The construction\\nof BERT makes it unsuitable for semantic sim-\\nilarity search as well as for unsupervised tasks\\nlike clustering.\\nIn this publication, we present Sentence-BERT\\n(SBERT), a modiﬁcation of the pretrained\\nBERT network that use siamese and triplet net-\\nwork structures to derive semantically mean-\\ningful sentence embeddings that can be com-\\npared using cosine-similarity. This reduces the\\neffort for ﬁnding the most similar pair from 65\\nhours with BERT / RoBERTa to about 5 sec-\\nonds with SBERT, while maintaining the ac-\\ncuracy from BERT.\\nWe evaluate SBERT and SRoBERTa on com-\\nmon STS tasks and transfer learning tasks,\\nwhere it outperforms other state-of-the-art\\nsentence embeddings methods.1\\n1 Introduction\\nIn this publication, we present Sentence-BERT\\n(SBERT), a modiﬁcation of the BERT network us-\\ning siamese and triplet networks that is able to\\nderive semantically meaningful sentence embed-\\ndings2. This enables BERT to be used for certain\\nnew tasks, which up-to-now were not applicable\\nfor BERT. These tasks include large-scale seman-\\n1Code available: https://github.com/UKPLab/\\nsentence-transformers\\n2With semantically meaningfulwe mean that semantically\\nsimilar sentences are close in vector space.\\ntic similarity comparison, clustering, and informa-\\ntion retrieval via semantic search.\\nBERT set new state-of-the-art performance on\\nvarious sentence classiﬁcation and sentence-pair\\nregression tasks. BERT uses a cross-encoder: Two\\nsentences are passed to the transformer network\\nand the target value is predicted. However, this\\nsetup is unsuitable for various pair regression tasks\\ndue to too many possible combinations. Finding\\nin a collection of n = 10 000sentences the pair\\nwith the highest similarity requires with BERT\\nn·(n−1)/2 = 49 995 000inference computations.\\nOn a modern V100 GPU, this requires about 65\\nhours. Similar, ﬁnding which of the over 40 mil-\\nlion existent questions of Quora is the most similar\\nfor a new question could be modeled as a pair-wise\\ncomparison with BERT, however, answering a sin-\\ngle query would require over 50 hours.\\nA common method to address clustering and se-\\nmantic search is to map each sentence to a vec-\\ntor space such that semantically similar sentences\\nare close. Researchers have started to input indi-\\nvidual sentences into BERT and to derive ﬁxed-\\nsize sentence embeddings. The most commonly\\nused approach is to average the BERT output layer\\n(known as BERT embeddings) or by using the out-\\nput of the ﬁrst token (the [CLS] token). As we\\nwill show, this common practice yields rather bad\\nsentence embeddings, often worse than averaging\\nGloVe embeddings (Pennington et al., 2014).\\nTo alleviate this issue, we developed SBERT.\\nThe siamese network architecture enables that\\nﬁxed-sized vectors for input sentences can be de-\\nrived. Using a similarity measure like cosine-\\nsimilarity or Manhatten / Euclidean distance, se-\\nmantically similar sentences can be found. These\\nsimilarity measures can be performed extremely\\nefﬁcient on modern hardware, allowing SBERT\\nto be used for semantic similarity search as well\\nas for clustering. The complexity for ﬁnding the\\narXiv:1908.10084v1  [cs.CL]  27 Aug 2019'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 1}, page_content='most similar sentence pair in a collection of 10,000\\nsentences is reduced from 65 hours with BERT to\\nthe computation of 10,000 sentence embeddings\\n(~5 seconds with SBERT) and computing cosine-\\nsimilarity (~0.01 seconds). By using optimized\\nindex structures, ﬁnding the most similar Quora\\nquestion can be reduced from 50 hours to a few\\nmilliseconds (Johnson et al., 2017).\\nWe ﬁne-tune SBERT on NLI data, which cre-\\nates sentence embeddings that signiﬁcantly out-\\nperform other state-of-the-art sentence embedding\\nmethods like InferSent (Conneau et al., 2017) and\\nUniversal Sentence Encoder (Cer et al., 2018). On\\nseven Semantic Textual Similarity (STS) tasks,\\nSBERT achieves an improvement of 11.7 points\\ncompared to InferSent and 5.5 points compared to\\nUniversal Sentence Encoder. On SentEval (Con-\\nneau and Kiela, 2018), an evaluation toolkit for\\nsentence embeddings, we achieve an improvement\\nof 2.1 and 2.6 points, respectively.\\nSBERT can be adapted to a speciﬁc task. It\\nsets new state-of-the-art performance on a chal-\\nlenging argument similarity dataset (Misra et al.,\\n2016) and on a triplet dataset to distinguish sen-\\ntences from different sections of a Wikipedia arti-\\ncle (Dor et al., 2018).\\nThe paper is structured in the following way:\\nSection 3 presents SBERT, section 4 evaluates\\nSBERT on common STS tasks and on the chal-\\nlenging Argument Facet Similarity (AFS) corpus\\n(Misra et al., 2016). Section 5 evaluates SBERT\\non SentEval. In section 6, we perform an ablation\\nstudy to test some design aspect of SBERT. In sec-\\ntion 7, we compare the computational efﬁciency of\\nSBERT sentence embeddings in contrast to other\\nstate-of-the-art sentence embedding methods.\\n2 Related Work\\nWe ﬁrst introduce BERT, then, we discuss state-\\nof-the-art sentence embedding methods.\\nBERT (Devlin et al., 2018) is a pre-trained\\ntransformer network (Vaswani et al., 2017), which\\nset for various NLP tasks new state-of-the-art re-\\nsults, including question answering, sentence clas-\\nsiﬁcation, and sentence-pair regression. The input\\nfor BERT for sentence-pair regression consists of\\nthe two sentences, separated by a special [SEP]\\ntoken. Multi-head attention over 12 (base-model)\\nor 24 layers (large-model) is applied and the out-\\nput is passed to a simple regression function to de-\\nrive the ﬁnal label. Using this setup, BERT set a\\nnew state-of-the-art performance on the Semantic\\nTextual Semilarity (STS) benchmark (Cer et al.,\\n2017). RoBERTa (Liu et al., 2019) showed, that\\nthe performance of BERT can further improved by\\nsmall adaptations to the pre-training process. We\\nalso tested XLNet (Yang et al., 2019), but it led in\\ngeneral to worse results than BERT.\\nA large disadvantage of the BERT network\\nstructure is that no independent sentence embed-\\ndings are computed, which makes it difﬁcult to de-\\nrive sentence embeddings from BERT. To bypass\\nthis limitations, researchers passed single sen-\\ntences through BERT and then derive a ﬁxed sized\\nvector by either averaging the outputs (similar to\\naverage word embeddings) or by using the output\\nof the special CLS token (for example: May et al.\\n(2019); Zhang et al. (2019); Qiao et al. (2019)).\\nThese two options are also provided by the popu-\\nlar bert-as-a-service-repository3. Up to our knowl-\\nedge, there is so far no evaluation if these methods\\nlead to useful sentence embeddings.\\nSentence embeddings are a well studied area\\nwith dozens of proposed methods. Skip-Thought\\n(Kiros et al., 2015) trains an encoder-decoder ar-\\nchitecture to predict the surrounding sentences.\\nInferSent (Conneau et al., 2017) uses labeled\\ndata of the Stanford Natural Language Inference\\ndataset (Bowman et al., 2015) and the Multi-\\nGenre NLI dataset (Williams et al., 2018) to train\\na siamese BiLSTM network with max-pooling\\nover the output. Conneau et al. showed, that\\nInferSent consistently outperforms unsupervised\\nmethods like SkipThought. Universal Sentence\\nEncoder (Cer et al., 2018) trains a transformer\\nnetwork and augments unsupervised learning with\\ntraining on SNLI. Hill et al. (2016) showed, that\\nthe task on which sentence embeddings are trained\\nsigniﬁcantly impacts their quality. Previous work\\n(Conneau et al., 2017; Cer et al., 2018) found that\\nthe SNLI datasets are suitable for training sen-\\ntence embeddings. Yang et al. (2018) presented\\na method to train on conversations from Reddit\\nusing siamese DAN and siamese transformer net-\\nworks, which yielded good results on the STS\\nbenchmark dataset.\\nHumeau et al. (2019) addresses the run-time\\noverhead of the cross-encoder from BERT and\\npresent a method (poly-encoders) to compute\\na score between m context vectors and pre-\\n3https://github.com/hanxiao/\\nbert-as-service/'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 2}, page_content='Sentence A Sentence B \\nBERT BERT \\nu v \\npooling pooling \\n(u, v, |u-v|) \\nSoftmax classifier \\nFigure 1: SBERT architecture with classiﬁcation ob-\\njective function, e.g., for ﬁne-tuning on SNLI dataset.\\nThe two BERT networks have tied weights (siamese\\nnetwork structure).\\ncomputed candidate embeddings using attention.\\nThis idea works for ﬁnding the highest scoring\\nsentence in a larger collection. However, poly-\\nencoders have the drawback that the score function\\nis not symmetric and the computational overhead\\nis too large for use-cases like clustering, which\\nwould require O(n2) score computations.\\nPrevious neural sentence embedding methods\\nstarted the training from a random initialization.\\nIn this publication, we use the pre-trained BERT\\nand RoBERTa network and only ﬁne-tune it to\\nyield useful sentence embeddings. This reduces\\nsigniﬁcantly the needed training time: SBERT can\\nbe tuned in less than 20 minutes, while yielding\\nbetter results than comparable sentence embed-\\nding methods.\\n3 Model\\nSBERT adds a pooling operation to the output\\nof BERT / RoBERTa to derive a ﬁxed sized sen-\\ntence embedding. We experiment with three pool-\\ning strategies: Using the output of the CLS-token,\\ncomputing the mean of all output vectors ( MEAN-\\nstrategy), and computing a max-over-time of the\\noutput vectors (MAX-strategy). The default conﬁg-\\nuration is MEAN.\\nIn order to ﬁne-tune BERT / RoBERTa, we cre-\\nate siamese and triplet networks (Schroff et al.,\\n2015) to update the weights such that the produced\\nsentence embeddings are semantically meaningful\\nand can be compared with cosine-similarity.\\nThe network structure depends on the available\\nSentence A Sentence B \\nBERT BERT \\nu v \\npooling pooling \\ncosine-sim(u, v) \\n-1 … 1 \\nFigure 2: SBERT architecture at inference, for exam-\\nple, to compute similarity scores. This architecture is\\nalso used with the regression objective function.\\ntraining data. We experiment with the following\\nstructures and objective functions.\\nClassiﬁcation Objective Function. We con-\\ncatenate the sentence embeddings u and v with\\nthe element-wise difference |u−v|and multiply it\\nwith the trainable weight Wt ∈R3n×k:\\no= softmax(Wt(u,v, |u−v|))\\nwhere n is the dimension of the sentence em-\\nbeddings and kthe number of labels. We optimize\\ncross-entropy loss. This structure is depicted in\\nFigure 1.\\nRegression Objective Function. The cosine-\\nsimilarity between the two sentence embeddings\\nu and v is computed (Figure 2). We use mean-\\nsquared-error loss as the objective function.\\nTriplet Objective Function. Given an anchor\\nsentence a, a positive sentence p, and a negative\\nsentence n, triplet loss tunes the network such that\\nthe distance between a and p is smaller than the\\ndistance between a and n. Mathematically, we\\nminimize the following loss function:\\nmax(||sa −sp||−||sa −sn||+ ϵ,0)\\nwith sx the sentence embedding for a/n/p, ||·||\\na distance metric and margin ϵ. Margin ϵensures\\nthat sp is at least ϵcloser to sa than sn. As metric\\nwe use Euclidean distance and we set ϵ= 1in our\\nexperiments.\\n3.1 Training Details\\nWe train SBERT on the combination of the SNLI\\n(Bowman et al., 2015) and the Multi-Genre NLI'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 3}, page_content='Model STS12 STS13 STS14 STS15 STS16 STSb SICK-R Avg.\\nAvg. GloVe embeddings 55.14 70.66 59.73 68.25 63.66 58.02 53.76 61.32\\nAvg. BERT embeddings 38.78 57.98 57.98 63.15 61.06 46.35 58.40 54.81\\nBERT CLS-vector 20.16 30.01 20.09 36.88 38.08 16.50 42.63 29.19\\nInferSent - Glove 52.86 66.75 62.15 72.77 66.87 68.03 65.65 65.01\\nUniversal Sentence Encoder 64.49 67.80 64.61 76.83 73.18 74.92 76.69 71.22\\nSBERT-NLI-base 70.97 76.53 73.19 79.09 74.30 77.03 72.91 74.89\\nSBERT-NLI-large 72.27 78.46 74.90 80.99 76.25 79.23 73.75 76.55\\nSRoBERTa-NLI-base 71.54 72.49 70.80 78.74 73.69 77.77 74.46 74.21\\nSRoBERTa-NLI-large 74.53 77.00 73.18 81.85 76.82 79.10 74.29 76.68\\nTable 1: Spearman rank correlation ρbetween the cosine similarity of sentence representations and the gold labels\\nfor various Textual Similarity (STS) tasks. Performance is reported by convention as ρ×100. STS12-STS16:\\nSemEval 2012-2016, STSb: STSbenchmark, SICK-R: SICK relatedness dataset.\\n(Williams et al., 2018) dataset. The SNLI is a col-\\nlection of 570,000 sentence pairs annotated with\\nthe labels contradiction, eintailment, and neu-\\ntral. MultiNLI contains 430,000 sentence pairs\\nand covers a range of genres of spoken and written\\ntext. We ﬁne-tune SBERT with a 3-way softmax-\\nclassiﬁer objective function for one epoch. We\\nused a batch-size of 16, Adam optimizer with\\nlearning rate 2e−5, and a linear learning rate\\nwarm-up over 10% of the training data. Our de-\\nfault pooling strategy is MEAN.\\n4 Evaluation - Semantic Textual\\nSimilarity\\nWe evaluate the performance of SBERT for com-\\nmon Semantic Textual Similarity (STS) tasks.\\nState-of-the-art methods often learn a (complex)\\nregression function that maps sentence embed-\\ndings to a similarity score. However, these regres-\\nsion functions work pair-wise and due to the com-\\nbinatorial explosion those are often not scalable if\\nthe collection of sentences reaches a certain size.\\nInstead, we always use cosine-similarity to com-\\npare the similarity between two sentence embed-\\ndings. We ran our experiments also with nega-\\ntive Manhatten and negative Euclidean distances\\nas similarity measures, but the results for all ap-\\nproaches remained roughly the same.\\n4.1 Unsupervised STS\\nWe evaluate the performance of SBERT for STS\\nwithout using any STS speciﬁc training data. We\\nuse the STS tasks 2012 - 2016 (Agirre et al., 2012,\\n2013, 2014, 2015, 2016), the STS benchmark (Cer\\net al., 2017), and the SICK-Relatedness dataset\\n(Marelli et al., 2014). These datasets provide la-\\nbels between 0 and 5 on the semantic relatedness\\nof sentence pairs. We showed in (Reimers et al.,\\n2016) that Pearson correlation is badly suited for\\nSTS. Instead, we compute the Spearman’s rank\\ncorrelation between the cosine-similarity of the\\nsentence embeddings and the gold labels. The\\nsetup for the other sentence embedding methods\\nis equivalent, the similarity is computed by cosine-\\nsimilarity. The results are depicted in Table 1.\\nThe results shows that directly using the output\\nof BERT leads to rather poor performances. Av-\\neraging the BERT embeddings achieves an aver-\\nage correlation of only 54.81, and using the CLS-\\ntoken output only achieves an average correlation\\nof 29.19. Both are worse than computing average\\nGloVe embeddings.\\nUsing the described siamese network structure\\nand ﬁne-tuning mechanism substantially improves\\nthe correlation, outperforming both InferSent and\\nUniversal Sentence Encoder substantially. The\\nonly dataset where SBERT performs worse than\\nUniversal Sentence Encoder is SICK-R. Universal\\nSentence Encoder was trained on various datasets,\\nincluding news, question-answer pages and dis-\\ncussion forums, which appears to be more suitable\\nto the data of SICK-R. In contrast, SBERT was\\npre-trained only on Wikipedia (via BERT) and on\\nNLI data.\\nWhile RoBERTa was able to improve the per-\\nformance for several supervised tasks, we only\\nobserve minor difference between SBERT and\\nSRoBERTa for generating sentence embeddings.\\n4.2 Supervised STS\\nThe STS benchmark (STSb) (Cer et al., 2017) pro-\\nvides is a popular dataset to evaluate supervised\\nSTS systems. The data includes 8,628 sentence\\npairs from the three categoriescaptions, news, and\\nforums. It is divided into train (5,749), dev (1,500)\\nand test (1,379). BERT set a new state-of-the-art\\nperformance on this dataset by passing both sen-\\ntences to the network and using a simple regres-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 4}, page_content='sion method for the output.\\nModel Spearman\\nNot trained for STS\\nAvg. GloVe embeddings 58.02\\nAvg. BERT embeddings 46.35\\nInferSent - GloVe 68.03\\nUniversal Sentence Encoder 74.92\\nSBERT-NLI-base 77.03\\nSBERT-NLI-large 79.23\\nTrained on STS benchmark dataset\\nBERT-STSb-base 84.30 ±0.76\\nSBERT-STSb-base 84.67 ±0.19\\nSRoBERTa-STSb-base 84.92 ±0.34\\nBERT-STSb-large 85.64 ±0.81\\nSBERT-STSb-large 84.45 ±0.43\\nSRoBERTa-STSb-large 85.02 ±0.76\\nTrained on NLI data + STS benchmark data\\nBERT-NLI-STSb-base 88.33 ±0.19\\nSBERT-NLI-STSb-base 85.35 ±0.17\\nSRoBERTa-NLI-STSb-base 84.79 ±0.38\\nBERT-NLI-STSb-large 88.77 ±0.46\\nSBERT-NLI-STSb-large 86.10 ±0.13\\nSRoBERTa-NLI-STSb-large 86.15 ±0.35\\nTable 2: Evaluation on the STS benchmark test set.\\nBERT systems were trained with 10 random seeds and\\n4 epochs. SBERT was ﬁne-tuned on the STSb dataset,\\nSBERT-NLI was pretrained on the NLI datasets, then\\nﬁne-tuned on the STSb dataset.\\nWe use the training set to ﬁne-tune SBERT us-\\ning the regression objective function. At predic-\\ntion time, we compute the cosine-similarity be-\\ntween the sentence embeddings. All systems are\\ntrained with 10 random seeds to counter variances\\n(Reimers and Gurevych, 2018).\\nThe results are depicted in Table 2. We ex-\\nperimented with two setups: Only training on\\nSTSb, and ﬁrst training on NLI, then training on\\nSTSb. We observe that the later strategy leads to a\\nslight improvement of 1-2 points. This two-step\\napproach had an especially large impact for the\\nBERT cross-encoder, which improved the perfor-\\nmance by 3-4 points. We do not observe a signiﬁ-\\ncant difference between BERT and RoBERTa.\\n4.3 Argument Facet Similarity\\nWe evaluate SBERT on the Argument Facet Sim-\\nilarity (AFS) corpus by Misra et al. (2016). The\\nAFS corpus annotated 6,000 sentential argument\\npairs from social media dialogs on three contro-\\nversial topics: gun control, gay marriage, and\\ndeath penalty. The data was annotated on a scale\\nfrom 0 (“different topic”) to 5 (“completely equiv-\\nalent”). The similarity notion in the AFS corpus\\nis fairly different to the similarity notion in the\\nSTS datasets from SemEval. STS data is usually\\ndescriptive, while AFS data are argumentative ex-\\ncerpts from dialogs. To be considered similar, ar-\\nguments must not only make similar claims, but\\nalso provide a similar reasoning. Further, the lex-\\nical gap between the sentences in AFS is much\\nlarger. Hence, simple unsupervised methods as\\nwell as state-of-the-art STS systems perform badly\\non this dataset (Reimers et al., 2019).\\nWe evaluate SBERT on this dataset in two sce-\\nnarios: 1) As proposed by Misra et al., we evaluate\\nSBERT using 10-fold cross-validation. A draw-\\nback of this evaluation setup is that it is not clear\\nhow well approaches generalize to different top-\\nics. Hence, 2) we evaluate SBERT in a cross-topic\\nsetup. Two topics serve for training and the ap-\\nproach is evaluated on the left-out topic. We repeat\\nthis for all three topics and average the results.\\nSBERT is ﬁne-tuned using the Regression Ob-\\njective Function. The similarity score is computed\\nusing cosine-similarity based on the sentence em-\\nbeddings. We also provide the Pearson correla-\\ntion r to make the results comparable to Misra et\\nal. However, we showed (Reimers et al., 2016)\\nthat Pearson correlation has some serious draw-\\nbacks and should be avoided for comparing STS\\nsystems. The results are depicted in Table 3.\\nUnsupervised methods like tf-idf, average\\nGloVe embeddings or InferSent perform rather\\nbadly on this dataset with low scores. Training\\nSBERT in the 10-fold cross-validation setup gives\\na performance that is nearly on-par with BERT.\\nHowever, in the cross-topic evaluation, we ob-\\nserve a performance drop of SBERT by about 7\\npoints Spearman correlation. To be considered\\nsimilar, arguments should address the same claims\\nand provide the same reasoning. BERT is able to\\nuse attention to compare directly both sentences\\n(e.g. word-by-word comparison), while SBERT\\nmust map individual sentences from an unseen\\ntopic to a vector space such that arguments with\\nsimilar claims and reasons are close. This is a\\nmuch more challenging task, which appears to re-\\nquire more than just two topics for training to work\\non-par with BERT.\\n4.4 Wikipedia Sections Distinction\\nDor et al. (2018) use Wikipedia to create a the-\\nmatically ﬁne-grained train, dev and test set for\\nsentence embeddings methods. Wikipedia arti-\\ncles are separated into distinct sections focusing\\non certain aspects. Dor et al. assume that sen-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 5}, page_content='Model r ρ\\nUnsupervised methods\\ntf-idf 46.77 42.95\\nAvg. GloVe embeddings 32.40 34.00\\nInferSent - GloVe 27.08 26.63\\n10-fold Cross-Validation\\nSVR (Misra et al., 2016) 63.33 -\\nBERT-AFS-base 77.20 74.84\\nSBERT-AFS-base 76.57 74.13\\nBERT-AFS-large 78.68 76.38\\nSBERT-AFS-large 77.85 75.93\\nCross-Topic Evaluation\\nBERT-AFS-base 58.49 57.23\\nSBERT-AFS-base 52.34 50.65\\nBERT-AFS-large 62.02 60.34\\nSBERT-AFS-large 53.82 53.10\\nTable 3: Average Pearson correlation r and average\\nSpearman’s rank correlation ρon the Argument Facet\\nSimilarity (AFS) corpus (Misra et al., 2016). Misra et\\nal. proposes 10-fold cross-validation. We additionally\\nevaluate in a cross-topic scenario: Methods are trained\\non two topics, and are evaluated on the third topic.\\ntences in the same section are thematically closer\\nthan sentences in different sections. They use this\\nto create a large dataset of weakly labeled sen-\\ntence triplets: The anchor and the positive exam-\\nple come from the same section, while the neg-\\native example comes from a different section of\\nthe same article. For example, from the Alice\\nArnold article: Anchor: Arnold joined the BBC\\nRadio Drama Company in 1988., positive: Arnold\\ngained media attention in May 2012., negative:\\nBalding and Arnold are keen amateur golfers.\\nWe use the dataset from Dor et al. We use the\\nTriplet Objective, train SBERT for one epoch on\\nthe about 1.8 Million training triplets and evaluate\\nit on the 222,957 test triplets. Test triplets are from\\na distinct set of Wikipedia articles. As evaluation\\nmetric, we use accuracy: Is the positive example\\ncloser to the anchor than the negative example?\\nResults are presented in Table 4. Dor et al. ﬁne-\\ntuned a BiLSTM architecture with triplet loss to\\nderive sentence embeddings for this dataset. As\\nthe table shows, SBERT clearly outperforms the\\nBiLSTM approach by Dor et al.\\n5 Evaluation - SentEval\\nSentEval (Conneau and Kiela, 2018) is a popular\\ntoolkit to evaluate the quality of sentence embed-\\ndings. Sentence embeddings are used as features\\nfor a logistic regression classiﬁer. The logistic re-\\ngression classiﬁer is trained on various tasks in a\\n10-fold cross-validation setup and the prediction\\naccuracy is computed for the test-fold.\\nModel Accuracy\\nmean-vectors 0.65\\nskip-thoughts-CS 0.62\\nDor et al. 0.74\\nSBERT-WikiSec-base 0.8042\\nSBERT-WikiSec-large 0.8078\\nSRoBERTa-WikiSec-base 0.7945\\nSRoBERTa-WikiSec-large 0.7973\\nTable 4: Evaluation on the Wikipedia section triplets\\ndataset (Dor et al., 2018). SBERT trained with triplet\\nloss for one epoch.\\nThe purpose of SBERT sentence embeddings\\nare not to be used for transfer learning for other\\ntasks. Here, we think ﬁne-tuning BERT as de-\\nscribed by Devlin et al. (2018) for new tasks is\\nthe more suitable method, as it updates all layers\\nof the BERT network. However, SentEval can still\\ngive an impression on the quality of our sentence\\nembeddings for various tasks.\\nWe compare the SBERT sentence embeddings\\nto other sentence embeddings methods on the fol-\\nlowing seven SentEval transfer tasks:\\n• MR: Sentiment prediction for movie reviews\\nsnippets on a ﬁve start scale (Pang and Lee,\\n2005).\\n• CR: Sentiment prediction of customer prod-\\nuct reviews (Hu and Liu, 2004).\\n• SUBJ: Subjectivity prediction of sentences\\nfrom movie reviews and plot summaries\\n(Pang and Lee, 2004).\\n• MPQA: Phrase level opinion polarity classi-\\nﬁcation from newswire (Wiebe et al., 2005).\\n• SST: Stanford Sentiment Treebank with bi-\\nnary labels (Socher et al., 2013).\\n• TREC: Fine grained question-type classiﬁ-\\ncation from TREC (Li and Roth, 2002).\\n• MRPC: Microsoft Research Paraphrase Cor-\\npus from parallel news sources (Dolan et al.,\\n2004).\\nThe results can be found in Table 5. SBERT\\nis able to achieve the best performance in 5 out\\nof 7 tasks. The average performance increases\\nby about 2 percentage points compared to In-\\nferSent as well as the Universal Sentence Encoder.\\nEven though transfer learning is not the purpose of\\nSBERT, it outperforms other state-of-the-art sen-\\ntence embeddings methods on this task.'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 6}, page_content='Model MR CR SUBJ MPQA SST TREC MRPC Avg.\\nAvg. GloVe embeddings 77.25 78.30 91.17 87.85 80.18 83.0 72.87 81.52\\nAvg. fast-text embeddings 77.96 79.23 91.68 87.81 82.15 83.6 74.49 82.42\\nAvg. BERT embeddings 78.66 86.25 94.37 88.66 84.40 92.8 69.45 84.94\\nBERT CLS-vector 78.68 84.85 94.21 88.23 84.13 91.4 71.13 84.66\\nInferSent - GloVe 81.57 86.54 92.50 90.38 84.18 88.2 75.77 85.59\\nUniversal Sentence Encoder 80.09 85.19 93.98 86.70 86.38 93.2 70.14 85.10\\nSBERT-NLI-base 83.64 89.43 94.39 89.86 88.96 89.6 76.00 87.41\\nSBERT-NLI-large 84.88 90.07 94.52 90.33 90.66 87.4 75.94 87.69\\nTable 5: Evaluation of SBERT sentence embeddings using the SentEval toolkit. SentEval evaluates sentence\\nembeddings on different sentence classiﬁcation tasks by training a logistic regression classiﬁer using the sentence\\nembeddings as features. Scores are based on a 10-fold cross-validation.\\nIt appears that the sentence embeddings from\\nSBERT capture well sentiment information: We\\nobserve large improvements for all sentiment tasks\\n(MR, CR, and SST) from SentEval in comparison\\nto InferSent and Universal Sentence Encoder.\\nThe only dataset where SBERT is signiﬁcantly\\nworse than Universal Sentence Encoder is the\\nTREC dataset. Universal Sentence Encoder was\\npre-trained on question-answering data, which ap-\\npears to be beneﬁcial for the question-type classi-\\nﬁcation task of the TREC dataset.\\nAverage BERT embeddings or using the CLS-\\ntoken output from a BERT network achieved bad\\nresults for various STS tasks (Table 1), worse than\\naverage GloVe embeddings. However, for Sent-\\nEval, average BERT embeddings and the BERT\\nCLS-token output achieves decent results (Ta-\\nble 5), outperforming average GloVe embeddings.\\nThe reason for this are the different setups. For\\nthe STS tasks, we used cosine-similarity to es-\\ntimate the similarities between sentence embed-\\ndings. Cosine-similarity treats all dimensions\\nequally. In contrast, SentEval ﬁts a logistic regres-\\nsion classiﬁer to the sentence embeddings. This\\nallows that certain dimensions can have higher or\\nlower impact on the classiﬁcation result.\\nWe conclude that average BERT embeddings /\\nCLS-token output from BERT return sentence em-\\nbeddings that are infeasible to be used with cosine-\\nsimilarity or with Manhatten / Euclidean distance.\\nFor transfer learning, they yield slightly worse\\nresults than InferSent or Universal Sentence En-\\ncoder. However, using the described ﬁne-tuning\\nsetup with a siamese network structure on NLI\\ndatasets yields sentence embeddings that achieve\\na new state-of-the-art for the SentEval toolkit.\\n6 Ablation Study\\nWe have demonstrated strong empirical results for\\nthe quality of SBERT sentence embeddings. In\\nthis section, we perform an ablation study of dif-\\nferent aspects of SBERT in order to get a better\\nunderstanding of their relative importance.\\nWe evaluated different pooling strategies\\n(MEAN, MAX, and CLS). For the classiﬁcation\\nobjective function, we evaluate different concate-\\nnation methods. For each possible conﬁguration,\\nwe train SBERT with 10 different random seeds\\nand average the performances.\\nThe objective function (classiﬁcation vs. regres-\\nsion) depends on the annotated dataset. For the\\nclassiﬁcation objective function, we train SBERT-\\nbase on the SNLI and the Multi-NLI dataset. For\\nthe regression objective function, we train on the\\ntraining set of the STS benchmark dataset. Perfor-\\nmances are measured on the development split of\\nthe STS benchmark dataset. Results are shown in\\nTable 6.\\nNLI STSb\\nPooling Strategy\\nMEAN 80.78 87.44\\nMAX 79.07 69.92\\nCLS 79.80 86.62\\nConcatenation\\n(u,v) 66.04 -\\n(|u−v|) 69.78 -\\n(u∗v) 70.54 -\\n(|u−v|,u ∗v) 78.37 -\\n(u,v,u ∗v) 77.44 -\\n(u,v, |u−v|) 80.78 -\\n(u,v, |u−v|,u ∗v) 80.44 -\\nTable 6: SBERT trained on NLI data with the clas-\\nsiﬁcation objective function, on the STS benchmark\\n(STSb) with the regression objective function. Con-\\nﬁgurations are evaluated on the development set of the\\nSTSb using cosine-similarity and Spearman’s rank cor-\\nrelation. For the concatenation methods, we only report\\nscores with MEAN pooling strategy.\\nWhen trained with the classiﬁcation objective\\nfunction on NLI data, the pooling strategy has a\\nrather minor impact. The impact of the concate-\\nnation mode is much larger. InferSent (Conneau'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 7}, page_content='et al., 2017) and Universal Sentence Encoder (Cer\\net al., 2018) both use (u,v, |u−v|,u ∗v) as input\\nfor a softmax classiﬁer. However, in our architec-\\nture, adding the element-wise u∗vdecreased the\\nperformance.\\nThe most important component is the element-\\nwise difference |u−v|. Note, that the concate-\\nnation mode is only relevant for training the soft-\\nmax classiﬁer. At inference, when predicting sim-\\nilarities for the STS benchmark dataset, only the\\nsentence embeddings uand v are used in combi-\\nnation with cosine-similarity. The element-wise\\ndifference measures the distance between the di-\\nmensions of the two sentence embeddings, ensur-\\ning that similar pairs are closer and dissimilar pairs\\nare further apart.\\nWhen trained with the regression objective\\nfunction, we observe that the pooling strategy has\\na large impact. There, the MAX strategy perform\\nsigniﬁcantly worse than MEAN or CLS-token strat-\\negy. This is in contrast to (Conneau et al., 2017),\\nwho found it beneﬁcial for the BiLSTM-layer of\\nInferSent to use MAX instead of MEAN pooling.\\n7 Computational Efﬁciency\\nSentence embeddings need potentially be com-\\nputed for Millions of sentences, hence, a high\\ncomputation speed is desired. In this section, we\\ncompare SBERT to average GloVe embeddings,\\nInferSent (Conneau et al., 2017), and Universal\\nSentence Encoder (Cer et al., 2018).\\nFor our comparison we use the sentences from\\nthe STS benchmark (Cer et al., 2017). We com-\\npute average GloVe embeddings using a sim-\\nple for-loop with python dictionary lookups and\\nNumPy. InferSent 4 is based on PyTorch. For\\nUniversal Sentence Encoder, we use the Tensor-\\nFlow Hub version 5, which is based on Tensor-\\nFlow. SBERT is based on PyTorch. For improved\\ncomputation of sentence embeddings, we imple-\\nmented a smart batching strategy: Sentences with\\nsimilar lengths are grouped together and are only\\npadded to the longest element in a mini-batch.\\nThis drastically reduces computational overhead\\nfrom padding tokens.\\nPerformances were measured on a server with\\nIntel i7-5820K CPU @ 3.30GHz, Nvidia Tesla\\n4https://github.com/facebookresearch/\\nInferSent\\n5https://tfhub.dev/google/\\nuniversal-sentence-encoder-large/3\\nV100 GPU, CUDA 9.2 and cuDNN. The results\\nare depicted in Table 7.\\nModel CPU GPU\\nAvg. GloVe embeddings 6469 -\\nInferSent 137 1876\\nUniversal Sentence Encoder 67 1318\\nSBERT-base 44 1378\\nSBERT-base - smart batching 83 2042\\nTable 7: Computation speed (sentences per second) of\\nsentence embedding methods. Higher is better.\\nOn CPU, InferSent is about 65% faster than\\nSBERT. This is due to the much simpler net-\\nwork architecture. InferSent uses a single Bi-\\nLSTM layer, while BERT uses 12 stacked trans-\\nformer layers. However, an advantage of trans-\\nformer networks is the computational efﬁciency\\non GPUs. There, SBERT with smart batching\\nis about 9% faster than InferSent and about 55%\\nfaster than Universal Sentence Encoder. Smart\\nbatching achieves a speed-up of 89% on CPU and\\n48% on GPU. Average GloVe embeddings is obvi-\\nously by a large margin the fastest method to com-\\npute sentence embeddings.\\n8 Conclusion\\nWe showed that BERT out-of-the-box maps sen-\\ntences to a vector space that is rather unsuit-\\nable to be used with common similarity measures\\nlike cosine-similarity. The performance for seven\\nSTS tasks was below the performance of average\\nGloVe embeddings.\\nTo overcome this shortcoming, we presented\\nSentence-BERT (SBERT). SBERT ﬁne-tunes\\nBERT in a siamese / triplet network architec-\\nture. We evaluated the quality on various com-\\nmon benchmarks, where it could achieve a sig-\\nniﬁcant improvement over state-of-the-art sen-\\ntence embeddings methods. Replacing BERT with\\nRoBERTa did not yield a signiﬁcant improvement\\nin our experiments.\\nSBERT is computationally efﬁcient. On a GPU,\\nit is about 9% faster than InferSent and about 55%\\nfaster than Universal Sentence Encoder. SBERT\\ncan be used for tasks which are computationally\\nnot feasible to be modeled with BERT. For exam-\\nple, clustering of 10,000 sentences with hierarchi-\\ncal clustering requires with BERT about 65 hours,\\nas around 50 Million sentence combinations must\\nbe computed. With SBERT, we were able to re-\\nduce the effort to about 5 seconds.'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 8}, page_content='Acknowledgments\\nThis work has been supported by the German\\nResearch Foundation through the German-Israeli\\nProject Cooperation (DIP, grant DA 1600/1-1 and\\ngrant GU 798/17-1). It has been co-funded by the\\nGerman Federal Ministry of Education and Re-\\nsearch (BMBF) under the promotional references\\n03VP02540 (ArgumenText).\\nReferences\\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel\\nCer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei\\nGuo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada\\nMihalcea, German Rigau, Larraitz Uria, and Janyce\\nWiebe. 2015. SemEval-2015 Task 2: Semantic Tex-\\ntual Similarity, English, Spanish and Pilot on Inter-\\npretability. In Proceedings of the 9th International\\nWorkshop on Semantic Evaluation (SemEval 2015),\\npages 252–263, Denver, Colorado. Association for\\nComputational Linguistics.\\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel\\nCer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei\\nGuo, Rada Mihalcea, German Rigau, and Janyce\\nWiebe. 2014. SemEval-2014 Task 10: Multilingual\\nSemantic Textual Similarity. In Proceedings of the\\n8th International Workshop on Semantic Evaluation\\n(SemEval 2014), pages 81–91, Dublin, Ireland. As-\\nsociation for Computational Linguistics.\\nEneko Agirre, Carmen Banea, Daniel M. Cer, Mona T.\\nDiab, Aitor Gonzalez-Agirre, Rada Mihalcea, Ger-\\nman Rigau, and Janyce Wiebe. 2016. SemEval-\\n2016 Task 1: Semantic Textual Similarity, Mono-\\nlingual and Cross-Lingual Evaluation. In Proceed-\\nings of the 10th International Workshop on Seman-\\ntic Evaluation, SemEval@NAACL-HLT 2016, San\\nDiego, CA, USA, June 16-17, 2016, pages 497–511.\\nEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-\\nAgirre, and Weiwei Guo. 2013. *SEM 2013 shared\\ntask: Semantic Textual Similarity. In Second Joint\\nConference on Lexical and Computational Seman-\\ntics (*SEM), Volume 1: Proceedings of the Main\\nConference and the Shared Task: Semantic Textual\\nSimilarity, pages 32–43, Atlanta, Georgia, USA. As-\\nsociation for Computational Linguistics.\\nEneko Agirre, Mona Diab, Daniel Cer, and Aitor\\nGonzalez-Agirre. 2012. SemEval-2012 Task 6: A\\nPilot on Semantic Textual Similarity. In Proceed-\\nings of the First Joint Conference on Lexical and\\nComputational Semantics - Volume 1: Proceedings\\nof the Main Conference and the Shared Task, and\\nVolume 2: Proceedings of the Sixth International\\nWorkshop on Semantic Evaluation, SemEval ’12,\\npages 385–393, Stroudsburg, PA, USA. Association\\nfor Computational Linguistics.\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn Proceedings of the 2015 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n632–642, Lisbon, Portugal. Association for Compu-\\ntational Linguistics.\\nDaniel Cer, Mona Diab, Eneko Agirre, Iigo Lopez-\\nGazpio, and Lucia Specia. 2017. SemEval-2017\\nTask 1: Semantic Textual Similarity Multilingual\\nand Crosslingual Focused Evaluation. In Proceed-\\nings of the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1–14, Vancou-\\nver, Canada.\\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\\nNicole Limtiaco, Rhomni St. John, Noah Constant,\\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\\nYun-Hsuan Sung, Brian Strope, and Ray Kurzweil.\\n2018. Universal Sentence Encoder. arXiv preprint\\narXiv:1803.11175.\\nAlexis Conneau and Douwe Kiela. 2018. SentEval: An\\nEvaluation Toolkit for Universal Sentence Represen-\\ntations. arXiv preprint arXiv:1803.05449.\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo ¨ıc\\nBarrault, and Antoine Bordes. 2017. Supervised\\nLearning of Universal Sentence Representations\\nfrom Natural Language Inference Data. In Proceed-\\nings of the 2017 Conference on Empirical Methods\\nin Natural Language Processing, pages 670–680,\\nCopenhagen, Denmark. Association for Computa-\\ntional Linguistics.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2018. BERT: Pre-training of\\nDeep Bidirectional Transformers for Language Un-\\nderstanding. arXiv preprint arXiv:1810.04805.\\nBill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-\\nsupervised Construction of Large Paraphrase Cor-\\npora: Exploiting Massively Parallel News Sources.\\nIn Proceedings of the 20th International Confer-\\nence on Computational Linguistics, COLING ’04,\\nStroudsburg, PA, USA. Association for Computa-\\ntional Linguistics.\\nLiat Ein Dor, Yosi Mass, Alon Halfon, Elad Venezian,\\nIlya Shnayderman, Ranit Aharonov, and Noam\\nSlonim. 2018. Learning Thematic Similarity Metric\\nfrom Article Sections Using Triplet Networks. In\\nProceedings of the 56th Annual Meeting of the As-\\nsociation for Computational Linguistics (Volume 2:\\nShort Papers), pages 49–54, Melbourne, Australia.\\nAssociation for Computational Linguistics.\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen.\\n2016. Learning Distributed Representations of Sen-\\ntences from Unlabelled Data. In Proceedings of\\nthe 2016 Conference of the North American Chap-\\nter of the Association for Computational Linguis-\\ntics: Human Language Technologies, pages 1367–\\n1377, San Diego, California. Association for Com-\\nputational Linguistics.'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 9}, page_content='Minqing Hu and Bing Liu. 2004. Mining and Sum-\\nmarizing Customer Reviews. In Proceedings of the\\nTenth ACM SIGKDD International Conference on\\nKnowledge Discovery and Data Mining, KDD ’04,\\npages 168–177, New York, NY , USA. ACM.\\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\\nand Jason Weston. 2019. Real-time Inference\\nin Multi-sentence Tasks with Deep Pretrained\\nTransformers. arXiv preprint arXiv:1905.01969,\\nabs/1905.01969.\\nJeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2017.\\nBillion-scale similarity search with GPUs. arXiv\\npreprint arXiv:1702.08734.\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-Thought Vectors. In\\nC. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama,\\nand R. Garnett, editors, Advances in Neural Infor-\\nmation Processing Systems 28, pages 3294–3302.\\nCurran Associates, Inc.\\nXin Li and Dan Roth. 2002. Learning Question Classi-\\nﬁers. In Proceedings of the 19th International Con-\\nference on Computational Linguistics - Volume 1,\\nCOLING ’02, pages 1–7, Stroudsburg, PA, USA.\\nAssociation for Computational Linguistics.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoBERTa: A Robustly Optimized BERT Pretrain-\\ning Approach. arXiv preprint arXiv:1907.11692.\\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\\nBentivogli, Raffaella Bernardi, and Roberto Zam-\\nparelli. 2014. A SICK cure for the evaluation of\\ncompositional distributional semantic models. In\\nProceedings of the Ninth International Conference\\non Language Resources and Evaluation (LREC’14),\\npages 216–223, Reykjavik, Iceland. European Lan-\\nguage Resources Association (ELRA).\\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\\nBowman, and Rachel Rudinger. 2019. On Mea-\\nsuring Social Biases in Sentence Encoders. arXiv\\npreprint arXiv:1903.10561.\\nAmita Misra, Brian Ecker, and Marilyn A. Walker.\\n2016. Measuring the Similarity of Sentential Ar-\\nguments in Dialogue. In Proceedings of the SIG-\\nDIAL 2016 Conference, The 17th Annual Meeting\\nof the Special Interest Group on Discourse and Di-\\nalogue, 13-15 September 2016, Los Angeles, CA,\\nUSA, pages 276–287.\\nBo Pang and Lillian Lee. 2004. A Sentimental Educa-\\ntion: Sentiment Analysis Using Subjectivity Sum-\\nmarization Based on Minimum Cuts. In Proceed-\\nings of the 42nd Meeting of the Association for\\nComputational Linguistics (ACL’04), Main Volume,\\npages 271–278, Barcelona, Spain.\\nBo Pang and Lillian Lee. 2005. Seeing Stars: Exploit-\\ning Class Relationships for Sentiment Categoriza-\\ntion with Respect to Rating Scales. In Proceedings\\nof the 43rd Annual Meeting of the Association for\\nComputational Linguistics (ACL’05), pages 115–\\n124, Ann Arbor, Michigan. Association for Compu-\\ntational Linguistics.\\nJeffrey Pennington, Richard Socher, and Christo-\\npher D. Manning. 2014. GloVe: Global Vectors for\\nWord Representation. In Empirical Methods in Nat-\\nural Language Processing (EMNLP), pages 1532–\\n1543.\\nYifan Qiao, Chenyan Xiong, Zheng-Hao Liu, and\\nZhiyuan Liu. 2019. Understanding the Be-\\nhaviors of BERT in Ranking. arXiv preprint\\narXiv:1904.07531.\\nNils Reimers, Philip Beyer, and Iryna Gurevych. 2016.\\nTask-Oriented Intrinsic Evaluation of Semantic Tex-\\ntual Similarity. In Proceedings of the 26th Inter-\\nnational Conference on Computational Linguistics\\n(COLING), pages 87–96.\\nNils Reimers and Iryna Gurevych. 2018. Why Com-\\nparing Single Performance Scores Does Not Al-\\nlow to Draw Conclusions About Machine Learn-\\ning Approaches. arXiv preprint arXiv:1803.09578,\\nabs/1803.09578.\\nNils Reimers, Benjamin Schiller, Tilman Beck, Jo-\\nhannes Daxenberger, Christian Stab, and Iryna\\nGurevych. 2019. Classiﬁcation and Clustering of\\nArguments with Contextualized Word Embeddings.\\nIn Proceedings of the 57th Annual Meeting of the As-\\nsociation for Computational Linguistics, pages 567–\\n578, Florence, Italy. Association for Computational\\nLinguistics.\\nFlorian Schroff, Dmitry Kalenichenko, and James\\nPhilbin. 2015. FaceNet: A Uniﬁed Embedding for\\nFace Recognition and Clustering. arXiv preprint\\narXiv:1503.03832, abs/1503.03832.\\nRichard Socher, Alex Perelygin, Jean Wu, Jason\\nChuang, Christopher D. Manning, Andrew Ng, and\\nChristopher Potts. 2013. Recursive Deep Models for\\nSemantic Compositionality Over a Sentiment Tree-\\nbank. In Proceedings of the 2013 Conference on\\nEmpirical Methods in Natural Language Process-\\ning, pages 1631–1642, Seattle, Washington, USA.\\nAssociation for Computational Linguistics.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is All\\nyou Need. In I. Guyon, U. V . Luxburg, S. Bengio,\\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\\nnett, editors, Advances in Neural Information Pro-\\ncessing Systems 30, pages 5998–6008.\\nJanyce Wiebe, Theresa Wilson, and Claire Cardie.\\n2005. Annotating Expressions of Opinions and\\nEmotions in Language. Language Resources and\\nEvaluation, 39(2):165–210.'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 10}, page_content='Adina Williams, Nikita Nangia, and Samuel Bowman.\\n2018. A Broad-Coverage Challenge Corpus for\\nSentence Understanding through Inference. In Pro-\\nceedings of the 2018 Conference of the North Amer-\\nican Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Vol-\\nume 1 (Long Papers), pages 1112–1122. Association\\nfor Computational Linguistics.\\nYinfei Yang, Steve Yuan, Daniel Cer, Sheng-Yi Kong,\\nNoah Constant, Petr Pilar, Heming Ge, Yun-hsuan\\nSung, Brian Strope, and Ray Kurzweil. 2018.\\nLearning Semantic Textual Similarity from Conver-\\nsations. In Proceedings of The Third Workshop\\non Representation Learning for NLP, pages 164–\\n174, Melbourne, Australia. Association for Compu-\\ntational Linguistics.\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G.\\nCarbonell, Ruslan Salakhutdinov, and Quoc V . Le.\\n2019. XLNet: Generalized Autoregressive Pretrain-\\ning for Language Understanding. arXiv preprint\\narXiv:1906.08237, abs/1906.08237.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\\nWeinberger, and Yoav Artzi. 2019. BERTScore:\\nEvaluating Text Generation with BERT. arXiv\\npreprint arXiv:1904.09675.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## reading a pdf file\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"SentBert.pdf\")\n",
    "docs= loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to recursively split text by characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'SentBert.pdf', 'page': 0}, page_content='Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\\nNils Reimers and Iryna Gurevych\\nUbiquitous Knowledge Processing Lab (UKP-TUDA)\\nDepartment of Computer Science, Technische Universit¨at Darmstadt\\nwww.ukp.tu-darmstadt.de\\nAbstract\\nBERT (Devlin et al., 2018) and RoBERTa (Liu\\net al., 2019) has set a new state-of-the-art\\nperformance on sentence-pair regression tasks\\nlike semantic textual similarity (STS). How-\\never, it requires that both sentences are fed'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 0}, page_content='ever, it requires that both sentences are fed\\ninto the network, which causes a massive com-\\nputational overhead: Finding the most sim-\\nilar pair in a collection of 10,000 sentences\\nrequires about 50 million inference computa-\\ntions (~65 hours) with BERT. The construction\\nof BERT makes it unsuitable for semantic sim-\\nilarity search as well as for unsupervised tasks\\nlike clustering.\\nIn this publication, we present Sentence-BERT\\n(SBERT), a modiﬁcation of the pretrained'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 0}, page_content='(SBERT), a modiﬁcation of the pretrained\\nBERT network that use siamese and triplet net-\\nwork structures to derive semantically mean-\\ningful sentence embeddings that can be com-\\npared using cosine-similarity. This reduces the\\neffort for ﬁnding the most similar pair from 65\\nhours with BERT / RoBERTa to about 5 sec-\\nonds with SBERT, while maintaining the ac-\\ncuracy from BERT.\\nWe evaluate SBERT and SRoBERTa on com-\\nmon STS tasks and transfer learning tasks,'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 0}, page_content='mon STS tasks and transfer learning tasks,\\nwhere it outperforms other state-of-the-art\\nsentence embeddings methods.1\\n1 Introduction\\nIn this publication, we present Sentence-BERT\\n(SBERT), a modiﬁcation of the BERT network us-\\ning siamese and triplet networks that is able to\\nderive semantically meaningful sentence embed-\\ndings2. This enables BERT to be used for certain\\nnew tasks, which up-to-now were not applicable\\nfor BERT. These tasks include large-scale seman-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 0}, page_content='for BERT. These tasks include large-scale seman-\\n1Code available: https://github.com/UKPLab/\\nsentence-transformers\\n2With semantically meaningfulwe mean that semantically\\nsimilar sentences are close in vector space.\\ntic similarity comparison, clustering, and informa-\\ntion retrieval via semantic search.\\nBERT set new state-of-the-art performance on\\nvarious sentence classiﬁcation and sentence-pair\\nregression tasks. BERT uses a cross-encoder: Two\\nsentences are passed to the transformer network'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 0}, page_content='sentences are passed to the transformer network\\nand the target value is predicted. However, this\\nsetup is unsuitable for various pair regression tasks\\ndue to too many possible combinations. Finding\\nin a collection of n = 10 000sentences the pair\\nwith the highest similarity requires with BERT\\nn·(n−1)/2 = 49 995 000inference computations.\\nOn a modern V100 GPU, this requires about 65\\nhours. Similar, ﬁnding which of the over 40 mil-\\nlion existent questions of Quora is the most similar'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 0}, page_content='for a new question could be modeled as a pair-wise\\ncomparison with BERT, however, answering a sin-\\ngle query would require over 50 hours.\\nA common method to address clustering and se-\\nmantic search is to map each sentence to a vec-\\ntor space such that semantically similar sentences\\nare close. Researchers have started to input indi-\\nvidual sentences into BERT and to derive ﬁxed-\\nsize sentence embeddings. The most commonly\\nused approach is to average the BERT output layer'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 0}, page_content='used approach is to average the BERT output layer\\n(known as BERT embeddings) or by using the out-\\nput of the ﬁrst token (the [CLS] token). As we\\nwill show, this common practice yields rather bad\\nsentence embeddings, often worse than averaging\\nGloVe embeddings (Pennington et al., 2014).\\nTo alleviate this issue, we developed SBERT.\\nThe siamese network architecture enables that\\nﬁxed-sized vectors for input sentences can be de-\\nrived. Using a similarity measure like cosine-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 0}, page_content='rived. Using a similarity measure like cosine-\\nsimilarity or Manhatten / Euclidean distance, se-\\nmantically similar sentences can be found. These\\nsimilarity measures can be performed extremely\\nefﬁcient on modern hardware, allowing SBERT\\nto be used for semantic similarity search as well\\nas for clustering. The complexity for ﬁnding the\\narXiv:1908.10084v1  [cs.CL]  27 Aug 2019'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 1}, page_content='most similar sentence pair in a collection of 10,000\\nsentences is reduced from 65 hours with BERT to\\nthe computation of 10,000 sentence embeddings\\n(~5 seconds with SBERT) and computing cosine-\\nsimilarity (~0.01 seconds). By using optimized\\nindex structures, ﬁnding the most similar Quora\\nquestion can be reduced from 50 hours to a few\\nmilliseconds (Johnson et al., 2017).\\nWe ﬁne-tune SBERT on NLI data, which cre-\\nates sentence embeddings that signiﬁcantly out-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 1}, page_content='ates sentence embeddings that signiﬁcantly out-\\nperform other state-of-the-art sentence embedding\\nmethods like InferSent (Conneau et al., 2017) and\\nUniversal Sentence Encoder (Cer et al., 2018). On\\nseven Semantic Textual Similarity (STS) tasks,\\nSBERT achieves an improvement of 11.7 points\\ncompared to InferSent and 5.5 points compared to\\nUniversal Sentence Encoder. On SentEval (Con-\\nneau and Kiela, 2018), an evaluation toolkit for\\nsentence embeddings, we achieve an improvement'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 1}, page_content='sentence embeddings, we achieve an improvement\\nof 2.1 and 2.6 points, respectively.\\nSBERT can be adapted to a speciﬁc task. It\\nsets new state-of-the-art performance on a chal-\\nlenging argument similarity dataset (Misra et al.,\\n2016) and on a triplet dataset to distinguish sen-\\ntences from different sections of a Wikipedia arti-\\ncle (Dor et al., 2018).\\nThe paper is structured in the following way:\\nSection 3 presents SBERT, section 4 evaluates\\nSBERT on common STS tasks and on the chal-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 1}, page_content='SBERT on common STS tasks and on the chal-\\nlenging Argument Facet Similarity (AFS) corpus\\n(Misra et al., 2016). Section 5 evaluates SBERT\\non SentEval. In section 6, we perform an ablation\\nstudy to test some design aspect of SBERT. In sec-\\ntion 7, we compare the computational efﬁciency of\\nSBERT sentence embeddings in contrast to other\\nstate-of-the-art sentence embedding methods.\\n2 Related Work\\nWe ﬁrst introduce BERT, then, we discuss state-\\nof-the-art sentence embedding methods.'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 1}, page_content='of-the-art sentence embedding methods.\\nBERT (Devlin et al., 2018) is a pre-trained\\ntransformer network (Vaswani et al., 2017), which\\nset for various NLP tasks new state-of-the-art re-\\nsults, including question answering, sentence clas-\\nsiﬁcation, and sentence-pair regression. The input\\nfor BERT for sentence-pair regression consists of\\nthe two sentences, separated by a special [SEP]\\ntoken. Multi-head attention over 12 (base-model)\\nor 24 layers (large-model) is applied and the out-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 1}, page_content='put is passed to a simple regression function to de-\\nrive the ﬁnal label. Using this setup, BERT set a\\nnew state-of-the-art performance on the Semantic\\nTextual Semilarity (STS) benchmark (Cer et al.,\\n2017). RoBERTa (Liu et al., 2019) showed, that\\nthe performance of BERT can further improved by\\nsmall adaptations to the pre-training process. We\\nalso tested XLNet (Yang et al., 2019), but it led in\\ngeneral to worse results than BERT.\\nA large disadvantage of the BERT network'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 1}, page_content='A large disadvantage of the BERT network\\nstructure is that no independent sentence embed-\\ndings are computed, which makes it difﬁcult to de-\\nrive sentence embeddings from BERT. To bypass\\nthis limitations, researchers passed single sen-\\ntences through BERT and then derive a ﬁxed sized\\nvector by either averaging the outputs (similar to\\naverage word embeddings) or by using the output\\nof the special CLS token (for example: May et al.\\n(2019); Zhang et al. (2019); Qiao et al. (2019)).'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 1}, page_content='(2019); Zhang et al. (2019); Qiao et al. (2019)).\\nThese two options are also provided by the popu-\\nlar bert-as-a-service-repository3. Up to our knowl-\\nedge, there is so far no evaluation if these methods\\nlead to useful sentence embeddings.\\nSentence embeddings are a well studied area\\nwith dozens of proposed methods. Skip-Thought\\n(Kiros et al., 2015) trains an encoder-decoder ar-\\nchitecture to predict the surrounding sentences.\\nInferSent (Conneau et al., 2017) uses labeled'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 1}, page_content='InferSent (Conneau et al., 2017) uses labeled\\ndata of the Stanford Natural Language Inference\\ndataset (Bowman et al., 2015) and the Multi-\\nGenre NLI dataset (Williams et al., 2018) to train\\na siamese BiLSTM network with max-pooling\\nover the output. Conneau et al. showed, that\\nInferSent consistently outperforms unsupervised\\nmethods like SkipThought. Universal Sentence\\nEncoder (Cer et al., 2018) trains a transformer\\nnetwork and augments unsupervised learning with'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 1}, page_content='network and augments unsupervised learning with\\ntraining on SNLI. Hill et al. (2016) showed, that\\nthe task on which sentence embeddings are trained\\nsigniﬁcantly impacts their quality. Previous work\\n(Conneau et al., 2017; Cer et al., 2018) found that\\nthe SNLI datasets are suitable for training sen-\\ntence embeddings. Yang et al. (2018) presented\\na method to train on conversations from Reddit\\nusing siamese DAN and siamese transformer net-\\nworks, which yielded good results on the STS'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 1}, page_content='works, which yielded good results on the STS\\nbenchmark dataset.\\nHumeau et al. (2019) addresses the run-time\\noverhead of the cross-encoder from BERT and\\npresent a method (poly-encoders) to compute\\na score between m context vectors and pre-\\n3https://github.com/hanxiao/\\nbert-as-service/'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 2}, page_content='Sentence A Sentence B \\nBERT BERT \\nu v \\npooling pooling \\n(u, v, |u-v|) \\nSoftmax classifier \\nFigure 1: SBERT architecture with classiﬁcation ob-\\njective function, e.g., for ﬁne-tuning on SNLI dataset.\\nThe two BERT networks have tied weights (siamese\\nnetwork structure).\\ncomputed candidate embeddings using attention.\\nThis idea works for ﬁnding the highest scoring\\nsentence in a larger collection. However, poly-\\nencoders have the drawback that the score function'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 2}, page_content='is not symmetric and the computational overhead\\nis too large for use-cases like clustering, which\\nwould require O(n2) score computations.\\nPrevious neural sentence embedding methods\\nstarted the training from a random initialization.\\nIn this publication, we use the pre-trained BERT\\nand RoBERTa network and only ﬁne-tune it to\\nyield useful sentence embeddings. This reduces\\nsigniﬁcantly the needed training time: SBERT can\\nbe tuned in less than 20 minutes, while yielding'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 2}, page_content='be tuned in less than 20 minutes, while yielding\\nbetter results than comparable sentence embed-\\nding methods.\\n3 Model\\nSBERT adds a pooling operation to the output\\nof BERT / RoBERTa to derive a ﬁxed sized sen-\\ntence embedding. We experiment with three pool-\\ning strategies: Using the output of the CLS-token,\\ncomputing the mean of all output vectors ( MEAN-\\nstrategy), and computing a max-over-time of the\\noutput vectors (MAX-strategy). The default conﬁg-\\nuration is MEAN.'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 2}, page_content='uration is MEAN.\\nIn order to ﬁne-tune BERT / RoBERTa, we cre-\\nate siamese and triplet networks (Schroff et al.,\\n2015) to update the weights such that the produced\\nsentence embeddings are semantically meaningful\\nand can be compared with cosine-similarity.\\nThe network structure depends on the available\\nSentence A Sentence B \\nBERT BERT \\nu v \\npooling pooling \\ncosine-sim(u, v) \\n-1 … 1 \\nFigure 2: SBERT architecture at inference, for exam-\\nple, to compute similarity scores. This architecture is'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 2}, page_content='also used with the regression objective function.\\ntraining data. We experiment with the following\\nstructures and objective functions.\\nClassiﬁcation Objective Function. We con-\\ncatenate the sentence embeddings u and v with\\nthe element-wise difference |u−v|and multiply it\\nwith the trainable weight Wt ∈R3n×k:\\no= softmax(Wt(u,v, |u−v|))\\nwhere n is the dimension of the sentence em-\\nbeddings and kthe number of labels. We optimize\\ncross-entropy loss. This structure is depicted in\\nFigure 1.'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 2}, page_content='Figure 1.\\nRegression Objective Function. The cosine-\\nsimilarity between the two sentence embeddings\\nu and v is computed (Figure 2). We use mean-\\nsquared-error loss as the objective function.\\nTriplet Objective Function. Given an anchor\\nsentence a, a positive sentence p, and a negative\\nsentence n, triplet loss tunes the network such that\\nthe distance between a and p is smaller than the\\ndistance between a and n. Mathematically, we\\nminimize the following loss function:'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 2}, page_content='minimize the following loss function:\\nmax(||sa −sp||−||sa −sn||+ ϵ,0)\\nwith sx the sentence embedding for a/n/p, ||·||\\na distance metric and margin ϵ. Margin ϵensures\\nthat sp is at least ϵcloser to sa than sn. As metric\\nwe use Euclidean distance and we set ϵ= 1in our\\nexperiments.\\n3.1 Training Details\\nWe train SBERT on the combination of the SNLI\\n(Bowman et al., 2015) and the Multi-Genre NLI'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 3}, page_content='Model STS12 STS13 STS14 STS15 STS16 STSb SICK-R Avg.\\nAvg. GloVe embeddings 55.14 70.66 59.73 68.25 63.66 58.02 53.76 61.32\\nAvg. BERT embeddings 38.78 57.98 57.98 63.15 61.06 46.35 58.40 54.81\\nBERT CLS-vector 20.16 30.01 20.09 36.88 38.08 16.50 42.63 29.19\\nInferSent - Glove 52.86 66.75 62.15 72.77 66.87 68.03 65.65 65.01\\nUniversal Sentence Encoder 64.49 67.80 64.61 76.83 73.18 74.92 76.69 71.22\\nSBERT-NLI-base 70.97 76.53 73.19 79.09 74.30 77.03 72.91 74.89'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 3}, page_content='SBERT-NLI-large 72.27 78.46 74.90 80.99 76.25 79.23 73.75 76.55\\nSRoBERTa-NLI-base 71.54 72.49 70.80 78.74 73.69 77.77 74.46 74.21\\nSRoBERTa-NLI-large 74.53 77.00 73.18 81.85 76.82 79.10 74.29 76.68\\nTable 1: Spearman rank correlation ρbetween the cosine similarity of sentence representations and the gold labels\\nfor various Textual Similarity (STS) tasks. Performance is reported by convention as ρ×100. STS12-STS16:\\nSemEval 2012-2016, STSb: STSbenchmark, SICK-R: SICK relatedness dataset.'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 3}, page_content='(Williams et al., 2018) dataset. The SNLI is a col-\\nlection of 570,000 sentence pairs annotated with\\nthe labels contradiction, eintailment, and neu-\\ntral. MultiNLI contains 430,000 sentence pairs\\nand covers a range of genres of spoken and written\\ntext. We ﬁne-tune SBERT with a 3-way softmax-\\nclassiﬁer objective function for one epoch. We\\nused a batch-size of 16, Adam optimizer with\\nlearning rate 2e−5, and a linear learning rate\\nwarm-up over 10% of the training data. Our de-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 3}, page_content='warm-up over 10% of the training data. Our de-\\nfault pooling strategy is MEAN.\\n4 Evaluation - Semantic Textual\\nSimilarity\\nWe evaluate the performance of SBERT for com-\\nmon Semantic Textual Similarity (STS) tasks.\\nState-of-the-art methods often learn a (complex)\\nregression function that maps sentence embed-\\ndings to a similarity score. However, these regres-\\nsion functions work pair-wise and due to the com-\\nbinatorial explosion those are often not scalable if'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 3}, page_content='the collection of sentences reaches a certain size.\\nInstead, we always use cosine-similarity to com-\\npare the similarity between two sentence embed-\\ndings. We ran our experiments also with nega-\\ntive Manhatten and negative Euclidean distances\\nas similarity measures, but the results for all ap-\\nproaches remained roughly the same.\\n4.1 Unsupervised STS\\nWe evaluate the performance of SBERT for STS\\nwithout using any STS speciﬁc training data. We\\nuse the STS tasks 2012 - 2016 (Agirre et al., 2012,'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 3}, page_content='2013, 2014, 2015, 2016), the STS benchmark (Cer\\net al., 2017), and the SICK-Relatedness dataset\\n(Marelli et al., 2014). These datasets provide la-\\nbels between 0 and 5 on the semantic relatedness\\nof sentence pairs. We showed in (Reimers et al.,\\n2016) that Pearson correlation is badly suited for\\nSTS. Instead, we compute the Spearman’s rank\\ncorrelation between the cosine-similarity of the\\nsentence embeddings and the gold labels. The\\nsetup for the other sentence embedding methods'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 3}, page_content='setup for the other sentence embedding methods\\nis equivalent, the similarity is computed by cosine-\\nsimilarity. The results are depicted in Table 1.\\nThe results shows that directly using the output\\nof BERT leads to rather poor performances. Av-\\neraging the BERT embeddings achieves an aver-\\nage correlation of only 54.81, and using the CLS-\\ntoken output only achieves an average correlation\\nof 29.19. Both are worse than computing average\\nGloVe embeddings.'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 3}, page_content='GloVe embeddings.\\nUsing the described siamese network structure\\nand ﬁne-tuning mechanism substantially improves\\nthe correlation, outperforming both InferSent and\\nUniversal Sentence Encoder substantially. The\\nonly dataset where SBERT performs worse than\\nUniversal Sentence Encoder is SICK-R. Universal\\nSentence Encoder was trained on various datasets,\\nincluding news, question-answer pages and dis-\\ncussion forums, which appears to be more suitable\\nto the data of SICK-R. In contrast, SBERT was'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 3}, page_content='to the data of SICK-R. In contrast, SBERT was\\npre-trained only on Wikipedia (via BERT) and on\\nNLI data.\\nWhile RoBERTa was able to improve the per-\\nformance for several supervised tasks, we only\\nobserve minor difference between SBERT and\\nSRoBERTa for generating sentence embeddings.\\n4.2 Supervised STS\\nThe STS benchmark (STSb) (Cer et al., 2017) pro-\\nvides is a popular dataset to evaluate supervised\\nSTS systems. The data includes 8,628 sentence\\npairs from the three categoriescaptions, news, and'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 3}, page_content='forums. It is divided into train (5,749), dev (1,500)\\nand test (1,379). BERT set a new state-of-the-art\\nperformance on this dataset by passing both sen-\\ntences to the network and using a simple regres-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 4}, page_content='sion method for the output.\\nModel Spearman\\nNot trained for STS\\nAvg. GloVe embeddings 58.02\\nAvg. BERT embeddings 46.35\\nInferSent - GloVe 68.03\\nUniversal Sentence Encoder 74.92\\nSBERT-NLI-base 77.03\\nSBERT-NLI-large 79.23\\nTrained on STS benchmark dataset\\nBERT-STSb-base 84.30 ±0.76\\nSBERT-STSb-base 84.67 ±0.19\\nSRoBERTa-STSb-base 84.92 ±0.34\\nBERT-STSb-large 85.64 ±0.81\\nSBERT-STSb-large 84.45 ±0.43\\nSRoBERTa-STSb-large 85.02 ±0.76\\nTrained on NLI data + STS benchmark data\\nBERT-NLI-STSb-base 88.33 ±0.19'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 4}, page_content='BERT-NLI-STSb-base 88.33 ±0.19\\nSBERT-NLI-STSb-base 85.35 ±0.17\\nSRoBERTa-NLI-STSb-base 84.79 ±0.38\\nBERT-NLI-STSb-large 88.77 ±0.46\\nSBERT-NLI-STSb-large 86.10 ±0.13\\nSRoBERTa-NLI-STSb-large 86.15 ±0.35\\nTable 2: Evaluation on the STS benchmark test set.\\nBERT systems were trained with 10 random seeds and\\n4 epochs. SBERT was ﬁne-tuned on the STSb dataset,\\nSBERT-NLI was pretrained on the NLI datasets, then\\nﬁne-tuned on the STSb dataset.\\nWe use the training set to ﬁne-tune SBERT us-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 4}, page_content='We use the training set to ﬁne-tune SBERT us-\\ning the regression objective function. At predic-\\ntion time, we compute the cosine-similarity be-\\ntween the sentence embeddings. All systems are\\ntrained with 10 random seeds to counter variances\\n(Reimers and Gurevych, 2018).\\nThe results are depicted in Table 2. We ex-\\nperimented with two setups: Only training on\\nSTSb, and ﬁrst training on NLI, then training on\\nSTSb. We observe that the later strategy leads to a'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 4}, page_content='slight improvement of 1-2 points. This two-step\\napproach had an especially large impact for the\\nBERT cross-encoder, which improved the perfor-\\nmance by 3-4 points. We do not observe a signiﬁ-\\ncant difference between BERT and RoBERTa.\\n4.3 Argument Facet Similarity\\nWe evaluate SBERT on the Argument Facet Sim-\\nilarity (AFS) corpus by Misra et al. (2016). The\\nAFS corpus annotated 6,000 sentential argument\\npairs from social media dialogs on three contro-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 4}, page_content='pairs from social media dialogs on three contro-\\nversial topics: gun control, gay marriage, and\\ndeath penalty. The data was annotated on a scale\\nfrom 0 (“different topic”) to 5 (“completely equiv-\\nalent”). The similarity notion in the AFS corpus\\nis fairly different to the similarity notion in the\\nSTS datasets from SemEval. STS data is usually\\ndescriptive, while AFS data are argumentative ex-\\ncerpts from dialogs. To be considered similar, ar-\\nguments must not only make similar claims, but'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 4}, page_content='guments must not only make similar claims, but\\nalso provide a similar reasoning. Further, the lex-\\nical gap between the sentences in AFS is much\\nlarger. Hence, simple unsupervised methods as\\nwell as state-of-the-art STS systems perform badly\\non this dataset (Reimers et al., 2019).\\nWe evaluate SBERT on this dataset in two sce-\\nnarios: 1) As proposed by Misra et al., we evaluate\\nSBERT using 10-fold cross-validation. A draw-\\nback of this evaluation setup is that it is not clear'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 4}, page_content='how well approaches generalize to different top-\\nics. Hence, 2) we evaluate SBERT in a cross-topic\\nsetup. Two topics serve for training and the ap-\\nproach is evaluated on the left-out topic. We repeat\\nthis for all three topics and average the results.\\nSBERT is ﬁne-tuned using the Regression Ob-\\njective Function. The similarity score is computed\\nusing cosine-similarity based on the sentence em-\\nbeddings. We also provide the Pearson correla-\\ntion r to make the results comparable to Misra et'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 4}, page_content='tion r to make the results comparable to Misra et\\nal. However, we showed (Reimers et al., 2016)\\nthat Pearson correlation has some serious draw-\\nbacks and should be avoided for comparing STS\\nsystems. The results are depicted in Table 3.\\nUnsupervised methods like tf-idf, average\\nGloVe embeddings or InferSent perform rather\\nbadly on this dataset with low scores. Training\\nSBERT in the 10-fold cross-validation setup gives\\na performance that is nearly on-par with BERT.'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 4}, page_content='a performance that is nearly on-par with BERT.\\nHowever, in the cross-topic evaluation, we ob-\\nserve a performance drop of SBERT by about 7\\npoints Spearman correlation. To be considered\\nsimilar, arguments should address the same claims\\nand provide the same reasoning. BERT is able to\\nuse attention to compare directly both sentences\\n(e.g. word-by-word comparison), while SBERT\\nmust map individual sentences from an unseen\\ntopic to a vector space such that arguments with'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 4}, page_content='topic to a vector space such that arguments with\\nsimilar claims and reasons are close. This is a\\nmuch more challenging task, which appears to re-\\nquire more than just two topics for training to work\\non-par with BERT.\\n4.4 Wikipedia Sections Distinction\\nDor et al. (2018) use Wikipedia to create a the-\\nmatically ﬁne-grained train, dev and test set for\\nsentence embeddings methods. Wikipedia arti-\\ncles are separated into distinct sections focusing\\non certain aspects. Dor et al. assume that sen-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 5}, page_content='Model r ρ\\nUnsupervised methods\\ntf-idf 46.77 42.95\\nAvg. GloVe embeddings 32.40 34.00\\nInferSent - GloVe 27.08 26.63\\n10-fold Cross-Validation\\nSVR (Misra et al., 2016) 63.33 -\\nBERT-AFS-base 77.20 74.84\\nSBERT-AFS-base 76.57 74.13\\nBERT-AFS-large 78.68 76.38\\nSBERT-AFS-large 77.85 75.93\\nCross-Topic Evaluation\\nBERT-AFS-base 58.49 57.23\\nSBERT-AFS-base 52.34 50.65\\nBERT-AFS-large 62.02 60.34\\nSBERT-AFS-large 53.82 53.10\\nTable 3: Average Pearson correlation r and average'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 5}, page_content='Spearman’s rank correlation ρon the Argument Facet\\nSimilarity (AFS) corpus (Misra et al., 2016). Misra et\\nal. proposes 10-fold cross-validation. We additionally\\nevaluate in a cross-topic scenario: Methods are trained\\non two topics, and are evaluated on the third topic.\\ntences in the same section are thematically closer\\nthan sentences in different sections. They use this\\nto create a large dataset of weakly labeled sen-\\ntence triplets: The anchor and the positive exam-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 5}, page_content='tence triplets: The anchor and the positive exam-\\nple come from the same section, while the neg-\\native example comes from a different section of\\nthe same article. For example, from the Alice\\nArnold article: Anchor: Arnold joined the BBC\\nRadio Drama Company in 1988., positive: Arnold\\ngained media attention in May 2012., negative:\\nBalding and Arnold are keen amateur golfers.\\nWe use the dataset from Dor et al. We use the\\nTriplet Objective, train SBERT for one epoch on'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 5}, page_content='Triplet Objective, train SBERT for one epoch on\\nthe about 1.8 Million training triplets and evaluate\\nit on the 222,957 test triplets. Test triplets are from\\na distinct set of Wikipedia articles. As evaluation\\nmetric, we use accuracy: Is the positive example\\ncloser to the anchor than the negative example?\\nResults are presented in Table 4. Dor et al. ﬁne-\\ntuned a BiLSTM architecture with triplet loss to\\nderive sentence embeddings for this dataset. As\\nthe table shows, SBERT clearly outperforms the'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 5}, page_content='the table shows, SBERT clearly outperforms the\\nBiLSTM approach by Dor et al.\\n5 Evaluation - SentEval\\nSentEval (Conneau and Kiela, 2018) is a popular\\ntoolkit to evaluate the quality of sentence embed-\\ndings. Sentence embeddings are used as features\\nfor a logistic regression classiﬁer. The logistic re-\\ngression classiﬁer is trained on various tasks in a\\n10-fold cross-validation setup and the prediction\\naccuracy is computed for the test-fold.\\nModel Accuracy\\nmean-vectors 0.65\\nskip-thoughts-CS 0.62'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 5}, page_content='mean-vectors 0.65\\nskip-thoughts-CS 0.62\\nDor et al. 0.74\\nSBERT-WikiSec-base 0.8042\\nSBERT-WikiSec-large 0.8078\\nSRoBERTa-WikiSec-base 0.7945\\nSRoBERTa-WikiSec-large 0.7973\\nTable 4: Evaluation on the Wikipedia section triplets\\ndataset (Dor et al., 2018). SBERT trained with triplet\\nloss for one epoch.\\nThe purpose of SBERT sentence embeddings\\nare not to be used for transfer learning for other\\ntasks. Here, we think ﬁne-tuning BERT as de-\\nscribed by Devlin et al. (2018) for new tasks is'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 5}, page_content='scribed by Devlin et al. (2018) for new tasks is\\nthe more suitable method, as it updates all layers\\nof the BERT network. However, SentEval can still\\ngive an impression on the quality of our sentence\\nembeddings for various tasks.\\nWe compare the SBERT sentence embeddings\\nto other sentence embeddings methods on the fol-\\nlowing seven SentEval transfer tasks:\\n• MR: Sentiment prediction for movie reviews\\nsnippets on a ﬁve start scale (Pang and Lee,\\n2005).\\n• CR: Sentiment prediction of customer prod-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 5}, page_content='• CR: Sentiment prediction of customer prod-\\nuct reviews (Hu and Liu, 2004).\\n• SUBJ: Subjectivity prediction of sentences\\nfrom movie reviews and plot summaries\\n(Pang and Lee, 2004).\\n• MPQA: Phrase level opinion polarity classi-\\nﬁcation from newswire (Wiebe et al., 2005).\\n• SST: Stanford Sentiment Treebank with bi-\\nnary labels (Socher et al., 2013).\\n• TREC: Fine grained question-type classiﬁ-\\ncation from TREC (Li and Roth, 2002).\\n• MRPC: Microsoft Research Paraphrase Cor-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 5}, page_content='• MRPC: Microsoft Research Paraphrase Cor-\\npus from parallel news sources (Dolan et al.,\\n2004).\\nThe results can be found in Table 5. SBERT\\nis able to achieve the best performance in 5 out\\nof 7 tasks. The average performance increases\\nby about 2 percentage points compared to In-\\nferSent as well as the Universal Sentence Encoder.\\nEven though transfer learning is not the purpose of\\nSBERT, it outperforms other state-of-the-art sen-\\ntence embeddings methods on this task.'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 6}, page_content='Model MR CR SUBJ MPQA SST TREC MRPC Avg.\\nAvg. GloVe embeddings 77.25 78.30 91.17 87.85 80.18 83.0 72.87 81.52\\nAvg. fast-text embeddings 77.96 79.23 91.68 87.81 82.15 83.6 74.49 82.42\\nAvg. BERT embeddings 78.66 86.25 94.37 88.66 84.40 92.8 69.45 84.94\\nBERT CLS-vector 78.68 84.85 94.21 88.23 84.13 91.4 71.13 84.66\\nInferSent - GloVe 81.57 86.54 92.50 90.38 84.18 88.2 75.77 85.59\\nUniversal Sentence Encoder 80.09 85.19 93.98 86.70 86.38 93.2 70.14 85.10'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 6}, page_content='SBERT-NLI-base 83.64 89.43 94.39 89.86 88.96 89.6 76.00 87.41\\nSBERT-NLI-large 84.88 90.07 94.52 90.33 90.66 87.4 75.94 87.69\\nTable 5: Evaluation of SBERT sentence embeddings using the SentEval toolkit. SentEval evaluates sentence\\nembeddings on different sentence classiﬁcation tasks by training a logistic regression classiﬁer using the sentence\\nembeddings as features. Scores are based on a 10-fold cross-validation.\\nIt appears that the sentence embeddings from'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 6}, page_content='It appears that the sentence embeddings from\\nSBERT capture well sentiment information: We\\nobserve large improvements for all sentiment tasks\\n(MR, CR, and SST) from SentEval in comparison\\nto InferSent and Universal Sentence Encoder.\\nThe only dataset where SBERT is signiﬁcantly\\nworse than Universal Sentence Encoder is the\\nTREC dataset. Universal Sentence Encoder was\\npre-trained on question-answering data, which ap-\\npears to be beneﬁcial for the question-type classi-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 6}, page_content='ﬁcation task of the TREC dataset.\\nAverage BERT embeddings or using the CLS-\\ntoken output from a BERT network achieved bad\\nresults for various STS tasks (Table 1), worse than\\naverage GloVe embeddings. However, for Sent-\\nEval, average BERT embeddings and the BERT\\nCLS-token output achieves decent results (Ta-\\nble 5), outperforming average GloVe embeddings.\\nThe reason for this are the different setups. For\\nthe STS tasks, we used cosine-similarity to es-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 6}, page_content='the STS tasks, we used cosine-similarity to es-\\ntimate the similarities between sentence embed-\\ndings. Cosine-similarity treats all dimensions\\nequally. In contrast, SentEval ﬁts a logistic regres-\\nsion classiﬁer to the sentence embeddings. This\\nallows that certain dimensions can have higher or\\nlower impact on the classiﬁcation result.\\nWe conclude that average BERT embeddings /\\nCLS-token output from BERT return sentence em-\\nbeddings that are infeasible to be used with cosine-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 6}, page_content='similarity or with Manhatten / Euclidean distance.\\nFor transfer learning, they yield slightly worse\\nresults than InferSent or Universal Sentence En-\\ncoder. However, using the described ﬁne-tuning\\nsetup with a siamese network structure on NLI\\ndatasets yields sentence embeddings that achieve\\na new state-of-the-art for the SentEval toolkit.\\n6 Ablation Study\\nWe have demonstrated strong empirical results for\\nthe quality of SBERT sentence embeddings. In'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 6}, page_content='the quality of SBERT sentence embeddings. In\\nthis section, we perform an ablation study of dif-\\nferent aspects of SBERT in order to get a better\\nunderstanding of their relative importance.\\nWe evaluated different pooling strategies\\n(MEAN, MAX, and CLS). For the classiﬁcation\\nobjective function, we evaluate different concate-\\nnation methods. For each possible conﬁguration,\\nwe train SBERT with 10 different random seeds\\nand average the performances.\\nThe objective function (classiﬁcation vs. regres-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 6}, page_content='The objective function (classiﬁcation vs. regres-\\nsion) depends on the annotated dataset. For the\\nclassiﬁcation objective function, we train SBERT-\\nbase on the SNLI and the Multi-NLI dataset. For\\nthe regression objective function, we train on the\\ntraining set of the STS benchmark dataset. Perfor-\\nmances are measured on the development split of\\nthe STS benchmark dataset. Results are shown in\\nTable 6.\\nNLI STSb\\nPooling Strategy\\nMEAN 80.78 87.44\\nMAX 79.07 69.92\\nCLS 79.80 86.62\\nConcatenation'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 6}, page_content='MAX 79.07 69.92\\nCLS 79.80 86.62\\nConcatenation\\n(u,v) 66.04 -\\n(|u−v|) 69.78 -\\n(u∗v) 70.54 -\\n(|u−v|,u ∗v) 78.37 -\\n(u,v,u ∗v) 77.44 -\\n(u,v, |u−v|) 80.78 -\\n(u,v, |u−v|,u ∗v) 80.44 -\\nTable 6: SBERT trained on NLI data with the clas-\\nsiﬁcation objective function, on the STS benchmark\\n(STSb) with the regression objective function. Con-\\nﬁgurations are evaluated on the development set of the\\nSTSb using cosine-similarity and Spearman’s rank cor-\\nrelation. For the concatenation methods, we only report'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 6}, page_content='scores with MEAN pooling strategy.\\nWhen trained with the classiﬁcation objective\\nfunction on NLI data, the pooling strategy has a\\nrather minor impact. The impact of the concate-\\nnation mode is much larger. InferSent (Conneau'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 7}, page_content='et al., 2017) and Universal Sentence Encoder (Cer\\net al., 2018) both use (u,v, |u−v|,u ∗v) as input\\nfor a softmax classiﬁer. However, in our architec-\\nture, adding the element-wise u∗vdecreased the\\nperformance.\\nThe most important component is the element-\\nwise difference |u−v|. Note, that the concate-\\nnation mode is only relevant for training the soft-\\nmax classiﬁer. At inference, when predicting sim-\\nilarities for the STS benchmark dataset, only the\\nsentence embeddings uand v are used in combi-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 7}, page_content='sentence embeddings uand v are used in combi-\\nnation with cosine-similarity. The element-wise\\ndifference measures the distance between the di-\\nmensions of the two sentence embeddings, ensur-\\ning that similar pairs are closer and dissimilar pairs\\nare further apart.\\nWhen trained with the regression objective\\nfunction, we observe that the pooling strategy has\\na large impact. There, the MAX strategy perform\\nsigniﬁcantly worse than MEAN or CLS-token strat-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 7}, page_content='signiﬁcantly worse than MEAN or CLS-token strat-\\negy. This is in contrast to (Conneau et al., 2017),\\nwho found it beneﬁcial for the BiLSTM-layer of\\nInferSent to use MAX instead of MEAN pooling.\\n7 Computational Efﬁciency\\nSentence embeddings need potentially be com-\\nputed for Millions of sentences, hence, a high\\ncomputation speed is desired. In this section, we\\ncompare SBERT to average GloVe embeddings,\\nInferSent (Conneau et al., 2017), and Universal\\nSentence Encoder (Cer et al., 2018).'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 7}, page_content='Sentence Encoder (Cer et al., 2018).\\nFor our comparison we use the sentences from\\nthe STS benchmark (Cer et al., 2017). We com-\\npute average GloVe embeddings using a sim-\\nple for-loop with python dictionary lookups and\\nNumPy. InferSent 4 is based on PyTorch. For\\nUniversal Sentence Encoder, we use the Tensor-\\nFlow Hub version 5, which is based on Tensor-\\nFlow. SBERT is based on PyTorch. For improved\\ncomputation of sentence embeddings, we imple-\\nmented a smart batching strategy: Sentences with'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 7}, page_content='mented a smart batching strategy: Sentences with\\nsimilar lengths are grouped together and are only\\npadded to the longest element in a mini-batch.\\nThis drastically reduces computational overhead\\nfrom padding tokens.\\nPerformances were measured on a server with\\nIntel i7-5820K CPU @ 3.30GHz, Nvidia Tesla\\n4https://github.com/facebookresearch/\\nInferSent\\n5https://tfhub.dev/google/\\nuniversal-sentence-encoder-large/3\\nV100 GPU, CUDA 9.2 and cuDNN. The results\\nare depicted in Table 7.\\nModel CPU GPU'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 7}, page_content='are depicted in Table 7.\\nModel CPU GPU\\nAvg. GloVe embeddings 6469 -\\nInferSent 137 1876\\nUniversal Sentence Encoder 67 1318\\nSBERT-base 44 1378\\nSBERT-base - smart batching 83 2042\\nTable 7: Computation speed (sentences per second) of\\nsentence embedding methods. Higher is better.\\nOn CPU, InferSent is about 65% faster than\\nSBERT. This is due to the much simpler net-\\nwork architecture. InferSent uses a single Bi-\\nLSTM layer, while BERT uses 12 stacked trans-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 7}, page_content='LSTM layer, while BERT uses 12 stacked trans-\\nformer layers. However, an advantage of trans-\\nformer networks is the computational efﬁciency\\non GPUs. There, SBERT with smart batching\\nis about 9% faster than InferSent and about 55%\\nfaster than Universal Sentence Encoder. Smart\\nbatching achieves a speed-up of 89% on CPU and\\n48% on GPU. Average GloVe embeddings is obvi-\\nously by a large margin the fastest method to com-\\npute sentence embeddings.\\n8 Conclusion'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 7}, page_content='pute sentence embeddings.\\n8 Conclusion\\nWe showed that BERT out-of-the-box maps sen-\\ntences to a vector space that is rather unsuit-\\nable to be used with common similarity measures\\nlike cosine-similarity. The performance for seven\\nSTS tasks was below the performance of average\\nGloVe embeddings.\\nTo overcome this shortcoming, we presented\\nSentence-BERT (SBERT). SBERT ﬁne-tunes\\nBERT in a siamese / triplet network architec-\\nture. We evaluated the quality on various com-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 7}, page_content='ture. We evaluated the quality on various com-\\nmon benchmarks, where it could achieve a sig-\\nniﬁcant improvement over state-of-the-art sen-\\ntence embeddings methods. Replacing BERT with\\nRoBERTa did not yield a signiﬁcant improvement\\nin our experiments.\\nSBERT is computationally efﬁcient. On a GPU,\\nit is about 9% faster than InferSent and about 55%\\nfaster than Universal Sentence Encoder. SBERT\\ncan be used for tasks which are computationally\\nnot feasible to be modeled with BERT. For exam-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 7}, page_content='not feasible to be modeled with BERT. For exam-\\nple, clustering of 10,000 sentences with hierarchi-\\ncal clustering requires with BERT about 65 hours,\\nas around 50 Million sentence combinations must\\nbe computed. With SBERT, we were able to re-\\nduce the effort to about 5 seconds.'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 8}, page_content='Acknowledgments\\nThis work has been supported by the German\\nResearch Foundation through the German-Israeli\\nProject Cooperation (DIP, grant DA 1600/1-1 and\\ngrant GU 798/17-1). It has been co-funded by the\\nGerman Federal Ministry of Education and Re-\\nsearch (BMBF) under the promotional references\\n03VP02540 (ArgumenText).\\nReferences\\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel\\nCer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei\\nGuo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 8}, page_content='Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada\\nMihalcea, German Rigau, Larraitz Uria, and Janyce\\nWiebe. 2015. SemEval-2015 Task 2: Semantic Tex-\\ntual Similarity, English, Spanish and Pilot on Inter-\\npretability. In Proceedings of the 9th International\\nWorkshop on Semantic Evaluation (SemEval 2015),\\npages 252–263, Denver, Colorado. Association for\\nComputational Linguistics.\\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel\\nCer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 8}, page_content='Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei\\nGuo, Rada Mihalcea, German Rigau, and Janyce\\nWiebe. 2014. SemEval-2014 Task 10: Multilingual\\nSemantic Textual Similarity. In Proceedings of the\\n8th International Workshop on Semantic Evaluation\\n(SemEval 2014), pages 81–91, Dublin, Ireland. As-\\nsociation for Computational Linguistics.\\nEneko Agirre, Carmen Banea, Daniel M. Cer, Mona T.\\nDiab, Aitor Gonzalez-Agirre, Rada Mihalcea, Ger-\\nman Rigau, and Janyce Wiebe. 2016. SemEval-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 8}, page_content='man Rigau, and Janyce Wiebe. 2016. SemEval-\\n2016 Task 1: Semantic Textual Similarity, Mono-\\nlingual and Cross-Lingual Evaluation. In Proceed-\\nings of the 10th International Workshop on Seman-\\ntic Evaluation, SemEval@NAACL-HLT 2016, San\\nDiego, CA, USA, June 16-17, 2016, pages 497–511.\\nEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-\\nAgirre, and Weiwei Guo. 2013. *SEM 2013 shared\\ntask: Semantic Textual Similarity. In Second Joint\\nConference on Lexical and Computational Seman-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 8}, page_content='Conference on Lexical and Computational Seman-\\ntics (*SEM), Volume 1: Proceedings of the Main\\nConference and the Shared Task: Semantic Textual\\nSimilarity, pages 32–43, Atlanta, Georgia, USA. As-\\nsociation for Computational Linguistics.\\nEneko Agirre, Mona Diab, Daniel Cer, and Aitor\\nGonzalez-Agirre. 2012. SemEval-2012 Task 6: A\\nPilot on Semantic Textual Similarity. In Proceed-\\nings of the First Joint Conference on Lexical and\\nComputational Semantics - Volume 1: Proceedings'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 8}, page_content='Computational Semantics - Volume 1: Proceedings\\nof the Main Conference and the Shared Task, and\\nVolume 2: Proceedings of the Sixth International\\nWorkshop on Semantic Evaluation, SemEval ’12,\\npages 385–393, Stroudsburg, PA, USA. Association\\nfor Computational Linguistics.\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn Proceedings of the 2015 Conference on Empiri-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 8}, page_content='In Proceedings of the 2015 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n632–642, Lisbon, Portugal. Association for Compu-\\ntational Linguistics.\\nDaniel Cer, Mona Diab, Eneko Agirre, Iigo Lopez-\\nGazpio, and Lucia Specia. 2017. SemEval-2017\\nTask 1: Semantic Textual Similarity Multilingual\\nand Crosslingual Focused Evaluation. In Proceed-\\nings of the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1–14, Vancou-\\nver, Canada.'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 8}, page_content='ver, Canada.\\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\\nNicole Limtiaco, Rhomni St. John, Noah Constant,\\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\\nYun-Hsuan Sung, Brian Strope, and Ray Kurzweil.\\n2018. Universal Sentence Encoder. arXiv preprint\\narXiv:1803.11175.\\nAlexis Conneau and Douwe Kiela. 2018. SentEval: An\\nEvaluation Toolkit for Universal Sentence Represen-\\ntations. arXiv preprint arXiv:1803.05449.\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo ¨ıc'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 8}, page_content='Barrault, and Antoine Bordes. 2017. Supervised\\nLearning of Universal Sentence Representations\\nfrom Natural Language Inference Data. In Proceed-\\nings of the 2017 Conference on Empirical Methods\\nin Natural Language Processing, pages 670–680,\\nCopenhagen, Denmark. Association for Computa-\\ntional Linguistics.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2018. BERT: Pre-training of\\nDeep Bidirectional Transformers for Language Un-\\nderstanding. arXiv preprint arXiv:1810.04805.'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 8}, page_content='derstanding. arXiv preprint arXiv:1810.04805.\\nBill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-\\nsupervised Construction of Large Paraphrase Cor-\\npora: Exploiting Massively Parallel News Sources.\\nIn Proceedings of the 20th International Confer-\\nence on Computational Linguistics, COLING ’04,\\nStroudsburg, PA, USA. Association for Computa-\\ntional Linguistics.\\nLiat Ein Dor, Yosi Mass, Alon Halfon, Elad Venezian,\\nIlya Shnayderman, Ranit Aharonov, and Noam'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 8}, page_content='Ilya Shnayderman, Ranit Aharonov, and Noam\\nSlonim. 2018. Learning Thematic Similarity Metric\\nfrom Article Sections Using Triplet Networks. In\\nProceedings of the 56th Annual Meeting of the As-\\nsociation for Computational Linguistics (Volume 2:\\nShort Papers), pages 49–54, Melbourne, Australia.\\nAssociation for Computational Linguistics.\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen.\\n2016. Learning Distributed Representations of Sen-\\ntences from Unlabelled Data. In Proceedings of'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 8}, page_content='tences from Unlabelled Data. In Proceedings of\\nthe 2016 Conference of the North American Chap-\\nter of the Association for Computational Linguis-\\ntics: Human Language Technologies, pages 1367–\\n1377, San Diego, California. Association for Com-\\nputational Linguistics.'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 9}, page_content='Minqing Hu and Bing Liu. 2004. Mining and Sum-\\nmarizing Customer Reviews. In Proceedings of the\\nTenth ACM SIGKDD International Conference on\\nKnowledge Discovery and Data Mining, KDD ’04,\\npages 168–177, New York, NY , USA. ACM.\\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\\nand Jason Weston. 2019. Real-time Inference\\nin Multi-sentence Tasks with Deep Pretrained\\nTransformers. arXiv preprint arXiv:1905.01969,\\nabs/1905.01969.\\nJeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2017.'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 9}, page_content='Billion-scale similarity search with GPUs. arXiv\\npreprint arXiv:1702.08734.\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-Thought Vectors. In\\nC. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama,\\nand R. Garnett, editors, Advances in Neural Infor-\\nmation Processing Systems 28, pages 3294–3302.\\nCurran Associates, Inc.\\nXin Li and Dan Roth. 2002. Learning Question Classi-\\nﬁers. In Proceedings of the 19th International Con-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 9}, page_content='ference on Computational Linguistics - Volume 1,\\nCOLING ’02, pages 1–7, Stroudsburg, PA, USA.\\nAssociation for Computational Linguistics.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoBERTa: A Robustly Optimized BERT Pretrain-\\ning Approach. arXiv preprint arXiv:1907.11692.\\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\\nBentivogli, Raffaella Bernardi, and Roberto Zam-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 9}, page_content='Bentivogli, Raffaella Bernardi, and Roberto Zam-\\nparelli. 2014. A SICK cure for the evaluation of\\ncompositional distributional semantic models. In\\nProceedings of the Ninth International Conference\\non Language Resources and Evaluation (LREC’14),\\npages 216–223, Reykjavik, Iceland. European Lan-\\nguage Resources Association (ELRA).\\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\\nBowman, and Rachel Rudinger. 2019. On Mea-\\nsuring Social Biases in Sentence Encoders. arXiv\\npreprint arXiv:1903.10561.'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 9}, page_content='preprint arXiv:1903.10561.\\nAmita Misra, Brian Ecker, and Marilyn A. Walker.\\n2016. Measuring the Similarity of Sentential Ar-\\nguments in Dialogue. In Proceedings of the SIG-\\nDIAL 2016 Conference, The 17th Annual Meeting\\nof the Special Interest Group on Discourse and Di-\\nalogue, 13-15 September 2016, Los Angeles, CA,\\nUSA, pages 276–287.\\nBo Pang and Lillian Lee. 2004. A Sentimental Educa-\\ntion: Sentiment Analysis Using Subjectivity Sum-\\nmarization Based on Minimum Cuts. In Proceed-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 9}, page_content='marization Based on Minimum Cuts. In Proceed-\\nings of the 42nd Meeting of the Association for\\nComputational Linguistics (ACL’04), Main Volume,\\npages 271–278, Barcelona, Spain.\\nBo Pang and Lillian Lee. 2005. Seeing Stars: Exploit-\\ning Class Relationships for Sentiment Categoriza-\\ntion with Respect to Rating Scales. In Proceedings\\nof the 43rd Annual Meeting of the Association for\\nComputational Linguistics (ACL’05), pages 115–\\n124, Ann Arbor, Michigan. Association for Compu-\\ntational Linguistics.'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 9}, page_content='tational Linguistics.\\nJeffrey Pennington, Richard Socher, and Christo-\\npher D. Manning. 2014. GloVe: Global Vectors for\\nWord Representation. In Empirical Methods in Nat-\\nural Language Processing (EMNLP), pages 1532–\\n1543.\\nYifan Qiao, Chenyan Xiong, Zheng-Hao Liu, and\\nZhiyuan Liu. 2019. Understanding the Be-\\nhaviors of BERT in Ranking. arXiv preprint\\narXiv:1904.07531.\\nNils Reimers, Philip Beyer, and Iryna Gurevych. 2016.\\nTask-Oriented Intrinsic Evaluation of Semantic Tex-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 9}, page_content='tual Similarity. In Proceedings of the 26th Inter-\\nnational Conference on Computational Linguistics\\n(COLING), pages 87–96.\\nNils Reimers and Iryna Gurevych. 2018. Why Com-\\nparing Single Performance Scores Does Not Al-\\nlow to Draw Conclusions About Machine Learn-\\ning Approaches. arXiv preprint arXiv:1803.09578,\\nabs/1803.09578.\\nNils Reimers, Benjamin Schiller, Tilman Beck, Jo-\\nhannes Daxenberger, Christian Stab, and Iryna\\nGurevych. 2019. Classiﬁcation and Clustering of'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 9}, page_content='Gurevych. 2019. Classiﬁcation and Clustering of\\nArguments with Contextualized Word Embeddings.\\nIn Proceedings of the 57th Annual Meeting of the As-\\nsociation for Computational Linguistics, pages 567–\\n578, Florence, Italy. Association for Computational\\nLinguistics.\\nFlorian Schroff, Dmitry Kalenichenko, and James\\nPhilbin. 2015. FaceNet: A Uniﬁed Embedding for\\nFace Recognition and Clustering. arXiv preprint\\narXiv:1503.03832, abs/1503.03832.\\nRichard Socher, Alex Perelygin, Jean Wu, Jason'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 9}, page_content='Richard Socher, Alex Perelygin, Jean Wu, Jason\\nChuang, Christopher D. Manning, Andrew Ng, and\\nChristopher Potts. 2013. Recursive Deep Models for\\nSemantic Compositionality Over a Sentiment Tree-\\nbank. In Proceedings of the 2013 Conference on\\nEmpirical Methods in Natural Language Process-\\ning, pages 1631–1642, Seattle, Washington, USA.\\nAssociation for Computational Linguistics.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 9}, page_content='Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is All\\nyou Need. In I. Guyon, U. V . Luxburg, S. Bengio,\\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\\nnett, editors, Advances in Neural Information Pro-\\ncessing Systems 30, pages 5998–6008.\\nJanyce Wiebe, Theresa Wilson, and Claire Cardie.\\n2005. Annotating Expressions of Opinions and\\nEmotions in Language. Language Resources and\\nEvaluation, 39(2):165–210.'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 10}, page_content='Adina Williams, Nikita Nangia, and Samuel Bowman.\\n2018. A Broad-Coverage Challenge Corpus for\\nSentence Understanding through Inference. In Pro-\\nceedings of the 2018 Conference of the North Amer-\\nican Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Vol-\\nume 1 (Long Papers), pages 1112–1122. Association\\nfor Computational Linguistics.\\nYinfei Yang, Steve Yuan, Daniel Cer, Sheng-Yi Kong,\\nNoah Constant, Petr Pilar, Heming Ge, Yun-hsuan'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 10}, page_content='Noah Constant, Petr Pilar, Heming Ge, Yun-hsuan\\nSung, Brian Strope, and Ray Kurzweil. 2018.\\nLearning Semantic Textual Similarity from Conver-\\nsations. In Proceedings of The Third Workshop\\non Representation Learning for NLP, pages 164–\\n174, Melbourne, Australia. Association for Compu-\\ntational Linguistics.\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G.\\nCarbonell, Ruslan Salakhutdinov, and Quoc V . Le.\\n2019. XLNet: Generalized Autoregressive Pretrain-'),\n",
       " Document(metadata={'source': 'SentBert.pdf', 'page': 10}, page_content='2019. XLNet: Generalized Autoregressive Pretrain-\\ning for Language Understanding. arXiv preprint\\narXiv:1906.08237, abs/1906.08237.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\\nWeinberger, and Yoav Artzi. 2019. BERTScore:\\nEvaluating Text Generation with BERT. arXiv\\npreprint arXiv:1904.09675.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "final_documents=text_splitter.split_documents(docs)\n",
    "final_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\n",
      "Nils Reimers and Iryna Gurevych\n",
      "Ubiquitous Knowledge Processing Lab (UKP-TUDA)\n",
      "Department of Computer Science, Technische Universit¨at Darmstadt\n",
      "www.ukp.tu-darmstadt.de\n",
      "Abstract\n",
      "BERT (Devlin et al., 2018) and RoBERTa (Liu\n",
      "et al., 2019) has set a new state-of-the-art\n",
      "performance on sentence-pair regression tasks\n",
      "like semantic textual similarity (STS). How-\n",
      "ever, it requires that both sentences are fed' metadata={'source': 'SentBert.pdf', 'page': 0}\n",
      "page_content='ever, it requires that both sentences are fed\n",
      "into the network, which causes a massive com-\n",
      "putational overhead: Finding the most sim-\n",
      "ilar pair in a collection of 10,000 sentences\n",
      "requires about 50 million inference computa-\n",
      "tions (~65 hours) with BERT. The construction\n",
      "of BERT makes it unsuitable for semantic sim-\n",
      "ilarity search as well as for unsupervised tasks\n",
      "like clustering.\n",
      "In this publication, we present Sentence-BERT\n",
      "(SBERT), a modiﬁcation of the pretrained' metadata={'source': 'SentBert.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(final_documents[0])\n",
    "print(final_documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
